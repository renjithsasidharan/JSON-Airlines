{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transformer_combo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "359px",
        "width": "160px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "303799afa5e449008b0cee8aaf4f594d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3db39f9896674cccaf8e887ed86429a7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_1be5f6ff8d614fccb0d0852e4b98937b",
              "IPY_MODEL_48f38c0c4a4a40f59e2bb666ac60b260"
            ]
          }
        },
        "3db39f9896674cccaf8e887ed86429a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1be5f6ff8d614fccb0d0852e4b98937b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bf518af2ff5b49dcab27b6c22afc0225",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 2000,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 2000,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_af8d9a20fa464a6dbf716582957ffc8e"
          }
        },
        "48f38c0c4a4a40f59e2bb666ac60b260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_edf7928d9c1e42a1b17f895a43fe8aac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 2000/2000 [30:21&lt;00:00,  1.13it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2db29f5a3f43416299ccbc8362d70959"
          }
        },
        "bf518af2ff5b49dcab27b6c22afc0225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "af8d9a20fa464a6dbf716582957ffc8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "edf7928d9c1e42a1b17f895a43fe8aac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2db29f5a3f43416299ccbc8362d70959": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renjithsasidharan/JSON-Airlines/blob/master/transformer_combo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bWfG3BLaOcjN",
        "outputId": "401dd066-9386-46bf-c450-75f33f5045d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0zJjX6ljOYHW",
        "outputId": "81548120-8c6d-4470-e09b-cc93f585f49c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "!unzip ./drive/My\\ Drive/training_data/er_transformer/training_14_as_12_seq_nums.zip\n",
        "!cp -r ./training_14_as_12_seq_nums/* ./"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./drive/My Drive/training_data/er_transformer/training_14_as_12_seq_nums.zip\n",
            "   creating: training_14_as_12_seq_nums/\n",
            "  inflating: training_14_as_12_seq_nums/transformer_dates.py  \n",
            "  inflating: training_14_as_12_seq_nums/eval.tsv  \n",
            "  inflating: training_14_as_12_seq_nums/ocr_text.py  \n",
            "  inflating: training_14_as_12_seq_nums/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/training_14_as_12_seq_nums/\n",
            "  inflating: __MACOSX/training_14_as_12_seq_nums/._.DS_Store  \n",
            "  inflating: training_14_as_12_seq_nums/train.tsv.ocr.subwords  \n",
            "  inflating: training_14_as_12_seq_nums/transformer_dates.ipynb  \n",
            "  inflating: training_14_as_12_seq_nums/create_eval_data.py  \n",
            "  inflating: __MACOSX/training_14_as_12_seq_nums/._create_eval_data.py  \n",
            "  inflating: training_14_as_12_seq_nums/train.tsv  \n",
            "  inflating: training_14_as_12_seq_nums/create_training_data_test.py  \n",
            "  inflating: training_14_as_12_seq_nums/config.json  \n",
            "  inflating: training_14_as_12_seq_nums/augmentation.py  \n",
            "  inflating: __MACOSX/training_14_as_12_seq_nums/._augmentation.py  \n",
            "   creating: training_14_as_12_seq_nums/checkpoints/\n",
            "   creating: training_14_as_12_seq_nums/checkpoints/train/\n",
            "  inflating: training_14_as_12_seq_nums/checkpoints/train/.DS_Store  \n",
            "   creating: __MACOSX/training_14_as_12_seq_nums/checkpoints/\n",
            "   creating: __MACOSX/training_14_as_12_seq_nums/checkpoints/train/\n",
            "  inflating: __MACOSX/training_14_as_12_seq_nums/checkpoints/train/._.DS_Store  \n",
            "  inflating: __MACOSX/training_14_as_12_seq_nums/checkpoints/._train  \n",
            "  inflating: __MACOSX/training_14_as_12_seq_nums/._checkpoints  \n",
            "   creating: training_14_as_12_seq_nums/__pycache__/\n",
            "  inflating: training_14_as_12_seq_nums/__pycache__/augmentation.cpython-37.pyc  \n",
            "  inflating: training_14_as_12_seq_nums/__pycache__/augmentation.cpython-36.pyc  \n",
            "  inflating: training_14_as_12_seq_nums/__pycache__/training_helper.cpython-36.pyc  \n",
            "  inflating: training_14_as_12_seq_nums/__pycache__/ocr_text.cpython-37.pyc  \n",
            "  inflating: training_14_as_12_seq_nums/__pycache__/ocr_text.cpython-36.pyc  \n",
            "  inflating: training_14_as_12_seq_nums/currency_classes.json  \n",
            "  inflating: training_14_as_12_seq_nums/data.json  \n",
            "  inflating: training_14_as_12_seq_nums/abbyy_ocr.ipynb  \n",
            "  inflating: training_14_as_12_seq_nums/train.tsv.date.subwords  \n",
            "  inflating: __MACOSX/training_14_as_12_seq_nums/._train.tsv.date.subwords  \n",
            "  inflating: training_14_as_12_seq_nums/.rsyncignore  \n",
            "  inflating: training_14_as_12_seq_nums/test.tsv  \n",
            "  inflating: training_14_as_12_seq_nums/transformer_dates_amounts.ipynb  \n",
            "  inflating: __MACOSX/training_14_as_12_seq_nums/._transformer_dates_amounts.ipynb  \n",
            "  inflating: training_14_as_12_seq_nums/combo.weights.12.h5  \n",
            "  inflating: __MACOSX/training_14_as_12_seq_nums/._combo.weights.12.h5  \n",
            "  inflating: training_14_as_12_seq_nums/country_classes.json  \n",
            "   creating: training_14_as_12_seq_nums/.ipynb_checkpoints/\n",
            "  inflating: training_14_as_12_seq_nums/.ipynb_checkpoints/transformer_combo-checkpoint.ipynb  \n",
            "  inflating: training_14_as_12_seq_nums/.ipynb_checkpoints/abbyy_ocr-checkpoint.ipynb  \n",
            "  inflating: training_14_as_12_seq_nums/.ipynb_checkpoints/transformer_dates-checkpoint.ipynb  \n",
            "  inflating: training_14_as_12_seq_nums/.ipynb_checkpoints/transformer_dates_amounts-checkpoint.ipynb  \n",
            "   creating: training_14_as_12_seq_nums/.vscode/\n",
            "  inflating: training_14_as_12_seq_nums/.vscode/launch.json  \n",
            "  inflating: training_14_as_12_seq_nums/create_training_data.py  \n",
            "  inflating: training_14_as_12_seq_nums/transformer_combo.ipynb  \n",
            "  inflating: __MACOSX/training_14_as_12_seq_nums/._transformer_combo.ipynb  \n",
            "  inflating: training_14_as_12_seq_nums/analyze_logs.py  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bJieQZPgCrdG",
        "colab": {}
      },
      "source": [
        "!rm -rf ./checkpoints/train/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q66QCGEuxO16",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "18e3836a-8b24-4cf6-aea3-3ac43f9f2530"
      },
      "source": [
        "!pip install -q pyonmttok"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyonmttok\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/20/3c57198ffe690b580fbf23d33d5000eb411862e60e4bb6853b61dc989187/pyonmttok-1.18.3-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.2MB 4.9MB/s \n",
            "\u001b[?25hInstalling collected packages: pyonmttok\n",
            "Successfully installed pyonmttok-1.18.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Transformer model for language understanding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:21.831641Z",
          "start_time": "2019-11-10T06:32:19.420135Z"
        },
        "colab_type": "code",
        "id": "JjJJyJTZYebt",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pyonmttok\n",
        "import pandas as pd\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:21.837459Z",
          "start_time": "2019-11-10T06:32:21.833884Z"
        },
        "colab_type": "code",
        "id": "y7hAaZD7uYZa",
        "outputId": "ddc383e7-28e3-437b-fb10-b8e71b6f8ece",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fd1NWMxjfsDd"
      },
      "source": [
        "## Setup input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:21.842399Z",
          "start_time": "2019-11-10T06:32:21.839958Z"
        },
        "colab_type": "code",
        "id": "w4cXX4eKuYZh",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "train_file_path='./train.tsv'\n",
        "test_file_path='./test.tsv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yD_0WyRpgQY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_currency_classes():\n",
        "    with open('currency_classes.json') as f:\n",
        "        currency_classes = json.load(f)\n",
        "    return currency_classes\n",
        "\n",
        "def load_country_classes():\n",
        "    with open('country_classes.json') as f:\n",
        "        country_classes = json.load(f)\n",
        "    return country_classes\n",
        "\n",
        "\n",
        "def encode_label(label, classes):\n",
        "    label_encoded = np.zeros(len(classes))\n",
        "    label_encoded[classes.index(label)] = 1\n",
        "    return label_encoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzwDutTMxFMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(x):\n",
        "    t, _ = tokenizer_pre.tokenize(x)\n",
        "    return \" \".join(t)\n",
        "\n",
        "tokenizer_pre = pyonmttok.Tokenizer(\"aggressive\", joiner_annotate=True, segment_numbers=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:23.718935Z",
          "start_time": "2019-11-10T06:32:21.844334Z"
        },
        "colab_type": "code",
        "id": "89PSUjtfuYZj",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "\n",
        "currency_classes = load_currency_classes()\n",
        "country_classes = load_country_classes()\n",
        "\n",
        "training_df = pd.read_csv(train_file_path,\n",
        "                          sep='\\t',\n",
        "                          float_precision='round_trip',\n",
        "                          dtype={'amount': str,\n",
        "                                 'date': str,\n",
        "                                 'country': str,\n",
        "                                 'currency': str,\n",
        "                                 'ocr_text': object})\n",
        "\n",
        "testing_df = pd.read_csv(test_file_path,\n",
        "                          sep='\\t',\n",
        "                          dtype={'amount': str,\n",
        "                                 'date': str,\n",
        "                                 'country': str,\n",
        "                                 'currency': str,\n",
        "                                 'ocr_text': object})\n",
        "\n",
        "training_df[\"ocr_text\"] = training_df[\"ocr_text\"].apply(lambda x: x.lower())\n",
        "testing_df[\"ocr_text\"] = testing_df[\"ocr_text\"].apply(lambda x: x.lower())\n",
        "\n",
        "training_df[\"ocr_text\"] = training_df[\"ocr_text\"].apply(lambda x: preprocess(x))\n",
        "training_df[\"amount\"] = training_df[\"amount\"].apply(lambda x: preprocess(x))\n",
        "training_df[\"date\"] = training_df[\"date\"].apply(lambda x: preprocess(x))\n",
        "testing_df[\"ocr_text\"] = testing_df[\"ocr_text\"].apply(lambda x: preprocess(x))\n",
        "testing_df[\"amount\"] = testing_df[\"amount\"].apply(lambda x: preprocess(x))\n",
        "testing_df[\"date\"] = testing_df[\"date\"].apply(lambda x: preprocess(x))\n",
        "\n",
        "training_examples = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(training_df['ocr_text'].values, tf.string),\n",
        "            tf.cast(training_df['amount'].values, tf.string),\n",
        "            tf.cast(training_df['date'].values, tf.string),\n",
        "            tf.cast(training_df['country'].apply(lambda x: country_classes.index(x)).values, tf.int32)\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "val_examples = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(testing_df['ocr_text'].values, tf.string),\n",
        "            tf.cast(testing_df['amount'].values, tf.string),\n",
        "            tf.cast(testing_df['date'].values, tf.string),\n",
        "            tf.cast(testing_df['country'].apply(lambda x: country_classes.index(x)).values, tf.int32)\n",
        "        )\n",
        "    )\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ8M43tSjMkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_COUNTRY_CLASSES = len(country_classes)\n",
        "NUM_CURRENCY_CLASSES = len(currency_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RCEKotqosGfq"
      },
      "source": [
        "Create a custom subwords tokenizer from the training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:23.812888Z",
          "start_time": "2019-11-10T06:32:23.720666Z"
        },
        "colab_type": "code",
        "id": "KVBg5Q8tBk5z",
        "outputId": "8b7f2c3a-9c1d-45a0-e245-9791c8e6bdcc",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"{}.ocr.subwords\".format(train_file_path))\n",
        "if os.path.isfile(\"{}.ocr.subwords\".format(train_file_path)):\n",
        "  print('Loading input tokenizer from disk')\n",
        "  tokenizer_ocr = tfds.features.text.SubwordTextEncoder.load_from_file(train_file_path + \".ocr\")\n",
        "else:\n",
        "  print('Creating new tokenizer from training set')\n",
        "  tokenizer_ocr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "      (ocr.numpy() for ocr, _, _, _ in training_examples), target_vocab_size=2**13)\n",
        "  tokenizer_ocr.save_to_file(train_file_path + \".ocr\")\n",
        "print(tokenizer_ocr.vocab_size)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./train.tsv.ocr.subwords\n",
            "Loading input tokenizer from disk\n",
            "32835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.021201Z",
          "start_time": "2019-11-10T06:32:23.814923Z"
        },
        "colab_type": "code",
        "id": "-jxOS89-uYZp",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "tokenizer_date = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (date.numpy() for _, _, date, _ in training_examples), target_vocab_size=2**9)\n",
        "tokenizer_date.save_to_file(train_file_path + \".date\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u0mYFVU5OMVe",
        "colab": {}
      },
      "source": [
        "tokenizer_amount = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (amount.numpy() for _, amount, _, _ in training_examples), target_vocab_size=2**9)\n",
        "tokenizer_amount.save_to_file(train_file_path + \".amount\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YlV1E42iCVzg",
        "colab": {}
      },
      "source": [
        "tokenizer_amount = tokenizer_ocr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.026527Z",
          "start_time": "2019-11-10T06:32:42.023029Z"
        },
        "colab_type": "code",
        "id": "VTiBw2AsOMVg",
        "outputId": "c6f62bce-1bf7-4cdb-db7b-797d673e169b",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "date_str = \"13/11/2019\"\n",
        "print('Original date {}'.format(date_str))\n",
        "print('Tokenized date {}'.format(tokenizer_date.encode(date_str)))\n",
        "print('Tokens {}'.format([tokenizer_date.decode([x]) for x in tokenizer_date.encode(date_str)]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original date 13/11/2019\n",
            "Tokenized date [52, 54, 50, 52, 52, 50, 53, 51, 52, 60]\n",
            "Tokens ['1', '3', '/', '1', '1', '/', '2', '0', '1', '9']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MxO2Zpl8OMVj",
        "outputId": "aa5f2b6f-aad9-4e34-d81f-06b7faf18fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "amount_str = \"2185.90\"\n",
        "print('Original amount {}'.format(amount_str))\n",
        "print('Tokenized amount {}'.format(tokenizer_amount.encode(amount_str)))\n",
        "print('Tokens {}'.format([tokenizer_amount.decode([x]) for x in tokenizer_amount.encode(amount_str)]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original amount 2185.90\n",
            "Tokenized amount [32629, 32628, 32635, 32632, 32625, 32636, 32627]\n",
            "Tokens ['2', '1', '8', '5', '.', '9', '0']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.035344Z",
          "start_time": "2019-11-10T06:32:42.029953Z"
        },
        "colab_type": "code",
        "id": "4DYWukNFkGQN",
        "outputId": "1d3f4e8f-4018-4eb8-e649-833766143aa8",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "sample_string = 'The target date is 13.11.2019 and amount is 199.55'\n",
        "\n",
        "tokenized_string = tokenizer_ocr.encode(sample_string)\n",
        "print ('Tokenized string is {}'.format(tokenized_string))\n",
        "\n",
        "original_string = tokenizer_ocr.decode(tokenized_string)\n",
        "print ('The original string: {}'.format(original_string))\n",
        "\n",
        "print('Tokens {}'.format([tokenizer_ocr.decode([x]) for x in tokenizer_ocr.encode(original_string)]))\n",
        "\n",
        "assert original_string == sample_string"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized string is [32663, 1102, 20511, 93, 94, 233, 32628, 32630, 32625, 32628, 32628, 32625, 32629, 32627, 32628, 14, 209, 219, 233, 32628, 32636, 32636, 32625, 32632, 32632]\n",
            "The original string: The target date is 13.11.2019 and amount is 199.55\n",
            "Tokens ['T', 'he ', 'targ', 'et ', 'date ', 'is ', '1', '3', '.', '1', '1', '.', '2', '0', '1', '9 ', 'and ', 'amount ', 'is ', '1', '9', '9', '.', '5', '5']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.042778Z",
          "start_time": "2019-11-10T06:32:42.038651Z"
        },
        "colab_type": "code",
        "id": "sfY5_rr-CnVF",
        "outputId": "cb74fbf7-45ea-4fed-92e5-389acd97a5ab",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(tokenizer_amount.vocab_size)\n",
        "print(tokenizer_date.vocab_size)\n",
        "print(tokenizer_ocr.vocab_size)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32835\n",
            "259\n",
            "32835\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o9KJWJjrsZ4Y"
      },
      "source": [
        "The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.051074Z",
          "start_time": "2019-11-10T06:32:42.045507Z"
        },
        "colab_type": "code",
        "id": "bf2ntBxjkqK6",
        "outputId": "5047e88d-7f20-4070-809d-c49b8054c733",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_ocr.decode([ts])))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32663 ----> T\n",
            "1102 ----> he \n",
            "20511 ----> targ\n",
            "93 ----> et \n",
            "94 ----> date \n",
            "233 ----> is \n",
            "32628 ----> 1\n",
            "32630 ----> 3\n",
            "32625 ----> .\n",
            "32628 ----> 1\n",
            "32628 ----> 1\n",
            "32625 ----> .\n",
            "32629 ----> 2\n",
            "32627 ----> 0\n",
            "32628 ----> 1\n",
            "14 ----> 9 \n",
            "209 ----> and \n",
            "219 ----> amount \n",
            "233 ----> is \n",
            "32628 ----> 1\n",
            "32636 ----> 9\n",
            "32636 ----> 9\n",
            "32625 ----> .\n",
            "32632 ----> 5\n",
            "32632 ----> 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.056061Z",
          "start_time": "2019-11-10T06:32:42.053148Z"
        },
        "colab_type": "code",
        "id": "bcRp7VcQ5m6g",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kGi4PoVakxdc"
      },
      "source": [
        "Add a start and end token to the input and target. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.062005Z",
          "start_time": "2019-11-10T06:32:42.057738Z"
        },
        "colab_type": "code",
        "id": "UZwnPr4R055s",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def encode(ocr_param, amount_param, date_param, country_param, num_country_classes):\n",
        "  ocr = [tokenizer_ocr.vocab_size] + tokenizer_ocr.encode(\n",
        "      ocr_param.numpy()) + [tokenizer_ocr.vocab_size+1]\n",
        "\n",
        "  date = [tokenizer_date.vocab_size] + tokenizer_date.encode(\n",
        "      date_param.numpy()) + [tokenizer_date.vocab_size+1]\n",
        "    \n",
        "  amount = [tokenizer_amount.vocab_size] + tokenizer_amount.encode(\n",
        "      amount_param.numpy()) + [tokenizer_amount.vocab_size+1]\n",
        "    \n",
        "  country = tf.one_hot(country_param, num_country_classes)\n",
        "  \n",
        "  return ocr, amount, date, country"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6JrGp5Gek6Ql"
      },
      "source": [
        "Note: To keep this example small and relatively fast, drop examples with a length of over 40 tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.067042Z",
          "start_time": "2019-11-10T06:32:42.064342Z"
        },
        "colab_type": "code",
        "id": "2QEgbjntk6Yf",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 800"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.072228Z",
          "start_time": "2019-11-10T06:32:42.069216Z"
        },
        "colab_type": "code",
        "id": "c081xPGv1CPI",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def filter_max_length(x, y1, y2, y3, max_length=MAX_LENGTH):\n",
        "  return tf.logical_and(tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y1) <= max_length), tf.size(y2) <= max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tx1sFbR-9fRs"
      },
      "source": [
        "Operations inside `.map()` run in graph mode and receive a graph tensor that do not have a numpy attribute. The `tokenizer` expects a string or Unicode symbol to encode it into integers. Hence, you need to run the encoding inside a `tf.py_function`, which receives an eager tensor having a numpy attribute that contains the string value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.077875Z",
          "start_time": "2019-11-10T06:32:42.074724Z"
        },
        "colab_type": "code",
        "id": "Mah1cS-P70Iz",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def tf_encode(ocr, amount, date, country):\n",
        "  num_country_classes = len(country_classes)  \n",
        "  return tf.py_function(encode,\n",
        "                        [ocr, amount, date, country, num_country_classes],\n",
        "                        [tf.int64, tf.int64, tf.int64, tf.float32])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.088195Z",
          "start_time": "2019-11-10T06:32:42.079882Z"
        },
        "colab_type": "code",
        "id": "8iWeIrANuYZ9",
        "outputId": "1aacb48f-df2c-4480-951b-110322e34bd2",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_examples"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: ((), (), (), ()), types: (tf.string, tf.string, tf.string, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.368141Z",
          "start_time": "2019-11-10T06:32:42.089872Z"
        },
        "colab_type": "code",
        "id": "9mk9AZdZ5bcS",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "train_dataset = training_examples \\\n",
        "                .map(tf_encode) \\\n",
        "                .filter(filter_max_length) \\\n",
        "                .cache() \\\n",
        "                .shuffle(BUFFER_SIZE) \\\n",
        "                .padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1], [-1], [-1])) \\\n",
        "                .prefetch(tf.data.experimental.AUTOTUNE) \\\n",
        "\n",
        "valid_dataset = val_examples \\\n",
        "                .map(tf_encode) \\\n",
        "                .filter(filter_max_length) \\\n",
        "                .padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1], [-1], [-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nBQuibYA4n0n"
      },
      "source": [
        "## Positional encoding\n",
        "\n",
        "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n",
        "\n",
        "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
        "\n",
        "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.372964Z",
          "start_time": "2019-11-10T06:32:42.369990Z"
        },
        "colab_type": "code",
        "id": "WhIOZjMNKujn",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.379237Z",
          "start_time": "2019-11-10T06:32:42.374655Z"
        },
        "colab_type": "code",
        "id": "1Rz82wEs5biZ",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.622473Z",
          "start_time": "2019-11-10T06:32:42.381024Z"
        },
        "colab_type": "code",
        "id": "1kLCla68EloE",
        "outputId": "52da4a78-f894-4e53-da03-69414ddec070",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "pos_encoding = positional_encoding(50, 512)\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 50, 512)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a_b4ou4TYqUN"
      },
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s42Uydjkv0hF"
      },
      "source": [
        "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.627869Z",
          "start_time": "2019-11-10T06:32:42.624456Z"
        },
        "colab_type": "code",
        "id": "U2i8-e1s8ti9",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.635845Z",
          "start_time": "2019-11-10T06:32:42.629584Z"
        },
        "colab_type": "code",
        "id": "A7BYeBCNvi7n",
        "outputId": "d639082d-ce68-45aa-c1c4-493591342e4c",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
        "create_padding_mask(x)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
              "array([[[[0., 0., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z0hzukDBgVom"
      },
      "source": [
        "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
        "\n",
        "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.643086Z",
          "start_time": "2019-11-10T06:32:42.639838Z"
        },
        "colab_type": "code",
        "id": "dVxS8OPI9uI0",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.481Z"
        },
        "colab_type": "code",
        "id": "yxKGuXxaBeeE",
        "outputId": "592d5003-1ab6-4385-cc4e-094ba0e3376b",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "x = tf.random.uniform((1, 3))\n",
        "temp = create_look_ahead_mask(x.shape[1])\n",
        "temp"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
              "array([[0., 1., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xluDl5cXYy4y"
      },
      "source": [
        "## Scaled dot product attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vsxEE_-Wa1gF"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
        "\n",
        "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
        "\n",
        "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
        "\n",
        "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax. \n",
        "\n",
        "For example, consider that `Q` and `K` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `Q` and `K` should have a mean of 0 and variance of 1, and you get a gentler softmax.\n",
        "\n",
        "The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.485Z"
        },
        "colab_type": "code",
        "id": "LazzUq3bJ5SH",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FiqETnhCkoXh"
      },
      "source": [
        "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
        "\n",
        "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and the irrelevant words are flushed out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.488Z"
        },
        "colab_type": "code",
        "id": "n90YjClyInFy",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def print_out(q, k, v):\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\n",
        "      q, k, v, None)\n",
        "  print ('Attention weights are:')\n",
        "  print (temp_attn)\n",
        "  print ('Output is:')\n",
        "  print (temp_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.491Z"
        },
        "colab_type": "code",
        "id": "yAzUAf2DPlNt",
        "outputId": "30a152a1-f4c0-4728-cbe0-a1a6b9a63c1b",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "temp_k = tf.constant([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
        "\n",
        "temp_v = tf.constant([[   1,0],\n",
        "                      [  10,0],\n",
        "                      [ 100,5],\n",
        "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
        "\n",
        "# This `query` aligns with the second `key`,\n",
        "# so the second `value` is returned.\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.494Z"
        },
        "colab_type": "code",
        "id": "zg6k-fGhgXra",
        "outputId": "1c864669-b2d0-44a5-a962-db82efc7b06e",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# This query aligns with a repeated key (third and fourth), \n",
        "# so all associated values get averaged.\n",
        "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.496Z"
        },
        "colab_type": "code",
        "id": "UAq3YOzUgXhb",
        "outputId": "a3d4ebef-9ee4-46cd-b754-37354c95f6e1",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# This query aligns equally with the first and second key, \n",
        "# so their values get averaged.\n",
        "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aOz-4_XIhaTP"
      },
      "source": [
        "Pass all the queries together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.499Z"
        },
        "colab_type": "code",
        "id": "6dlU8Tm-hYrF",
        "outputId": "bf33e85d-a61e-4de5-bf9c-13a8dbb5ea01",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor(\n",
            "[[0.  0.  0.5 0.5]\n",
            " [0.  1.  0.  0. ]\n",
            " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor(\n",
            "[[550.    5.5]\n",
            " [ 10.    0. ]\n",
            " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kmzGPEy64qmA"
      },
      "source": [
        "## Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fz5BMC8Kaoqo"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
        "\n",
        "\n",
        "Multi-head attention consists of four parts:\n",
        "*    Linear layers and split into heads.\n",
        "*    Scaled dot-product attention.\n",
        "*    Concatenation of heads.\n",
        "*    Final linear layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JPmbr6F1C-v_"
      },
      "source": [
        "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
        "\n",
        "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
        "\n",
        "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.503Z"
        },
        "colab_type": "code",
        "id": "BSV3PPKsYecw",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0D8FJue5lDyZ"
      },
      "source": [
        "Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.507Z"
        },
        "colab_type": "code",
        "id": "Hu94p-_-2_BX",
        "outputId": "1bb04d09-e0b8-41a1-fb58-c642c74ca73b",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
        "out.shape, attn.shape"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RdDqGayx67vv"
      },
      "source": [
        "## Point wise feed forward network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gBqzJXGfHK3X"
      },
      "source": [
        "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.511Z"
        },
        "colab_type": "code",
        "id": "ET7xLt0yCT6Z",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.514Z"
        },
        "colab_type": "code",
        "id": "mytb1lPyOHLB",
        "outputId": "97fd43d3-f4dc-423f-93a3-92d6c331e3a6",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
        "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7e7hKcxn6-zd"
      },
      "source": [
        "## Encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yScbC0MUH8dS"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfYJG-Kvgwy2"
      },
      "source": [
        "The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n",
        "\n",
        "* The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.\n",
        "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFv-FNYUmvpn"
      },
      "source": [
        "### Encoder layer\n",
        "\n",
        "Each encoder layer consists of sublayers:\n",
        "\n",
        "1.   Multi-head attention (with padding mask) \n",
        "2.    Point wise feed forward networks. \n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
        "\n",
        "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.519Z"
        },
        "colab_type": "code",
        "id": "ncyS-Ms3i2x_",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.521Z"
        },
        "colab_type": "code",
        "id": "AzZRXdO0mI48",
        "outputId": "d6f1d397-1f99-42cc-a17b-85fdd153c583",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 43, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6LO_48Owmx_o"
      },
      "source": [
        "### Decoder layer\n",
        "\n",
        "Each decoder layer consists of sublayers:\n",
        "\n",
        "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
        "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
        "3.   Point wise feed forward networks\n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
        "\n",
        "There are N decoder layers in the transformer.\n",
        "\n",
        "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.524Z"
        },
        "colab_type": "code",
        "id": "9SoX0-vd1hue",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.527Z"
        },
        "colab_type": "code",
        "id": "Ne2Bqx8k71l0",
        "outputId": "0d2b9ee8-bb29-4368-8cb0-51750220c68d",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SE1H51Ajm0q1"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "The `Encoder` consists of:\n",
        "1.   Input Embedding\n",
        "2.   Positional Encoding\n",
        "3.   N encoder layers\n",
        "\n",
        "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.530Z"
        },
        "colab_type": "code",
        "id": "jpEox7gJ8FCI",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.533Z"
        },
        "colab_type": "code",
        "id": "8QG9nueFQKXx",
        "outputId": "819a4b28-bb77-4163-9110-8b55af970e17",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, input_vocab_size=8500,\n",
        "                         maximum_position_encoding=10000)\n",
        "\n",
        "sample_encoder_output = sample_encoder(tf.random.uniform((64, 62)), \n",
        "                                       training=False, mask=None)\n",
        "\n",
        "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 62, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p-uO6ls8m2O5"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZtT7PKzrXkNr"
      },
      "source": [
        " The `Decoder` consists of:\n",
        "1.   Output Embedding\n",
        "2.   Positional Encoding\n",
        "3.   N decoder layers\n",
        "\n",
        "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.537Z"
        },
        "colab_type": "code",
        "id": "d5_d5-PLQXwY",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.540Z"
        },
        "colab_type": "code",
        "id": "a1jXoAMRZyvu",
        "outputId": "aee20668-b407-447b-f96b-d06d3cfc3676",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, target_vocab_size=8000,\n",
        "                         maximum_position_encoding=5000)\n",
        "\n",
        "output, attn = sample_decoder(tf.random.uniform((64, 26)), \n",
        "                              enc_output=sample_encoder_output, \n",
        "                              training=False, look_ahead_mask=None, \n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmFVzdhqpgSR",
        "colab_type": "text"
      },
      "source": [
        "### Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUrwMC03pgSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Classifier(tf.keras.layers.Layer):\n",
        "  def __init__(self, name, num_classes, num_hidden_layers=128, rate=0.1):\n",
        "    super(Classifier, self).__init__()\n",
        "\n",
        "    self.pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    self.hidden_layer = tf.keras.layers.Dense(num_hidden_layers, activation=\"relu\")\n",
        "    self.final_layer = tf.keras.layers.Dense(num_classes, name=name, activation=\"softmax\")\n",
        "\n",
        "  def call(self, enc_output, training):\n",
        "    x = self.pooling(enc_output)\n",
        "    x = self.dropout(x, training=training)\n",
        "    x = self.hidden_layer(x)\n",
        "    y = self.final_layer(x)\n",
        "\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y54xnJnuYgJ7"
      },
      "source": [
        "## Create the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uERO1y54cOKq"
      },
      "source": [
        "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.543Z"
        },
        "colab_type": "code",
        "id": "PED3bIpOYkBu",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               num_layers,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               input_vocab_size, \n",
        "               target_vocab_size_dt,\n",
        "               target_vocab_size_amt,\n",
        "               pe_input,\n",
        "               pe_target_dt,\n",
        "               pe_target_amt,\n",
        "               num_country_classes,\n",
        "               rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                           input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder_dt = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size_dt, pe_target_dt, rate)\n",
        "    \n",
        "    self.decoder_amt = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size_amt, pe_target_amt, rate)\n",
        "    \n",
        "    self.country_classifier = Classifier(name=\"country_classifier\", num_classes=num_country_classes)\n",
        "\n",
        "    self.final_layer_dt = tf.keras.layers.Dense(target_vocab_size_dt)\n",
        "    self.final_layer_amt = tf.keras.layers.Dense(target_vocab_size_amt)\n",
        "    \n",
        "  def call(self,\n",
        "           inp,\n",
        "           tar_dt,\n",
        "           tar_amt,\n",
        "           training,\n",
        "           enc_padding_mask, \n",
        "           look_ahead_mask_dt,\n",
        "           look_ahead_mask_amt,\n",
        "           dec_padding_mask):\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output_dt, attention_weights_dt = self.decoder_dt(\n",
        "        tar_dt, enc_output, training, look_ahead_mask_dt, dec_padding_mask)\n",
        "    \n",
        "    dec_output_amt, attention_weights_amt = self.decoder_amt(\n",
        "        tar_amt, enc_output, training, look_ahead_mask_amt, dec_padding_mask)\n",
        "    \n",
        "    final_output_dt = self.final_layer_dt(dec_output_dt)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    final_output_amt = self.final_layer_amt(dec_output_amt)\n",
        "    final_output_country = self.country_classifier(enc_output, training)    \n",
        "    \n",
        "    return final_output_dt, attention_weights_dt, final_output_amt, attention_weights_amt, final_output_country"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.546Z"
        },
        "colab_type": "code",
        "id": "tJ4fbQcIkHW1",
        "outputId": "d75ce35f-4cdc-4cc2-c6c2-77d8b9404c8c",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
        "    input_vocab_size=8500,\n",
        "    target_vocab_size_dt=2000,\n",
        "    target_vocab_size_amt=8500, \n",
        "    pe_input=10000,\n",
        "    pe_target_dt=6000,\n",
        "    pe_target_amt=6000,\n",
        "    num_country_classes=150)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 62))\n",
        "temp_target_dt = tf.random.uniform((64, 26))\n",
        "temp_target_amt = tf.random.uniform((64, 32))\n",
        "\n",
        "fn_out_dt, _, fn_out_amt, _,  fn_out_country  = sample_transformer(temp_input,\n",
        "                                                                   temp_target_dt,\n",
        "                                                                   temp_target_amt,\n",
        "                                                                   training=False, \n",
        "                                                                   enc_padding_mask=None, \n",
        "                                                                   look_ahead_mask_dt=None,\n",
        "                                                                   look_ahead_mask_amt=None,\n",
        "                                                                   dec_padding_mask=None)\n",
        "\n",
        "print(fn_out_dt.shape)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "print(fn_out_amt.shape)\n",
        "print(fn_out_country.shape)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 26, 2000)\n",
            "(64, 32, 8500)\n",
            "(64, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wsINyf1VEQLC"
      },
      "source": [
        "## Set hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zVjWCxFNcgbt"
      },
      "source": [
        "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
        "\n",
        "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
        "\n",
        "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.549Z"
        },
        "colab_type": "code",
        "id": "lnJn5SLA2ahP",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "num_layers = 2\n",
        "d_model = 128\n",
        "dff = 512\n",
        "num_heads = 4\n",
        "\n",
        "input_vocab_size = tokenizer_ocr.vocab_size + 2\n",
        "target_vocab_size_dt = tokenizer_date.vocab_size + 2\n",
        "target_vocab_size_amt = tokenizer_amount.vocab_size + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xYEGhEOtzn5W"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GOmWW--yP3zx"
      },
      "source": [
        "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.553Z"
        },
        "colab_type": "code",
        "id": "iYQdOO1axwEI",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.555Z"
        },
        "colab_type": "code",
        "id": "7r4scdulztRx",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.558Z"
        },
        "colab_type": "code",
        "id": "f33ZCgvHpPdG",
        "outputId": "2cc5ed09-bcd2-44cb-dfee-7bf679b6a6fe",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YgkDE7hzo8r5"
      },
      "source": [
        "## Loss and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oxGJtoDuYIHL"
      },
      "source": [
        "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.561Z"
        },
        "colab_type": "code",
        "id": "MlhsJMm0TW_B",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "loss_object_seq = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "loss_object_classifier = tf.keras.losses.CategoricalCrossentropy(\n",
        "    from_logits=False, reduction='none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.564Z"
        },
        "colab_type": "code",
        "id": "67oqVHiT0Eiu",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def loss_function_seq(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object_seq(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)\n",
        "\n",
        "def loss_function_classifier(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object_classifier(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  \n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.566Z"
        },
        "colab_type": "code",
        "id": "phlyxMnm-Tpx",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "train_loss_dt = tf.keras.metrics.Mean(name='train_loss_dt')\n",
        "train_loss_amt = tf.keras.metrics.Mean(name='train_loss_amt')\n",
        "train_loss_country = tf.keras.metrics.Mean(name='train_loss_country')\n",
        "train_accuracy_dt = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy_dt')\n",
        "train_accuracy_amt = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy_amt')\n",
        "train_accuracy_country = tf.keras.metrics.CategoricalAccuracy(\n",
        "    name='train_accuracy_country')\n",
        "\n",
        "valid_loss_dt = tf.keras.metrics.Mean(name='valid_loss_dt')\n",
        "valid_loss_amt = tf.keras.metrics.Mean(name='valid_loss_amt')\n",
        "valid_loss_country = tf.keras.metrics.Mean(name='valid_loss_country')\n",
        "valid_accuracy_dt = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='valid_accuracy_dt')\n",
        "valid_accuracy_amt = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='valid_accuracy_amt')\n",
        "valid_accuracy_country = tf.keras.metrics.CategoricalAccuracy(\n",
        "    name='valid_accuracy_country')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aeHumfr7zmMa"
      },
      "source": [
        "## Training and checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.569Z"
        },
        "colab_type": "code",
        "id": "UiysUa--4tOU",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size,\n",
        "                          target_vocab_size_dt,\n",
        "                          target_vocab_size_amt,\n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target_dt=target_vocab_size_dt,\n",
        "                          pe_target_amt=target_vocab_size_amt,\n",
        "                          num_country_classes=len(country_classes),\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.571Z"
        },
        "colab_type": "code",
        "id": "ZOJUSB1T8GjM",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar_dt, tar_amt):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by \n",
        "  # the decoder.\n",
        "  look_ahead_mask_dt = create_look_ahead_mask(tf.shape(tar_dt)[1])\n",
        "  dec_target_padding_mask_dt = create_padding_mask(tar_dt)\n",
        "  combined_mask_dt = tf.maximum(dec_target_padding_mask_dt, look_ahead_mask_dt)\n",
        "\n",
        "  look_ahead_mask_amt = create_look_ahead_mask(tf.shape(tar_amt)[1])\n",
        "  dec_target_padding_mask_amt = create_padding_mask(tar_amt)\n",
        "  combined_mask_amt = tf.maximum(dec_target_padding_mask_amt, look_ahead_mask_amt)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask_dt, dec_padding_mask, combined_mask_amt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fzuf06YZp66w"
      },
      "source": [
        "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.574Z"
        },
        "colab_type": "code",
        "id": "hNhuYfllndLZ",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Di_Yaa1gf9r"
      },
      "source": [
        "The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n",
        "\n",
        "For example, `sentence` = \"SOS A lion in the jungle is sleeping EOS\"\n",
        "\n",
        "`tar_inp` =  \"SOS A lion in the jungle is sleeping\"\n",
        "\n",
        "`tar_real` = \"A lion in the jungle is sleeping EOS\"\n",
        "\n",
        "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n",
        "\n",
        "During training this example uses teacher-forcing (like in the [text generation tutorial](./text_generation.ipynb)). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
        "\n",
        "As the transformer predicts each word, *self-attention* allows it to look at the previous words in the input sequence to better predict the next word.\n",
        "\n",
        "To prevent the model from peaking at the expected output the model uses a look-ahead mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.577Z"
        },
        "colab_type": "code",
        "id": "LKpoA6q1sJFj",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.579Z"
        },
        "colab_type": "code",
        "id": "iJwmp9OE29oj",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.float32),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar_dt, tar_amt, tar_country):\n",
        "  tar_inp_dt = tar_dt[:, :-1]\n",
        "  tar_real_dt = tar_dt[:, 1:]\n",
        "  tar_inp_amt = tar_amt[:, :-1]\n",
        "  tar_real_amt = tar_amt[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask_dt, dec_padding_mask, combined_mask_amt = create_masks(inp, tar_inp_dt, tar_inp_amt)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions_dt, _, predictions_amt, _, predictions_country = transformer(inp,\n",
        "                                                                             tar_inp_dt,\n",
        "                                                                             tar_inp_amt,\n",
        "                                                                             True, \n",
        "                                                                             enc_padding_mask, \n",
        "                                                                             combined_mask_dt,\n",
        "                                                                             combined_mask_amt,\n",
        "                                                                             dec_padding_mask)\n",
        "    loss_dt = loss_function_seq(tar_real_dt, predictions_dt)\n",
        "    loss_amt = loss_function_seq(tar_real_amt, predictions_amt)\n",
        "    loss_country = loss_function_classifier(tar_country, predictions_country)\n",
        "    loss = loss_dt + loss_amt + loss_country\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss_dt(loss_dt)\n",
        "  train_loss_amt(loss_amt)\n",
        "  train_loss_country(loss_country)\n",
        "  train_accuracy_dt(tar_real_dt, predictions_dt)\n",
        "  train_accuracy_amt(tar_real_amt, predictions_amt)\n",
        "  train_accuracy_country(tar_country, predictions_country)\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def valid_epoch(inp, tar_dt, tar_amt, tar_country):\n",
        "  tar_inp_dt = tar_dt[:, :-1]\n",
        "  tar_real_dt = tar_dt[:, 1:]\n",
        "  tar_inp_amt = tar_amt[:, :-1]\n",
        "  tar_real_amt = tar_amt[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask_dt, dec_padding_mask, combined_mask_amt = create_masks(inp,\n",
        "                                                                                         tar_inp_dt,\n",
        "                                                                                         tar_inp_amt)\n",
        "  \n",
        "  predictions_dt, _, predictions_amt, _, predictions_country = transformer(inp, \n",
        "                                                                           tar_inp_dt,\n",
        "                                                                           tar_inp_amt, \n",
        "                                                                           False, \n",
        "                                                                           enc_padding_mask, \n",
        "                                                                           combined_mask_dt,\n",
        "                                                                           combined_mask_amt, \n",
        "                                                                           dec_padding_mask)\n",
        "  loss_dt = loss_function_seq(tar_real_dt, predictions_dt)\n",
        "  loss_amt = loss_function_seq(tar_real_amt, predictions_amt)\n",
        "  loss_country = loss_function_classifier(tar_country, predictions_country)\n",
        "  \n",
        "  valid_loss_dt(loss_dt)\n",
        "  valid_loss_amt(loss_amt)\n",
        "  valid_loss_country(loss_country)\n",
        "  valid_accuracy_dt(tar_real_dt, predictions_dt)\n",
        "  valid_accuracy_amt(tar_real_amt, predictions_amt)\n",
        "  valid_accuracy_country(tar_country, predictions_country)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qM2PDWGDJ_8V"
      },
      "source": [
        "Ocr text is used as the input language and amount is the target language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.582Z"
        },
        "colab_type": "code",
        "id": "bbvmaKNiznHZ",
        "outputId": "3e261b92-1277-4a89-84f0-9ddd652427d1",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss_dt.reset_states()\n",
        "  train_accuracy_dt.reset_states()\n",
        "  train_loss_amt.reset_states()\n",
        "  train_accuracy_amt.reset_states()\n",
        "  train_loss_country.reset_states()\n",
        "  train_accuracy_country.reset_states()\n",
        "\n",
        "  valid_loss_dt.reset_states()\n",
        "  valid_accuracy_dt.reset_states()\n",
        "  valid_loss_amt.reset_states()\n",
        "  valid_accuracy_amt.reset_states()\n",
        "  valid_loss_country.reset_states()\n",
        "  valid_accuracy_country.reset_states()\n",
        "  \n",
        "  # inp -> ocr, tar -> amount, date, country\n",
        "  for (batch, (inp, tar_amt , tar_dt, tar_country)) in enumerate(train_dataset):\n",
        "    train_step(inp, tar_dt, tar_amt, tar_country)\n",
        "    \n",
        "    if batch % 100 == 0:\n",
        "      print (\"\"\"Epoch {} Batch {}  Training Loss Date {:.4f} Training Loss Amount {:.4f} Training Loss Country {:.4f} Accuracy Date {:.4f} Accuracy Amount {:.4f} Accuracy Country {:.4f}\"\"\"\n",
        "             .format(epoch + 1,\n",
        "                      batch,\n",
        "                      train_loss_dt.result(),\n",
        "                      train_loss_amt.result(),\n",
        "                      train_loss_country.result(),\n",
        "                      train_accuracy_dt.result(),\n",
        "                      train_accuracy_amt.result(),\n",
        "                      train_accuracy_country.result()))\n",
        "      \n",
        "  if (epoch + 1) % 1 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "    \n",
        "  print (\"\"\"Epoch {} Training Loss Date {:.4f} Training Loss Amount {:.4f} Training Loss Country {:.4f} Accuracy Date {:.4f} Accuracy Amount {:.4f} Accuracy Country {:.4f}\"\"\"\n",
        "         .format(epoch + 1, \n",
        "                train_loss_dt.result(), \n",
        "                train_loss_amt.result(), \n",
        "                train_loss_country.result(),\n",
        "                train_accuracy_dt.result(),\n",
        "                train_accuracy_amt.result(),\n",
        "                train_accuracy_country.result()))\n",
        "  \n",
        "  for (batch, (inp, tar_amt, tar_dt, tar_country)) in enumerate(valid_dataset):\n",
        "    valid_epoch(inp, tar_dt, tar_amt, tar_country)\n",
        "\n",
        "  print (\"\"\"Epoch {} Validation Loss Date {:.4f} Validation Loss Amount {:.4f} Validation Loss Country {:.4f} Accuracy Date {:.4f} Accuracy Amount {:.4f} Accuracy Country {:.4f}\"\"\"\n",
        "            .format(epoch + 1, \n",
        "                    valid_loss_dt.result(), \n",
        "                    valid_loss_amt.result(), \n",
        "                    valid_loss_country.result(), \n",
        "                    valid_accuracy_dt.result(),\n",
        "                    valid_accuracy_amt.result(),\n",
        "                    valid_accuracy_country.result()))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {:.2f} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0  Training Loss Date 5.8738 Training Loss Amount 4.6624 Training Loss Country 5.4792 Accuracy Date 0.0029 Accuracy Amount 0.0000 Accuracy Country 0.0156\n",
            "Epoch 1 Batch 100  Training Loss Date 4.5900 Training Loss Amount 6.4253 Training Loss Country 4.6579 Accuracy Date 0.1526 Accuracy Amount 0.0433 Accuracy Country 0.1638\n",
            "Epoch 1 Batch 200  Training Loss Date 3.5191 Training Loss Amount 6.1792 Training Loss Country 3.6521 Accuracy Date 0.2950 Accuracy Amount 0.1068 Accuracy Country 0.3060\n",
            "Epoch 1 Batch 300  Training Loss Date 2.6943 Training Loss Amount 5.6752 Training Loss Country 3.1770 Accuracy Date 0.4414 Accuracy Amount 0.1328 Accuracy Country 0.3589\n",
            "Epoch 1 Batch 400  Training Loss Date 2.1844 Training Loss Amount 4.9964 Training Loss Country 2.8402 Accuracy Date 0.5266 Accuracy Amount 0.1549 Accuracy Country 0.4154\n",
            "Epoch 1 Batch 500  Training Loss Date 1.8579 Training Loss Amount 4.3409 Training Loss Country 2.5704 Accuracy Date 0.5787 Accuracy Amount 0.1835 Accuracy Country 0.4677\n",
            "Epoch 1 Batch 600  Training Loss Date 1.6346 Training Loss Amount 3.8057 Training Loss Country 2.3463 Accuracy Date 0.6139 Accuracy Amount 0.2162 Accuracy Country 0.5113\n",
            "Epoch 1 Batch 700  Training Loss Date 1.4732 Training Loss Amount 3.3695 Training Loss Country 2.1728 Accuracy Date 0.6391 Accuracy Amount 0.2431 Accuracy Country 0.5462\n",
            "Epoch 1 Batch 800  Training Loss Date 1.3511 Training Loss Amount 3.0305 Training Loss Country 2.0263 Accuracy Date 0.6580 Accuracy Amount 0.2643 Accuracy Country 0.5756\n",
            "Epoch 1 Batch 900  Training Loss Date 1.2560 Training Loss Amount 2.7652 Training Loss Country 1.8976 Accuracy Date 0.6726 Accuracy Amount 0.2816 Accuracy Country 0.6016\n",
            "Epoch 1 Batch 1000  Training Loss Date 1.1795 Training Loss Amount 2.5511 Training Loss Country 1.7868 Accuracy Date 0.6843 Accuracy Amount 0.2946 Accuracy Country 0.6240\n",
            "Epoch 1 Batch 1100  Training Loss Date 1.1169 Training Loss Amount 2.3745 Training Loss Country 1.6883 Accuracy Date 0.6940 Accuracy Amount 0.3053 Accuracy Country 0.6450\n",
            "Epoch 1 Batch 1200  Training Loss Date 1.0645 Training Loss Amount 2.2268 Training Loss Country 1.6054 Accuracy Date 0.7020 Accuracy Amount 0.3141 Accuracy Country 0.6628\n",
            "Epoch 1 Batch 1300  Training Loss Date 1.0201 Training Loss Amount 2.1023 Training Loss Country 1.5304 Accuracy Date 0.7090 Accuracy Amount 0.3223 Accuracy Country 0.6787\n",
            "Epoch 1 Batch 1400  Training Loss Date 0.9819 Training Loss Amount 1.9941 Training Loss Country 1.4644 Accuracy Date 0.7149 Accuracy Amount 0.3286 Accuracy Country 0.6927\n",
            "Epoch 1 Batch 1500  Training Loss Date 0.9488 Training Loss Amount 1.9014 Training Loss Country 1.4021 Accuracy Date 0.7200 Accuracy Amount 0.3350 Accuracy Country 0.7060\n",
            "Epoch 1 Batch 1600  Training Loss Date 0.9199 Training Loss Amount 1.8198 Training Loss Country 1.3510 Accuracy Date 0.7244 Accuracy Amount 0.3405 Accuracy Country 0.7170\n",
            "Epoch 1 Batch 1700  Training Loss Date 0.8943 Training Loss Amount 1.7478 Training Loss Country 1.3028 Accuracy Date 0.7284 Accuracy Amount 0.3453 Accuracy Country 0.7272\n",
            "Epoch 1 Batch 1800  Training Loss Date 0.8714 Training Loss Amount 1.6835 Training Loss Country 1.2580 Accuracy Date 0.7320 Accuracy Amount 0.3495 Accuracy Country 0.7367\n",
            "Epoch 1 Batch 1900  Training Loss Date 0.8509 Training Loss Amount 1.6262 Training Loss Country 1.2181 Accuracy Date 0.7354 Accuracy Amount 0.3534 Accuracy Country 0.7450\n",
            "Epoch 1 Batch 2000  Training Loss Date 0.8323 Training Loss Amount 1.5746 Training Loss Country 1.1812 Accuracy Date 0.7384 Accuracy Amount 0.3570 Accuracy Country 0.7527\n",
            "Epoch 1 Batch 2100  Training Loss Date 0.8155 Training Loss Amount 1.5273 Training Loss Country 1.1488 Accuracy Date 0.7411 Accuracy Amount 0.3602 Accuracy Country 0.7594\n",
            "Epoch 1 Batch 2200  Training Loss Date 0.8002 Training Loss Amount 1.4840 Training Loss Country 1.1175 Accuracy Date 0.7436 Accuracy Amount 0.3629 Accuracy Country 0.7661\n",
            "Epoch 1 Batch 2300  Training Loss Date 0.7861 Training Loss Amount 1.4456 Training Loss Country 1.0900 Accuracy Date 0.7459 Accuracy Amount 0.3661 Accuracy Country 0.7720\n",
            "Epoch 1 Batch 2400  Training Loss Date 0.7732 Training Loss Amount 1.4098 Training Loss Country 1.0644 Accuracy Date 0.7480 Accuracy Amount 0.3689 Accuracy Country 0.7772\n",
            "Epoch 1 Batch 2500  Training Loss Date 0.7614 Training Loss Amount 1.3756 Training Loss Country 1.0410 Accuracy Date 0.7499 Accuracy Amount 0.3705 Accuracy Country 0.7822\n",
            "Epoch 1 Batch 2600  Training Loss Date 0.7504 Training Loss Amount 1.3450 Training Loss Country 1.0187 Accuracy Date 0.7517 Accuracy Amount 0.3726 Accuracy Country 0.7867\n",
            "Epoch 1 Batch 2700  Training Loss Date 0.7403 Training Loss Amount 1.3162 Training Loss Country 0.9973 Accuracy Date 0.7533 Accuracy Amount 0.3745 Accuracy Country 0.7912\n",
            "Epoch 1 Batch 2800  Training Loss Date 0.7308 Training Loss Amount 1.2896 Training Loss Country 0.9771 Accuracy Date 0.7549 Accuracy Amount 0.3764 Accuracy Country 0.7954\n",
            "Epoch 1 Batch 2900  Training Loss Date 0.7220 Training Loss Amount 1.2649 Training Loss Country 0.9577 Accuracy Date 0.7563 Accuracy Amount 0.3784 Accuracy Country 0.7995\n",
            "Epoch 1 Batch 3000  Training Loss Date 0.7138 Training Loss Amount 1.2416 Training Loss Country 0.9409 Accuracy Date 0.7577 Accuracy Amount 0.3798 Accuracy Country 0.8028\n",
            "Epoch 1 Batch 3100  Training Loss Date 0.7061 Training Loss Amount 1.2189 Training Loss Country 0.9248 Accuracy Date 0.7589 Accuracy Amount 0.3810 Accuracy Country 0.8062\n",
            "Epoch 1 Batch 3200  Training Loss Date 0.6989 Training Loss Amount 1.1983 Training Loss Country 0.9088 Accuracy Date 0.7601 Accuracy Amount 0.3825 Accuracy Country 0.8096\n",
            "Epoch 1 Batch 3300  Training Loss Date 0.6921 Training Loss Amount 1.1795 Training Loss Country 0.8939 Accuracy Date 0.7612 Accuracy Amount 0.3844 Accuracy Country 0.8126\n",
            "Epoch 1 Batch 3400  Training Loss Date 0.6857 Training Loss Amount 1.1611 Training Loss Country 0.8811 Accuracy Date 0.7622 Accuracy Amount 0.3855 Accuracy Country 0.8153\n",
            "Epoch 1 Batch 3500  Training Loss Date 0.6796 Training Loss Amount 1.1439 Training Loss Country 0.8685 Accuracy Date 0.7632 Accuracy Amount 0.3868 Accuracy Country 0.8180\n",
            "Epoch 1 Batch 3600  Training Loss Date 0.6739 Training Loss Amount 1.1272 Training Loss Country 0.8555 Accuracy Date 0.7642 Accuracy Amount 0.3879 Accuracy Country 0.8206\n",
            "Epoch 1 Batch 3700  Training Loss Date 0.6685 Training Loss Amount 1.1116 Training Loss Country 0.8436 Accuracy Date 0.7651 Accuracy Amount 0.3891 Accuracy Country 0.8231\n",
            "Epoch 1 Batch 3800  Training Loss Date 0.6633 Training Loss Amount 1.0965 Training Loss Country 0.8320 Accuracy Date 0.7660 Accuracy Amount 0.3901 Accuracy Country 0.8255\n",
            "Epoch 1 Batch 3900  Training Loss Date 0.6584 Training Loss Amount 1.0825 Training Loss Country 0.8202 Accuracy Date 0.7669 Accuracy Amount 0.3912 Accuracy Country 0.8280\n",
            "Epoch 1 Batch 4000  Training Loss Date 0.6537 Training Loss Amount 1.0694 Training Loss Country 0.8097 Accuracy Date 0.7677 Accuracy Amount 0.3926 Accuracy Country 0.8301\n",
            "Epoch 1 Batch 4100  Training Loss Date 0.6492 Training Loss Amount 1.0562 Training Loss Country 0.7999 Accuracy Date 0.7685 Accuracy Amount 0.3932 Accuracy Country 0.8323\n",
            "Epoch 1 Batch 4200  Training Loss Date 0.6449 Training Loss Amount 1.0438 Training Loss Country 0.7909 Accuracy Date 0.7692 Accuracy Amount 0.3941 Accuracy Country 0.8342\n",
            "Epoch 1 Batch 4300  Training Loss Date 0.6408 Training Loss Amount 1.0320 Training Loss Country 0.7828 Accuracy Date 0.7700 Accuracy Amount 0.3952 Accuracy Country 0.8359\n",
            "Epoch 1 Batch 4400  Training Loss Date 0.6369 Training Loss Amount 1.0206 Training Loss Country 0.7743 Accuracy Date 0.7707 Accuracy Amount 0.3962 Accuracy Country 0.8378\n",
            "Saving checkpoint for epoch 1 at ./checkpoints/train/ckpt-1\n",
            "Epoch 1 Training Loss Date 0.6360 Training Loss Amount 1.0180 Training Loss Country 0.7725 Accuracy Date 0.7708 Accuracy Amount 0.3963 Accuracy Country 0.8382\n",
            "Epoch 1 Validation Loss Date 0.4680 Validation Loss Amount 0.5204 Validation Loss Country 0.3922 Accuracy Date 0.8014 Accuracy Amount 0.4388 Accuracy Country 0.9174\n",
            "Time taken for 1 epoch: 1853.82 secs\n",
            "\n",
            "Epoch 2 Batch 0  Training Loss Date 0.4870 Training Loss Amount 0.5306 Training Loss Country 0.5158 Accuracy Date 0.7959 Accuracy Amount 0.4531 Accuracy Country 0.9219\n",
            "Epoch 2 Batch 100  Training Loss Date 0.4686 Training Loss Amount 0.5268 Training Loss Country 0.4334 Accuracy Date 0.8008 Accuracy Amount 0.4328 Accuracy Country 0.9141\n",
            "Epoch 2 Batch 200  Training Loss Date 0.4687 Training Loss Amount 0.5373 Training Loss Country 0.4022 Accuracy Date 0.8013 Accuracy Amount 0.4437 Accuracy Country 0.9182\n",
            "Epoch 2 Batch 300  Training Loss Date 0.4679 Training Loss Amount 0.5334 Training Loss Country 0.3932 Accuracy Date 0.8017 Accuracy Amount 0.4425 Accuracy Country 0.9207\n",
            "Epoch 2 Batch 400  Training Loss Date 0.4678 Training Loss Amount 0.5314 Training Loss Country 0.3899 Accuracy Date 0.8015 Accuracy Amount 0.4415 Accuracy Country 0.9221\n",
            "Epoch 2 Batch 500  Training Loss Date 0.4677 Training Loss Amount 0.5304 Training Loss Country 0.3901 Accuracy Date 0.8017 Accuracy Amount 0.4412 Accuracy Country 0.9204\n",
            "Epoch 2 Batch 600  Training Loss Date 0.4672 Training Loss Amount 0.5297 Training Loss Country 0.3913 Accuracy Date 0.8020 Accuracy Amount 0.4404 Accuracy Country 0.9194\n",
            "Epoch 2 Batch 700  Training Loss Date 0.4668 Training Loss Amount 0.5262 Training Loss Country 0.3907 Accuracy Date 0.8021 Accuracy Amount 0.4388 Accuracy Country 0.9195\n",
            "Epoch 2 Batch 800  Training Loss Date 0.4665 Training Loss Amount 0.5266 Training Loss Country 0.3889 Accuracy Date 0.8023 Accuracy Amount 0.4400 Accuracy Country 0.9197\n",
            "Epoch 2 Batch 900  Training Loss Date 0.4662 Training Loss Amount 0.5255 Training Loss Country 0.3877 Accuracy Date 0.8025 Accuracy Amount 0.4408 Accuracy Country 0.9199\n",
            "Epoch 2 Batch 1000  Training Loss Date 0.4657 Training Loss Amount 0.5243 Training Loss Country 0.3868 Accuracy Date 0.8028 Accuracy Amount 0.4410 Accuracy Country 0.9195\n",
            "Epoch 2 Batch 1100  Training Loss Date 0.4651 Training Loss Amount 0.5223 Training Loss Country 0.3836 Accuracy Date 0.8030 Accuracy Amount 0.4406 Accuracy Country 0.9200\n",
            "Epoch 2 Batch 1200  Training Loss Date 0.4647 Training Loss Amount 0.5210 Training Loss Country 0.3818 Accuracy Date 0.8033 Accuracy Amount 0.4405 Accuracy Country 0.9201\n",
            "Epoch 2 Batch 1300  Training Loss Date 0.4642 Training Loss Amount 0.5196 Training Loss Country 0.3808 Accuracy Date 0.8036 Accuracy Amount 0.4404 Accuracy Country 0.9198\n",
            "Epoch 2 Batch 1400  Training Loss Date 0.4637 Training Loss Amount 0.5187 Training Loss Country 0.3819 Accuracy Date 0.8039 Accuracy Amount 0.4409 Accuracy Country 0.9197\n",
            "Epoch 2 Batch 1500  Training Loss Date 0.4634 Training Loss Amount 0.5183 Training Loss Country 0.3808 Accuracy Date 0.8041 Accuracy Amount 0.4417 Accuracy Country 0.9198\n",
            "Epoch 2 Batch 1600  Training Loss Date 0.4629 Training Loss Amount 0.5176 Training Loss Country 0.3777 Accuracy Date 0.8044 Accuracy Amount 0.4422 Accuracy Country 0.9204\n",
            "Epoch 2 Batch 1700  Training Loss Date 0.4624 Training Loss Amount 0.5171 Training Loss Country 0.3771 Accuracy Date 0.8046 Accuracy Amount 0.4431 Accuracy Country 0.9205\n",
            "Epoch 2 Batch 1800  Training Loss Date 0.4620 Training Loss Amount 0.5162 Training Loss Country 0.3750 Accuracy Date 0.8049 Accuracy Amount 0.4436 Accuracy Country 0.9208\n",
            "Epoch 2 Batch 1900  Training Loss Date 0.4617 Training Loss Amount 0.5156 Training Loss Country 0.3725 Accuracy Date 0.8051 Accuracy Amount 0.4442 Accuracy Country 0.9213\n",
            "Epoch 2 Batch 2000  Training Loss Date 0.4612 Training Loss Amount 0.5142 Training Loss Country 0.3717 Accuracy Date 0.8053 Accuracy Amount 0.4441 Accuracy Country 0.9214\n",
            "Epoch 2 Batch 2100  Training Loss Date 0.4608 Training Loss Amount 0.5131 Training Loss Country 0.3700 Accuracy Date 0.8056 Accuracy Amount 0.4445 Accuracy Country 0.9218\n",
            "Epoch 2 Batch 2200  Training Loss Date 0.4605 Training Loss Amount 0.5119 Training Loss Country 0.3692 Accuracy Date 0.8058 Accuracy Amount 0.4446 Accuracy Country 0.9220\n",
            "Epoch 2 Batch 2300  Training Loss Date 0.4601 Training Loss Amount 0.5113 Training Loss Country 0.3686 Accuracy Date 0.8059 Accuracy Amount 0.4451 Accuracy Country 0.9221\n",
            "Epoch 2 Batch 2400  Training Loss Date 0.4597 Training Loss Amount 0.5107 Training Loss Country 0.3663 Accuracy Date 0.8061 Accuracy Amount 0.4458 Accuracy Country 0.9225\n",
            "Epoch 2 Batch 2500  Training Loss Date 0.4593 Training Loss Amount 0.5100 Training Loss Country 0.3652 Accuracy Date 0.8063 Accuracy Amount 0.4461 Accuracy Country 0.9228\n",
            "Epoch 2 Batch 2600  Training Loss Date 0.4590 Training Loss Amount 0.5094 Training Loss Country 0.3639 Accuracy Date 0.8065 Accuracy Amount 0.4464 Accuracy Country 0.9232\n",
            "Epoch 2 Batch 2700  Training Loss Date 0.4586 Training Loss Amount 0.5085 Training Loss Country 0.3633 Accuracy Date 0.8068 Accuracy Amount 0.4467 Accuracy Country 0.9233\n",
            "Epoch 2 Batch 2800  Training Loss Date 0.4582 Training Loss Amount 0.5075 Training Loss Country 0.3623 Accuracy Date 0.8070 Accuracy Amount 0.4469 Accuracy Country 0.9235\n",
            "Epoch 2 Batch 2900  Training Loss Date 0.4579 Training Loss Amount 0.5067 Training Loss Country 0.3605 Accuracy Date 0.8071 Accuracy Amount 0.4470 Accuracy Country 0.9239\n",
            "Epoch 2 Batch 3000  Training Loss Date 0.4576 Training Loss Amount 0.5059 Training Loss Country 0.3591 Accuracy Date 0.8073 Accuracy Amount 0.4474 Accuracy Country 0.9241\n",
            "Epoch 2 Batch 3100  Training Loss Date 0.4572 Training Loss Amount 0.5054 Training Loss Country 0.3582 Accuracy Date 0.8075 Accuracy Amount 0.4480 Accuracy Country 0.9243\n",
            "Epoch 2 Batch 3200  Training Loss Date 0.4568 Training Loss Amount 0.5043 Training Loss Country 0.3570 Accuracy Date 0.8077 Accuracy Amount 0.4482 Accuracy Country 0.9245\n",
            "Epoch 2 Batch 3300  Training Loss Date 0.4565 Training Loss Amount 0.5033 Training Loss Country 0.3556 Accuracy Date 0.8079 Accuracy Amount 0.4484 Accuracy Country 0.9247\n",
            "Epoch 2 Batch 3400  Training Loss Date 0.4561 Training Loss Amount 0.5024 Training Loss Country 0.3551 Accuracy Date 0.8081 Accuracy Amount 0.4486 Accuracy Country 0.9249\n",
            "Epoch 2 Batch 3500  Training Loss Date 0.4558 Training Loss Amount 0.5014 Training Loss Country 0.3536 Accuracy Date 0.8083 Accuracy Amount 0.4486 Accuracy Country 0.9251\n",
            "Epoch 2 Batch 3600  Training Loss Date 0.4554 Training Loss Amount 0.5006 Training Loss Country 0.3518 Accuracy Date 0.8085 Accuracy Amount 0.4491 Accuracy Country 0.9254\n",
            "Epoch 2 Batch 3700  Training Loss Date 0.4551 Training Loss Amount 0.4995 Training Loss Country 0.3512 Accuracy Date 0.8087 Accuracy Amount 0.4489 Accuracy Country 0.9256\n",
            "Epoch 2 Batch 3800  Training Loss Date 0.4547 Training Loss Amount 0.4987 Training Loss Country 0.3502 Accuracy Date 0.8089 Accuracy Amount 0.4491 Accuracy Country 0.9259\n",
            "Epoch 2 Batch 3900  Training Loss Date 0.4543 Training Loss Amount 0.4976 Training Loss Country 0.3492 Accuracy Date 0.8091 Accuracy Amount 0.4490 Accuracy Country 0.9261\n",
            "Epoch 2 Batch 4000  Training Loss Date 0.4539 Training Loss Amount 0.4971 Training Loss Country 0.3483 Accuracy Date 0.8093 Accuracy Amount 0.4492 Accuracy Country 0.9263\n",
            "Epoch 2 Batch 4100  Training Loss Date 0.4535 Training Loss Amount 0.4963 Training Loss Country 0.3473 Accuracy Date 0.8095 Accuracy Amount 0.4494 Accuracy Country 0.9265\n",
            "Epoch 2 Batch 4200  Training Loss Date 0.4532 Training Loss Amount 0.4957 Training Loss Country 0.3462 Accuracy Date 0.8097 Accuracy Amount 0.4497 Accuracy Country 0.9268\n",
            "Epoch 2 Batch 4300  Training Loss Date 0.4527 Training Loss Amount 0.4949 Training Loss Country 0.3449 Accuracy Date 0.8099 Accuracy Amount 0.4499 Accuracy Country 0.9271\n",
            "Epoch 2 Batch 4400  Training Loss Date 0.4523 Training Loss Amount 0.4941 Training Loss Country 0.3441 Accuracy Date 0.8102 Accuracy Amount 0.4502 Accuracy Country 0.9272\n",
            "Saving checkpoint for epoch 2 at ./checkpoints/train/ckpt-2\n",
            "Epoch 2 Training Loss Date 0.4522 Training Loss Amount 0.4941 Training Loss Country 0.3438 Accuracy Date 0.8102 Accuracy Amount 0.4504 Accuracy Country 0.9273\n",
            "Epoch 2 Validation Loss Date 0.4240 Validation Loss Amount 0.4550 Validation Loss Country 0.3468 Accuracy Date 0.8240 Accuracy Amount 0.4596 Accuracy Country 0.9289\n",
            "Time taken for 1 epoch: 1420.21 secs\n",
            "\n",
            "Epoch 3 Batch 0  Training Loss Date 0.4416 Training Loss Amount 0.4198 Training Loss Country 0.1299 Accuracy Date 0.8145 Accuracy Amount 0.4896 Accuracy Country 0.9844\n",
            "Epoch 3 Batch 100  Training Loss Date 0.4334 Training Loss Amount 0.4634 Training Loss Country 0.2985 Accuracy Date 0.8197 Accuracy Amount 0.4538 Accuracy Country 0.9383\n",
            "Epoch 3 Batch 200  Training Loss Date 0.4321 Training Loss Amount 0.4618 Training Loss Country 0.3058 Accuracy Date 0.8203 Accuracy Amount 0.4573 Accuracy Country 0.9357\n",
            "Epoch 3 Batch 300  Training Loss Date 0.4310 Training Loss Amount 0.4642 Training Loss Country 0.3010 Accuracy Date 0.8212 Accuracy Amount 0.4586 Accuracy Country 0.9369\n",
            "Epoch 3 Batch 400  Training Loss Date 0.4300 Training Loss Amount 0.4616 Training Loss Country 0.3058 Accuracy Date 0.8219 Accuracy Amount 0.4559 Accuracy Country 0.9362\n",
            "Epoch 3 Batch 500  Training Loss Date 0.4288 Training Loss Amount 0.4630 Training Loss Country 0.3026 Accuracy Date 0.8223 Accuracy Amount 0.4586 Accuracy Country 0.9358\n",
            "Epoch 3 Batch 600  Training Loss Date 0.4277 Training Loss Amount 0.4614 Training Loss Country 0.3007 Accuracy Date 0.8228 Accuracy Amount 0.4584 Accuracy Country 0.9363\n",
            "Epoch 3 Batch 700  Training Loss Date 0.4268 Training Loss Amount 0.4601 Training Loss Country 0.3014 Accuracy Date 0.8232 Accuracy Amount 0.4579 Accuracy Country 0.9361\n",
            "Epoch 3 Batch 800  Training Loss Date 0.4262 Training Loss Amount 0.4603 Training Loss Country 0.3000 Accuracy Date 0.8235 Accuracy Amount 0.4591 Accuracy Country 0.9366\n",
            "Epoch 3 Batch 900  Training Loss Date 0.4251 Training Loss Amount 0.4597 Training Loss Country 0.2972 Accuracy Date 0.8240 Accuracy Amount 0.4601 Accuracy Country 0.9368\n",
            "Epoch 3 Batch 1000  Training Loss Date 0.4241 Training Loss Amount 0.4599 Training Loss Country 0.2984 Accuracy Date 0.8244 Accuracy Amount 0.4612 Accuracy Country 0.9367\n",
            "Epoch 3 Batch 1100  Training Loss Date 0.4230 Training Loss Amount 0.4592 Training Loss Country 0.2971 Accuracy Date 0.8249 Accuracy Amount 0.4617 Accuracy Country 0.9368\n",
            "Epoch 3 Batch 1200  Training Loss Date 0.4219 Training Loss Amount 0.4578 Training Loss Country 0.2963 Accuracy Date 0.8255 Accuracy Amount 0.4618 Accuracy Country 0.9369\n",
            "Epoch 3 Batch 1300  Training Loss Date 0.4210 Training Loss Amount 0.4570 Training Loss Country 0.2966 Accuracy Date 0.8260 Accuracy Amount 0.4619 Accuracy Country 0.9367\n",
            "Epoch 3 Batch 1400  Training Loss Date 0.4200 Training Loss Amount 0.4572 Training Loss Country 0.2952 Accuracy Date 0.8264 Accuracy Amount 0.4629 Accuracy Country 0.9367\n",
            "Epoch 3 Batch 1500  Training Loss Date 0.4191 Training Loss Amount 0.4565 Training Loss Country 0.2948 Accuracy Date 0.8269 Accuracy Amount 0.4635 Accuracy Country 0.9367\n",
            "Epoch 3 Batch 1600  Training Loss Date 0.4181 Training Loss Amount 0.4554 Training Loss Country 0.2946 Accuracy Date 0.8274 Accuracy Amount 0.4632 Accuracy Country 0.9365\n",
            "Epoch 3 Batch 1700  Training Loss Date 0.4170 Training Loss Amount 0.4542 Training Loss Country 0.2925 Accuracy Date 0.8279 Accuracy Amount 0.4638 Accuracy Country 0.9370\n",
            "Epoch 3 Batch 1800  Training Loss Date 0.4159 Training Loss Amount 0.4533 Training Loss Country 0.2924 Accuracy Date 0.8284 Accuracy Amount 0.4639 Accuracy Country 0.9370\n",
            "Epoch 3 Batch 1900  Training Loss Date 0.4149 Training Loss Amount 0.4524 Training Loss Country 0.2915 Accuracy Date 0.8289 Accuracy Amount 0.4640 Accuracy Country 0.9372\n",
            "Epoch 3 Batch 2000  Training Loss Date 0.4138 Training Loss Amount 0.4518 Training Loss Country 0.2908 Accuracy Date 0.8294 Accuracy Amount 0.4644 Accuracy Country 0.9375\n",
            "Epoch 3 Batch 2100  Training Loss Date 0.4128 Training Loss Amount 0.4509 Training Loss Country 0.2902 Accuracy Date 0.8298 Accuracy Amount 0.4648 Accuracy Country 0.9377\n",
            "Epoch 3 Batch 2200  Training Loss Date 0.4119 Training Loss Amount 0.4499 Training Loss Country 0.2894 Accuracy Date 0.8303 Accuracy Amount 0.4656 Accuracy Country 0.9377\n",
            "Epoch 3 Batch 2300  Training Loss Date 0.4108 Training Loss Amount 0.4488 Training Loss Country 0.2885 Accuracy Date 0.8308 Accuracy Amount 0.4655 Accuracy Country 0.9379\n",
            "Epoch 3 Batch 2400  Training Loss Date 0.4098 Training Loss Amount 0.4480 Training Loss Country 0.2883 Accuracy Date 0.8312 Accuracy Amount 0.4659 Accuracy Country 0.9380\n",
            "Epoch 3 Batch 2500  Training Loss Date 0.4089 Training Loss Amount 0.4473 Training Loss Country 0.2879 Accuracy Date 0.8317 Accuracy Amount 0.4669 Accuracy Country 0.9380\n",
            "Epoch 3 Batch 2600  Training Loss Date 0.4079 Training Loss Amount 0.4464 Training Loss Country 0.2879 Accuracy Date 0.8321 Accuracy Amount 0.4673 Accuracy Country 0.9381\n",
            "Epoch 3 Batch 2700  Training Loss Date 0.4069 Training Loss Amount 0.4455 Training Loss Country 0.2877 Accuracy Date 0.8326 Accuracy Amount 0.4679 Accuracy Country 0.9382\n",
            "Epoch 3 Batch 2800  Training Loss Date 0.4060 Training Loss Amount 0.4444 Training Loss Country 0.2876 Accuracy Date 0.8331 Accuracy Amount 0.4680 Accuracy Country 0.9383\n",
            "Epoch 3 Batch 2900  Training Loss Date 0.4050 Training Loss Amount 0.4435 Training Loss Country 0.2879 Accuracy Date 0.8335 Accuracy Amount 0.4684 Accuracy Country 0.9383\n",
            "Epoch 3 Batch 3000  Training Loss Date 0.4040 Training Loss Amount 0.4423 Training Loss Country 0.2868 Accuracy Date 0.8340 Accuracy Amount 0.4687 Accuracy Country 0.9385\n",
            "Epoch 3 Batch 3100  Training Loss Date 0.4029 Training Loss Amount 0.4409 Training Loss Country 0.2856 Accuracy Date 0.8345 Accuracy Amount 0.4690 Accuracy Country 0.9387\n",
            "Epoch 3 Batch 3200  Training Loss Date 0.4019 Training Loss Amount 0.4399 Training Loss Country 0.2852 Accuracy Date 0.8350 Accuracy Amount 0.4694 Accuracy Country 0.9388\n",
            "Epoch 3 Batch 3300  Training Loss Date 0.4009 Training Loss Amount 0.4391 Training Loss Country 0.2841 Accuracy Date 0.8354 Accuracy Amount 0.4703 Accuracy Country 0.9390\n",
            "Epoch 3 Batch 3400  Training Loss Date 0.3998 Training Loss Amount 0.4380 Training Loss Country 0.2837 Accuracy Date 0.8359 Accuracy Amount 0.4707 Accuracy Country 0.9391\n",
            "Epoch 3 Batch 3500  Training Loss Date 0.3989 Training Loss Amount 0.4369 Training Loss Country 0.2823 Accuracy Date 0.8364 Accuracy Amount 0.4711 Accuracy Country 0.9393\n",
            "Epoch 3 Batch 3600  Training Loss Date 0.3978 Training Loss Amount 0.4358 Training Loss Country 0.2812 Accuracy Date 0.8369 Accuracy Amount 0.4712 Accuracy Country 0.9395\n",
            "Epoch 3 Batch 3700  Training Loss Date 0.3968 Training Loss Amount 0.4347 Training Loss Country 0.2808 Accuracy Date 0.8373 Accuracy Amount 0.4715 Accuracy Country 0.9396\n",
            "Epoch 3 Batch 3800  Training Loss Date 0.3957 Training Loss Amount 0.4333 Training Loss Country 0.2801 Accuracy Date 0.8378 Accuracy Amount 0.4713 Accuracy Country 0.9398\n",
            "Epoch 3 Batch 3900  Training Loss Date 0.3946 Training Loss Amount 0.4326 Training Loss Country 0.2795 Accuracy Date 0.8384 Accuracy Amount 0.4718 Accuracy Country 0.9399\n",
            "Epoch 3 Batch 4000  Training Loss Date 0.3935 Training Loss Amount 0.4316 Training Loss Country 0.2787 Accuracy Date 0.8389 Accuracy Amount 0.4720 Accuracy Country 0.9402\n",
            "Epoch 3 Batch 4100  Training Loss Date 0.3922 Training Loss Amount 0.4305 Training Loss Country 0.2783 Accuracy Date 0.8395 Accuracy Amount 0.4723 Accuracy Country 0.9403\n",
            "Epoch 3 Batch 4200  Training Loss Date 0.3909 Training Loss Amount 0.4294 Training Loss Country 0.2781 Accuracy Date 0.8401 Accuracy Amount 0.4726 Accuracy Country 0.9403\n",
            "Epoch 3 Batch 4300  Training Loss Date 0.3897 Training Loss Amount 0.4284 Training Loss Country 0.2778 Accuracy Date 0.8406 Accuracy Amount 0.4730 Accuracy Country 0.9404\n",
            "Epoch 3 Batch 4400  Training Loss Date 0.3884 Training Loss Amount 0.4272 Training Loss Country 0.2775 Accuracy Date 0.8412 Accuracy Amount 0.4731 Accuracy Country 0.9405\n",
            "Saving checkpoint for epoch 3 at ./checkpoints/train/ckpt-3\n",
            "Epoch 3 Training Loss Date 0.3881 Training Loss Amount 0.4269 Training Loss Country 0.2773 Accuracy Date 0.8414 Accuracy Amount 0.4732 Accuracy Country 0.9405\n",
            "Epoch 3 Validation Loss Date 0.2961 Validation Loss Amount 0.3710 Validation Loss Country 0.3701 Accuracy Date 0.8833 Accuracy Amount 0.4921 Accuracy Country 0.9317\n",
            "Time taken for 1 epoch: 1423.97 secs\n",
            "\n",
            "Epoch 4 Batch 0  Training Loss Date 0.3279 Training Loss Amount 0.3214 Training Loss Country 0.1899 Accuracy Date 0.8691 Accuracy Amount 0.4498 Accuracy Country 0.9531\n",
            "Epoch 4 Batch 100  Training Loss Date 0.3286 Training Loss Amount 0.3763 Training Loss Country 0.2570 Accuracy Date 0.8684 Accuracy Amount 0.4823 Accuracy Country 0.9482\n",
            "Epoch 4 Batch 200  Training Loss Date 0.3260 Training Loss Amount 0.3762 Training Loss Country 0.2702 Accuracy Date 0.8700 Accuracy Amount 0.4868 Accuracy Country 0.9453\n",
            "Epoch 4 Batch 300  Training Loss Date 0.3244 Training Loss Amount 0.3756 Training Loss Country 0.2632 Accuracy Date 0.8710 Accuracy Amount 0.4895 Accuracy Country 0.9459\n",
            "Epoch 4 Batch 400  Training Loss Date 0.3227 Training Loss Amount 0.3780 Training Loss Country 0.2567 Accuracy Date 0.8716 Accuracy Amount 0.4922 Accuracy Country 0.9465\n",
            "Epoch 4 Batch 500  Training Loss Date 0.3215 Training Loss Amount 0.3774 Training Loss Country 0.2589 Accuracy Date 0.8724 Accuracy Amount 0.4932 Accuracy Country 0.9460\n",
            "Epoch 4 Batch 600  Training Loss Date 0.3207 Training Loss Amount 0.3758 Training Loss Country 0.2602 Accuracy Date 0.8726 Accuracy Amount 0.4925 Accuracy Country 0.9456\n",
            "Epoch 4 Batch 700  Training Loss Date 0.3194 Training Loss Amount 0.3752 Training Loss Country 0.2570 Accuracy Date 0.8733 Accuracy Amount 0.4922 Accuracy Country 0.9460\n",
            "Epoch 4 Batch 800  Training Loss Date 0.3180 Training Loss Amount 0.3757 Training Loss Country 0.2556 Accuracy Date 0.8739 Accuracy Amount 0.4933 Accuracy Country 0.9460\n",
            "Epoch 4 Batch 900  Training Loss Date 0.3164 Training Loss Amount 0.3731 Training Loss Country 0.2552 Accuracy Date 0.8746 Accuracy Amount 0.4919 Accuracy Country 0.9460\n",
            "Epoch 4 Batch 1000  Training Loss Date 0.3148 Training Loss Amount 0.3726 Training Loss Country 0.2564 Accuracy Date 0.8753 Accuracy Amount 0.4927 Accuracy Country 0.9457\n",
            "Epoch 4 Batch 1100  Training Loss Date 0.3132 Training Loss Amount 0.3705 Training Loss Country 0.2540 Accuracy Date 0.8760 Accuracy Amount 0.4927 Accuracy Country 0.9460\n",
            "Epoch 4 Batch 1200  Training Loss Date 0.3117 Training Loss Amount 0.3692 Training Loss Country 0.2541 Accuracy Date 0.8767 Accuracy Amount 0.4928 Accuracy Country 0.9461\n",
            "Epoch 4 Batch 1300  Training Loss Date 0.3099 Training Loss Amount 0.3682 Training Loss Country 0.2540 Accuracy Date 0.8775 Accuracy Amount 0.4937 Accuracy Country 0.9462\n",
            "Epoch 4 Batch 1400  Training Loss Date 0.3083 Training Loss Amount 0.3671 Training Loss Country 0.2544 Accuracy Date 0.8782 Accuracy Amount 0.4943 Accuracy Country 0.9461\n",
            "Epoch 4 Batch 1500  Training Loss Date 0.3066 Training Loss Amount 0.3661 Training Loss Country 0.2541 Accuracy Date 0.8789 Accuracy Amount 0.4950 Accuracy Country 0.9459\n",
            "Epoch 4 Batch 1600  Training Loss Date 0.3053 Training Loss Amount 0.3646 Training Loss Country 0.2539 Accuracy Date 0.8796 Accuracy Amount 0.4954 Accuracy Country 0.9459\n",
            "Epoch 4 Batch 1700  Training Loss Date 0.3039 Training Loss Amount 0.3631 Training Loss Country 0.2528 Accuracy Date 0.8803 Accuracy Amount 0.4956 Accuracy Country 0.9464\n",
            "Epoch 4 Batch 1800  Training Loss Date 0.3026 Training Loss Amount 0.3620 Training Loss Country 0.2514 Accuracy Date 0.8808 Accuracy Amount 0.4957 Accuracy Country 0.9465\n",
            "Epoch 4 Batch 1900  Training Loss Date 0.3011 Training Loss Amount 0.3612 Training Loss Country 0.2511 Accuracy Date 0.8815 Accuracy Amount 0.4969 Accuracy Country 0.9466\n",
            "Epoch 4 Batch 2000  Training Loss Date 0.2996 Training Loss Amount 0.3603 Training Loss Country 0.2500 Accuracy Date 0.8821 Accuracy Amount 0.4979 Accuracy Country 0.9467\n",
            "Epoch 4 Batch 2100  Training Loss Date 0.2981 Training Loss Amount 0.3589 Training Loss Country 0.2492 Accuracy Date 0.8828 Accuracy Amount 0.4980 Accuracy Country 0.9469\n",
            "Epoch 4 Batch 2200  Training Loss Date 0.2966 Training Loss Amount 0.3576 Training Loss Country 0.2494 Accuracy Date 0.8834 Accuracy Amount 0.4981 Accuracy Country 0.9469\n",
            "Epoch 4 Batch 2300  Training Loss Date 0.2951 Training Loss Amount 0.3570 Training Loss Country 0.2487 Accuracy Date 0.8840 Accuracy Amount 0.4988 Accuracy Country 0.9470\n",
            "Epoch 4 Batch 2400  Training Loss Date 0.2937 Training Loss Amount 0.3557 Training Loss Country 0.2485 Accuracy Date 0.8847 Accuracy Amount 0.4993 Accuracy Country 0.9469\n",
            "Epoch 4 Batch 2500  Training Loss Date 0.2925 Training Loss Amount 0.3549 Training Loss Country 0.2480 Accuracy Date 0.8852 Accuracy Amount 0.4999 Accuracy Country 0.9472\n",
            "Epoch 4 Batch 2600  Training Loss Date 0.2912 Training Loss Amount 0.3539 Training Loss Country 0.2487 Accuracy Date 0.8858 Accuracy Amount 0.5001 Accuracy Country 0.9469\n",
            "Epoch 4 Batch 2700  Training Loss Date 0.2899 Training Loss Amount 0.3530 Training Loss Country 0.2488 Accuracy Date 0.8863 Accuracy Amount 0.5004 Accuracy Country 0.9468\n",
            "Epoch 4 Batch 2800  Training Loss Date 0.2885 Training Loss Amount 0.3518 Training Loss Country 0.2488 Accuracy Date 0.8869 Accuracy Amount 0.5007 Accuracy Country 0.9468\n",
            "Epoch 4 Batch 2900  Training Loss Date 0.2872 Training Loss Amount 0.3509 Training Loss Country 0.2478 Accuracy Date 0.8875 Accuracy Amount 0.5013 Accuracy Country 0.9470\n",
            "Epoch 4 Batch 3000  Training Loss Date 0.2861 Training Loss Amount 0.3496 Training Loss Country 0.2476 Accuracy Date 0.8880 Accuracy Amount 0.5013 Accuracy Country 0.9471\n",
            "Epoch 4 Batch 3100  Training Loss Date 0.2847 Training Loss Amount 0.3485 Training Loss Country 0.2470 Accuracy Date 0.8886 Accuracy Amount 0.5018 Accuracy Country 0.9471\n",
            "Epoch 4 Batch 3200  Training Loss Date 0.2835 Training Loss Amount 0.3478 Training Loss Country 0.2466 Accuracy Date 0.8891 Accuracy Amount 0.5021 Accuracy Country 0.9472\n",
            "Epoch 4 Batch 3300  Training Loss Date 0.2823 Training Loss Amount 0.3468 Training Loss Country 0.2462 Accuracy Date 0.8896 Accuracy Amount 0.5025 Accuracy Country 0.9473\n",
            "Epoch 4 Batch 3400  Training Loss Date 0.2810 Training Loss Amount 0.3456 Training Loss Country 0.2463 Accuracy Date 0.8901 Accuracy Amount 0.5028 Accuracy Country 0.9473\n",
            "Epoch 4 Batch 3500  Training Loss Date 0.2799 Training Loss Amount 0.3447 Training Loss Country 0.2454 Accuracy Date 0.8906 Accuracy Amount 0.5031 Accuracy Country 0.9475\n",
            "Epoch 4 Batch 3600  Training Loss Date 0.2787 Training Loss Amount 0.3437 Training Loss Country 0.2452 Accuracy Date 0.8911 Accuracy Amount 0.5033 Accuracy Country 0.9475\n",
            "Epoch 4 Batch 3700  Training Loss Date 0.2775 Training Loss Amount 0.3427 Training Loss Country 0.2446 Accuracy Date 0.8916 Accuracy Amount 0.5034 Accuracy Country 0.9476\n",
            "Epoch 4 Batch 3800  Training Loss Date 0.2764 Training Loss Amount 0.3417 Training Loss Country 0.2439 Accuracy Date 0.8921 Accuracy Amount 0.5036 Accuracy Country 0.9477\n",
            "Epoch 4 Batch 3900  Training Loss Date 0.2753 Training Loss Amount 0.3405 Training Loss Country 0.2440 Accuracy Date 0.8926 Accuracy Amount 0.5035 Accuracy Country 0.9477\n",
            "Epoch 4 Batch 4000  Training Loss Date 0.2741 Training Loss Amount 0.3396 Training Loss Country 0.2431 Accuracy Date 0.8931 Accuracy Amount 0.5040 Accuracy Country 0.9479\n",
            "Epoch 4 Batch 4100  Training Loss Date 0.2731 Training Loss Amount 0.3386 Training Loss Country 0.2425 Accuracy Date 0.8936 Accuracy Amount 0.5043 Accuracy Country 0.9480\n",
            "Epoch 4 Batch 4200  Training Loss Date 0.2719 Training Loss Amount 0.3380 Training Loss Country 0.2423 Accuracy Date 0.8941 Accuracy Amount 0.5048 Accuracy Country 0.9481\n",
            "Epoch 4 Batch 4300  Training Loss Date 0.2708 Training Loss Amount 0.3371 Training Loss Country 0.2417 Accuracy Date 0.8945 Accuracy Amount 0.5053 Accuracy Country 0.9482\n",
            "Epoch 4 Batch 4400  Training Loss Date 0.2698 Training Loss Amount 0.3361 Training Loss Country 0.2415 Accuracy Date 0.8949 Accuracy Amount 0.5057 Accuracy Country 0.9483\n",
            "Saving checkpoint for epoch 4 at ./checkpoints/train/ckpt-4\n",
            "Epoch 4 Training Loss Date 0.2695 Training Loss Amount 0.3358 Training Loss Country 0.2412 Accuracy Date 0.8950 Accuracy Amount 0.5057 Accuracy Country 0.9483\n",
            "Epoch 4 Validation Loss Date 0.1973 Validation Loss Amount 0.2691 Validation Loss Country 0.3628 Accuracy Date 0.9261 Accuracy Amount 0.5297 Accuracy Country 0.9342\n",
            "Time taken for 1 epoch: 1417.59 secs\n",
            "\n",
            "Epoch 5 Batch 0  Training Loss Date 0.2174 Training Loss Amount 0.2936 Training Loss Country 0.1347 Accuracy Date 0.9199 Accuracy Amount 0.4621 Accuracy Country 0.9531\n",
            "Epoch 5 Batch 100  Training Loss Date 0.2199 Training Loss Amount 0.2898 Training Loss Country 0.2311 Accuracy Date 0.9154 Accuracy Amount 0.5128 Accuracy Country 0.9503\n",
            "Epoch 5 Batch 200  Training Loss Date 0.2194 Training Loss Amount 0.2883 Training Loss Country 0.2369 Accuracy Date 0.9158 Accuracy Amount 0.5193 Accuracy Country 0.9490\n",
            "Epoch 5 Batch 300  Training Loss Date 0.2184 Training Loss Amount 0.2885 Training Loss Country 0.2386 Accuracy Date 0.9164 Accuracy Amount 0.5193 Accuracy Country 0.9487\n",
            "Epoch 5 Batch 400  Training Loss Date 0.2170 Training Loss Amount 0.2875 Training Loss Country 0.2344 Accuracy Date 0.9171 Accuracy Amount 0.5216 Accuracy Country 0.9500\n",
            "Epoch 5 Batch 500  Training Loss Date 0.2168 Training Loss Amount 0.2881 Training Loss Country 0.2314 Accuracy Date 0.9173 Accuracy Amount 0.5236 Accuracy Country 0.9508\n",
            "Epoch 5 Batch 600  Training Loss Date 0.2166 Training Loss Amount 0.2880 Training Loss Country 0.2279 Accuracy Date 0.9173 Accuracy Amount 0.5242 Accuracy Country 0.9511\n",
            "Epoch 5 Batch 700  Training Loss Date 0.2165 Training Loss Amount 0.2879 Training Loss Country 0.2279 Accuracy Date 0.9173 Accuracy Amount 0.5252 Accuracy Country 0.9514\n",
            "Epoch 5 Batch 800  Training Loss Date 0.2160 Training Loss Amount 0.2856 Training Loss Country 0.2274 Accuracy Date 0.9175 Accuracy Amount 0.5235 Accuracy Country 0.9513\n",
            "Epoch 5 Batch 900  Training Loss Date 0.2154 Training Loss Amount 0.2848 Training Loss Country 0.2266 Accuracy Date 0.9177 Accuracy Amount 0.5235 Accuracy Country 0.9515\n",
            "Epoch 5 Batch 1000  Training Loss Date 0.2145 Training Loss Amount 0.2831 Training Loss Country 0.2271 Accuracy Date 0.9180 Accuracy Amount 0.5240 Accuracy Country 0.9514\n",
            "Epoch 5 Batch 1100  Training Loss Date 0.2142 Training Loss Amount 0.2821 Training Loss Country 0.2276 Accuracy Date 0.9181 Accuracy Amount 0.5236 Accuracy Country 0.9512\n",
            "Epoch 5 Batch 1200  Training Loss Date 0.2136 Training Loss Amount 0.2805 Training Loss Country 0.2259 Accuracy Date 0.9183 Accuracy Amount 0.5233 Accuracy Country 0.9513\n",
            "Epoch 5 Batch 1300  Training Loss Date 0.2129 Training Loss Amount 0.2800 Training Loss Country 0.2252 Accuracy Date 0.9186 Accuracy Amount 0.5244 Accuracy Country 0.9514\n",
            "Epoch 5 Batch 1400  Training Loss Date 0.2121 Training Loss Amount 0.2787 Training Loss Country 0.2259 Accuracy Date 0.9190 Accuracy Amount 0.5241 Accuracy Country 0.9513\n",
            "Epoch 5 Batch 1500  Training Loss Date 0.2113 Training Loss Amount 0.2778 Training Loss Country 0.2262 Accuracy Date 0.9194 Accuracy Amount 0.5252 Accuracy Country 0.9511\n",
            "Epoch 5 Batch 1600  Training Loss Date 0.2108 Training Loss Amount 0.2775 Training Loss Country 0.2250 Accuracy Date 0.9196 Accuracy Amount 0.5263 Accuracy Country 0.9512\n",
            "Epoch 5 Batch 1700  Training Loss Date 0.2104 Training Loss Amount 0.2767 Training Loss Country 0.2252 Accuracy Date 0.9198 Accuracy Amount 0.5266 Accuracy Country 0.9513\n",
            "Epoch 5 Batch 1800  Training Loss Date 0.2099 Training Loss Amount 0.2760 Training Loss Country 0.2235 Accuracy Date 0.9200 Accuracy Amount 0.5273 Accuracy Country 0.9515\n",
            "Epoch 5 Batch 1900  Training Loss Date 0.2093 Training Loss Amount 0.2750 Training Loss Country 0.2241 Accuracy Date 0.9203 Accuracy Amount 0.5276 Accuracy Country 0.9515\n",
            "Epoch 5 Batch 2000  Training Loss Date 0.2087 Training Loss Amount 0.2739 Training Loss Country 0.2239 Accuracy Date 0.9205 Accuracy Amount 0.5275 Accuracy Country 0.9517\n",
            "Epoch 5 Batch 2100  Training Loss Date 0.2082 Training Loss Amount 0.2731 Training Loss Country 0.2242 Accuracy Date 0.9207 Accuracy Amount 0.5287 Accuracy Country 0.9516\n",
            "Epoch 5 Batch 2200  Training Loss Date 0.2079 Training Loss Amount 0.2721 Training Loss Country 0.2223 Accuracy Date 0.9209 Accuracy Amount 0.5293 Accuracy Country 0.9519\n",
            "Epoch 5 Batch 2300  Training Loss Date 0.2074 Training Loss Amount 0.2716 Training Loss Country 0.2227 Accuracy Date 0.9211 Accuracy Amount 0.5298 Accuracy Country 0.9518\n",
            "Epoch 5 Batch 2400  Training Loss Date 0.2067 Training Loss Amount 0.2705 Training Loss Country 0.2227 Accuracy Date 0.9214 Accuracy Amount 0.5300 Accuracy Country 0.9518\n",
            "Epoch 5 Batch 2500  Training Loss Date 0.2062 Training Loss Amount 0.2699 Training Loss Country 0.2223 Accuracy Date 0.9217 Accuracy Amount 0.5306 Accuracy Country 0.9519\n",
            "Epoch 5 Batch 2600  Training Loss Date 0.2058 Training Loss Amount 0.2690 Training Loss Country 0.2227 Accuracy Date 0.9218 Accuracy Amount 0.5311 Accuracy Country 0.9518\n",
            "Epoch 5 Batch 2700  Training Loss Date 0.2052 Training Loss Amount 0.2680 Training Loss Country 0.2227 Accuracy Date 0.9221 Accuracy Amount 0.5312 Accuracy Country 0.9518\n",
            "Epoch 5 Batch 2800  Training Loss Date 0.2046 Training Loss Amount 0.2671 Training Loss Country 0.2229 Accuracy Date 0.9223 Accuracy Amount 0.5316 Accuracy Country 0.9518\n",
            "Epoch 5 Batch 2900  Training Loss Date 0.2041 Training Loss Amount 0.2661 Training Loss Country 0.2224 Accuracy Date 0.9226 Accuracy Amount 0.5317 Accuracy Country 0.9520\n",
            "Epoch 5 Batch 3000  Training Loss Date 0.2035 Training Loss Amount 0.2654 Training Loss Country 0.2214 Accuracy Date 0.9228 Accuracy Amount 0.5326 Accuracy Country 0.9522\n",
            "Epoch 5 Batch 3100  Training Loss Date 0.2031 Training Loss Amount 0.2646 Training Loss Country 0.2209 Accuracy Date 0.9230 Accuracy Amount 0.5328 Accuracy Country 0.9523\n",
            "Epoch 5 Batch 3200  Training Loss Date 0.2027 Training Loss Amount 0.2639 Training Loss Country 0.2210 Accuracy Date 0.9232 Accuracy Amount 0.5332 Accuracy Country 0.9522\n",
            "Epoch 5 Batch 3300  Training Loss Date 0.2023 Training Loss Amount 0.2630 Training Loss Country 0.2216 Accuracy Date 0.9233 Accuracy Amount 0.5334 Accuracy Country 0.9521\n",
            "Epoch 5 Batch 3400  Training Loss Date 0.2019 Training Loss Amount 0.2620 Training Loss Country 0.2208 Accuracy Date 0.9235 Accuracy Amount 0.5336 Accuracy Country 0.9523\n",
            "Epoch 5 Batch 3500  Training Loss Date 0.2014 Training Loss Amount 0.2609 Training Loss Country 0.2200 Accuracy Date 0.9237 Accuracy Amount 0.5338 Accuracy Country 0.9525\n",
            "Epoch 5 Batch 3600  Training Loss Date 0.2010 Training Loss Amount 0.2602 Training Loss Country 0.2193 Accuracy Date 0.9239 Accuracy Amount 0.5343 Accuracy Country 0.9525\n",
            "Epoch 5 Batch 3700  Training Loss Date 0.2005 Training Loss Amount 0.2595 Training Loss Country 0.2191 Accuracy Date 0.9240 Accuracy Amount 0.5346 Accuracy Country 0.9526\n",
            "Epoch 5 Batch 3800  Training Loss Date 0.2001 Training Loss Amount 0.2586 Training Loss Country 0.2185 Accuracy Date 0.9242 Accuracy Amount 0.5348 Accuracy Country 0.9527\n",
            "Epoch 5 Batch 3900  Training Loss Date 0.1996 Training Loss Amount 0.2578 Training Loss Country 0.2182 Accuracy Date 0.9244 Accuracy Amount 0.5345 Accuracy Country 0.9528\n",
            "Epoch 5 Batch 4000  Training Loss Date 0.1992 Training Loss Amount 0.2572 Training Loss Country 0.2180 Accuracy Date 0.9246 Accuracy Amount 0.5349 Accuracy Country 0.9529\n",
            "Epoch 5 Batch 4100  Training Loss Date 0.1989 Training Loss Amount 0.2567 Training Loss Country 0.2175 Accuracy Date 0.9247 Accuracy Amount 0.5354 Accuracy Country 0.9530\n",
            "Epoch 5 Batch 4200  Training Loss Date 0.1986 Training Loss Amount 0.2561 Training Loss Country 0.2175 Accuracy Date 0.9248 Accuracy Amount 0.5355 Accuracy Country 0.9530\n",
            "Epoch 5 Batch 4300  Training Loss Date 0.1982 Training Loss Amount 0.2554 Training Loss Country 0.2172 Accuracy Date 0.9250 Accuracy Amount 0.5357 Accuracy Country 0.9531\n",
            "Epoch 5 Batch 4400  Training Loss Date 0.1978 Training Loss Amount 0.2547 Training Loss Country 0.2171 Accuracy Date 0.9252 Accuracy Amount 0.5359 Accuracy Country 0.9531\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-5\n",
            "Epoch 5 Training Loss Date 0.1977 Training Loss Amount 0.2546 Training Loss Country 0.2169 Accuracy Date 0.9252 Accuracy Amount 0.5360 Accuracy Country 0.9532\n",
            "Epoch 5 Validation Loss Date 0.1646 Validation Loss Amount 0.2041 Validation Loss Country 0.3998 Accuracy Date 0.9387 Accuracy Amount 0.5549 Accuracy Country 0.9306\n",
            "Time taken for 1 epoch: 1418.11 secs\n",
            "\n",
            "Epoch 6 Batch 0  Training Loss Date 0.1556 Training Loss Amount 0.1993 Training Loss Country 0.3894 Accuracy Date 0.9404 Accuracy Amount 0.4404 Accuracy Country 0.8906\n",
            "Epoch 6 Batch 100  Training Loss Date 0.1823 Training Loss Amount 0.2240 Training Loss Country 0.2023 Accuracy Date 0.9319 Accuracy Amount 0.5421 Accuracy Country 0.9581\n",
            "Epoch 6 Batch 200  Training Loss Date 0.1814 Training Loss Amount 0.2259 Training Loss Country 0.1980 Accuracy Date 0.9323 Accuracy Amount 0.5500 Accuracy Country 0.9588\n",
            "Epoch 6 Batch 300  Training Loss Date 0.1787 Training Loss Amount 0.2259 Training Loss Country 0.2009 Accuracy Date 0.9332 Accuracy Amount 0.5526 Accuracy Country 0.9583\n",
            "Epoch 6 Batch 400  Training Loss Date 0.1790 Training Loss Amount 0.2248 Training Loss Country 0.2022 Accuracy Date 0.9328 Accuracy Amount 0.5503 Accuracy Country 0.9578\n",
            "Epoch 6 Batch 500  Training Loss Date 0.1780 Training Loss Amount 0.2228 Training Loss Country 0.1987 Accuracy Date 0.9332 Accuracy Amount 0.5498 Accuracy Country 0.9584\n",
            "Epoch 6 Batch 600  Training Loss Date 0.1771 Training Loss Amount 0.2230 Training Loss Country 0.2010 Accuracy Date 0.9335 Accuracy Amount 0.5494 Accuracy Country 0.9581\n",
            "Epoch 6 Batch 700  Training Loss Date 0.1771 Training Loss Amount 0.2229 Training Loss Country 0.2031 Accuracy Date 0.9335 Accuracy Amount 0.5488 Accuracy Country 0.9573\n",
            "Epoch 6 Batch 800  Training Loss Date 0.1770 Training Loss Amount 0.2225 Training Loss Country 0.2023 Accuracy Date 0.9334 Accuracy Amount 0.5505 Accuracy Country 0.9572\n",
            "Epoch 6 Batch 900  Training Loss Date 0.1766 Training Loss Amount 0.2224 Training Loss Country 0.2044 Accuracy Date 0.9336 Accuracy Amount 0.5498 Accuracy Country 0.9565\n",
            "Epoch 6 Batch 1000  Training Loss Date 0.1761 Training Loss Amount 0.2212 Training Loss Country 0.2053 Accuracy Date 0.9337 Accuracy Amount 0.5497 Accuracy Country 0.9562\n",
            "Epoch 6 Batch 1100  Training Loss Date 0.1757 Training Loss Amount 0.2199 Training Loss Country 0.2059 Accuracy Date 0.9339 Accuracy Amount 0.5490 Accuracy Country 0.9562\n",
            "Epoch 6 Batch 1200  Training Loss Date 0.1757 Training Loss Amount 0.2199 Training Loss Country 0.2043 Accuracy Date 0.9339 Accuracy Amount 0.5494 Accuracy Country 0.9566\n",
            "Epoch 6 Batch 1300  Training Loss Date 0.1752 Training Loss Amount 0.2192 Training Loss Country 0.2053 Accuracy Date 0.9341 Accuracy Amount 0.5498 Accuracy Country 0.9564\n",
            "Epoch 6 Batch 1400  Training Loss Date 0.1750 Training Loss Amount 0.2192 Training Loss Country 0.2071 Accuracy Date 0.9342 Accuracy Amount 0.5504 Accuracy Country 0.9560\n",
            "Epoch 6 Batch 1500  Training Loss Date 0.1745 Training Loss Amount 0.2189 Training Loss Country 0.2076 Accuracy Date 0.9343 Accuracy Amount 0.5507 Accuracy Country 0.9558\n",
            "Epoch 6 Batch 1600  Training Loss Date 0.1743 Training Loss Amount 0.2181 Training Loss Country 0.2086 Accuracy Date 0.9344 Accuracy Amount 0.5503 Accuracy Country 0.9557\n",
            "Epoch 6 Batch 1700  Training Loss Date 0.1741 Training Loss Amount 0.2181 Training Loss Country 0.2078 Accuracy Date 0.9344 Accuracy Amount 0.5511 Accuracy Country 0.9557\n",
            "Epoch 6 Batch 1800  Training Loss Date 0.1740 Training Loss Amount 0.2179 Training Loss Country 0.2071 Accuracy Date 0.9345 Accuracy Amount 0.5511 Accuracy Country 0.9558\n",
            "Epoch 6 Batch 1900  Training Loss Date 0.1738 Training Loss Amount 0.2173 Training Loss Country 0.2063 Accuracy Date 0.9346 Accuracy Amount 0.5507 Accuracy Country 0.9557\n",
            "Epoch 6 Batch 2000  Training Loss Date 0.1737 Training Loss Amount 0.2169 Training Loss Country 0.2053 Accuracy Date 0.9346 Accuracy Amount 0.5506 Accuracy Country 0.9558\n",
            "Epoch 6 Batch 2100  Training Loss Date 0.1735 Training Loss Amount 0.2167 Training Loss Country 0.2050 Accuracy Date 0.9347 Accuracy Amount 0.5509 Accuracy Country 0.9559\n",
            "Epoch 6 Batch 2200  Training Loss Date 0.1733 Training Loss Amount 0.2163 Training Loss Country 0.2048 Accuracy Date 0.9347 Accuracy Amount 0.5514 Accuracy Country 0.9557\n",
            "Epoch 6 Batch 2300  Training Loss Date 0.1731 Training Loss Amount 0.2159 Training Loss Country 0.2045 Accuracy Date 0.9348 Accuracy Amount 0.5515 Accuracy Country 0.9557\n",
            "Epoch 6 Batch 2400  Training Loss Date 0.1728 Training Loss Amount 0.2155 Training Loss Country 0.2032 Accuracy Date 0.9350 Accuracy Amount 0.5519 Accuracy Country 0.9560\n",
            "Epoch 6 Batch 2500  Training Loss Date 0.1725 Training Loss Amount 0.2152 Training Loss Country 0.2033 Accuracy Date 0.9351 Accuracy Amount 0.5531 Accuracy Country 0.9560\n",
            "Epoch 6 Batch 2600  Training Loss Date 0.1722 Training Loss Amount 0.2148 Training Loss Country 0.2034 Accuracy Date 0.9352 Accuracy Amount 0.5531 Accuracy Country 0.9561\n",
            "Epoch 6 Batch 2700  Training Loss Date 0.1722 Training Loss Amount 0.2145 Training Loss Country 0.2031 Accuracy Date 0.9352 Accuracy Amount 0.5533 Accuracy Country 0.9561\n",
            "Epoch 6 Batch 2800  Training Loss Date 0.1721 Training Loss Amount 0.2142 Training Loss Country 0.2032 Accuracy Date 0.9352 Accuracy Amount 0.5534 Accuracy Country 0.9562\n",
            "Epoch 6 Batch 2900  Training Loss Date 0.1718 Training Loss Amount 0.2136 Training Loss Country 0.2025 Accuracy Date 0.9354 Accuracy Amount 0.5534 Accuracy Country 0.9563\n",
            "Epoch 6 Batch 3000  Training Loss Date 0.1715 Training Loss Amount 0.2132 Training Loss Country 0.2018 Accuracy Date 0.9355 Accuracy Amount 0.5534 Accuracy Country 0.9564\n",
            "Epoch 6 Batch 3100  Training Loss Date 0.1713 Training Loss Amount 0.2127 Training Loss Country 0.2016 Accuracy Date 0.9356 Accuracy Amount 0.5533 Accuracy Country 0.9563\n",
            "Epoch 6 Batch 3200  Training Loss Date 0.1710 Training Loss Amount 0.2125 Training Loss Country 0.2011 Accuracy Date 0.9357 Accuracy Amount 0.5534 Accuracy Country 0.9564\n",
            "Epoch 6 Batch 3300  Training Loss Date 0.1707 Training Loss Amount 0.2120 Training Loss Country 0.2008 Accuracy Date 0.9358 Accuracy Amount 0.5531 Accuracy Country 0.9564\n",
            "Epoch 6 Batch 3400  Training Loss Date 0.1704 Training Loss Amount 0.2115 Training Loss Country 0.2005 Accuracy Date 0.9359 Accuracy Amount 0.5535 Accuracy Country 0.9565\n",
            "Epoch 6 Batch 3500  Training Loss Date 0.1704 Training Loss Amount 0.2111 Training Loss Country 0.2002 Accuracy Date 0.9359 Accuracy Amount 0.5535 Accuracy Country 0.9565\n",
            "Epoch 6 Batch 3600  Training Loss Date 0.1702 Training Loss Amount 0.2108 Training Loss Country 0.1994 Accuracy Date 0.9360 Accuracy Amount 0.5534 Accuracy Country 0.9567\n",
            "Epoch 6 Batch 3700  Training Loss Date 0.1701 Training Loss Amount 0.2106 Training Loss Country 0.1992 Accuracy Date 0.9361 Accuracy Amount 0.5534 Accuracy Country 0.9567\n",
            "Epoch 6 Batch 3800  Training Loss Date 0.1699 Training Loss Amount 0.2101 Training Loss Country 0.1986 Accuracy Date 0.9362 Accuracy Amount 0.5530 Accuracy Country 0.9568\n",
            "Epoch 6 Batch 3900  Training Loss Date 0.1696 Training Loss Amount 0.2098 Training Loss Country 0.1988 Accuracy Date 0.9363 Accuracy Amount 0.5527 Accuracy Country 0.9568\n",
            "Epoch 6 Batch 4000  Training Loss Date 0.1693 Training Loss Amount 0.2095 Training Loss Country 0.1985 Accuracy Date 0.9364 Accuracy Amount 0.5527 Accuracy Country 0.9569\n",
            "Epoch 6 Batch 4100  Training Loss Date 0.1692 Training Loss Amount 0.2093 Training Loss Country 0.1980 Accuracy Date 0.9364 Accuracy Amount 0.5529 Accuracy Country 0.9569\n",
            "Epoch 6 Batch 4200  Training Loss Date 0.1691 Training Loss Amount 0.2092 Training Loss Country 0.1976 Accuracy Date 0.9365 Accuracy Amount 0.5531 Accuracy Country 0.9570\n",
            "Epoch 6 Batch 4300  Training Loss Date 0.1689 Training Loss Amount 0.2090 Training Loss Country 0.1977 Accuracy Date 0.9365 Accuracy Amount 0.5532 Accuracy Country 0.9570\n",
            "Epoch 6 Batch 4400  Training Loss Date 0.1688 Training Loss Amount 0.2089 Training Loss Country 0.1977 Accuracy Date 0.9366 Accuracy Amount 0.5533 Accuracy Country 0.9570\n",
            "Saving checkpoint for epoch 6 at ./checkpoints/train/ckpt-6\n",
            "Epoch 6 Training Loss Date 0.1687 Training Loss Amount 0.2088 Training Loss Country 0.1976 Accuracy Date 0.9366 Accuracy Amount 0.5533 Accuracy Country 0.9570\n",
            "Epoch 6 Validation Loss Date 0.1525 Validation Loss Amount 0.1849 Validation Loss Country 0.3812 Accuracy Date 0.9440 Accuracy Amount 0.5613 Accuracy Country 0.9328\n",
            "Time taken for 1 epoch: 1426.96 secs\n",
            "\n",
            "Epoch 7 Batch 0  Training Loss Date 0.1558 Training Loss Amount 0.2178 Training Loss Country 0.1653 Accuracy Date 0.9355 Accuracy Amount 0.6766 Accuracy Country 0.9531\n",
            "Epoch 7 Batch 100  Training Loss Date 0.1622 Training Loss Amount 0.1982 Training Loss Country 0.1964 Accuracy Date 0.9395 Accuracy Amount 0.5563 Accuracy Country 0.9578\n",
            "Epoch 7 Batch 200  Training Loss Date 0.1613 Training Loss Amount 0.1987 Training Loss Country 0.1905 Accuracy Date 0.9400 Accuracy Amount 0.5555 Accuracy Country 0.9593\n",
            "Epoch 7 Batch 300  Training Loss Date 0.1598 Training Loss Amount 0.1978 Training Loss Country 0.1887 Accuracy Date 0.9402 Accuracy Amount 0.5557 Accuracy Country 0.9599\n",
            "Epoch 7 Batch 400  Training Loss Date 0.1597 Training Loss Amount 0.1981 Training Loss Country 0.1852 Accuracy Date 0.9403 Accuracy Amount 0.5578 Accuracy Country 0.9607\n",
            "Epoch 7 Batch 500  Training Loss Date 0.1591 Training Loss Amount 0.1972 Training Loss Country 0.1861 Accuracy Date 0.9406 Accuracy Amount 0.5576 Accuracy Country 0.9605\n",
            "Epoch 7 Batch 600  Training Loss Date 0.1590 Training Loss Amount 0.1973 Training Loss Country 0.1868 Accuracy Date 0.9406 Accuracy Amount 0.5546 Accuracy Country 0.9599\n",
            "Epoch 7 Batch 700  Training Loss Date 0.1591 Training Loss Amount 0.1979 Training Loss Country 0.1854 Accuracy Date 0.9405 Accuracy Amount 0.5573 Accuracy Country 0.9602\n",
            "Epoch 7 Batch 800  Training Loss Date 0.1593 Training Loss Amount 0.1982 Training Loss Country 0.1871 Accuracy Date 0.9404 Accuracy Amount 0.5564 Accuracy Country 0.9601\n",
            "Epoch 7 Batch 900  Training Loss Date 0.1593 Training Loss Amount 0.1982 Training Loss Country 0.1860 Accuracy Date 0.9403 Accuracy Amount 0.5573 Accuracy Country 0.9602\n",
            "Epoch 7 Batch 1000  Training Loss Date 0.1592 Training Loss Amount 0.1974 Training Loss Country 0.1872 Accuracy Date 0.9403 Accuracy Amount 0.5572 Accuracy Country 0.9598\n",
            "Epoch 7 Batch 1100  Training Loss Date 0.1590 Training Loss Amount 0.1978 Training Loss Country 0.1873 Accuracy Date 0.9403 Accuracy Amount 0.5575 Accuracy Country 0.9598\n",
            "Epoch 7 Batch 1200  Training Loss Date 0.1591 Training Loss Amount 0.1973 Training Loss Country 0.1864 Accuracy Date 0.9403 Accuracy Amount 0.5587 Accuracy Country 0.9598\n",
            "Epoch 7 Batch 1300  Training Loss Date 0.1588 Training Loss Amount 0.1973 Training Loss Country 0.1866 Accuracy Date 0.9403 Accuracy Amount 0.5580 Accuracy Country 0.9597\n",
            "Epoch 7 Batch 1400  Training Loss Date 0.1584 Training Loss Amount 0.1967 Training Loss Country 0.1876 Accuracy Date 0.9405 Accuracy Amount 0.5578 Accuracy Country 0.9597\n",
            "Epoch 7 Batch 1500  Training Loss Date 0.1585 Training Loss Amount 0.1968 Training Loss Country 0.1886 Accuracy Date 0.9405 Accuracy Amount 0.5583 Accuracy Country 0.9595\n",
            "Epoch 7 Batch 1600  Training Loss Date 0.1584 Training Loss Amount 0.1963 Training Loss Country 0.1891 Accuracy Date 0.9405 Accuracy Amount 0.5579 Accuracy Country 0.9593\n",
            "Epoch 7 Batch 1700  Training Loss Date 0.1582 Training Loss Amount 0.1959 Training Loss Country 0.1885 Accuracy Date 0.9406 Accuracy Amount 0.5580 Accuracy Country 0.9593\n",
            "Epoch 7 Batch 1800  Training Loss Date 0.1580 Training Loss Amount 0.1958 Training Loss Country 0.1881 Accuracy Date 0.9406 Accuracy Amount 0.5580 Accuracy Country 0.9594\n",
            "Epoch 7 Batch 1900  Training Loss Date 0.1577 Training Loss Amount 0.1955 Training Loss Country 0.1878 Accuracy Date 0.9408 Accuracy Amount 0.5587 Accuracy Country 0.9593\n",
            "Epoch 7 Batch 2000  Training Loss Date 0.1575 Training Loss Amount 0.1952 Training Loss Country 0.1878 Accuracy Date 0.9408 Accuracy Amount 0.5585 Accuracy Country 0.9592\n",
            "Epoch 7 Batch 2100  Training Loss Date 0.1575 Training Loss Amount 0.1950 Training Loss Country 0.1862 Accuracy Date 0.9408 Accuracy Amount 0.5587 Accuracy Country 0.9595\n",
            "Epoch 7 Batch 2200  Training Loss Date 0.1576 Training Loss Amount 0.1950 Training Loss Country 0.1851 Accuracy Date 0.9408 Accuracy Amount 0.5588 Accuracy Country 0.9598\n",
            "Epoch 7 Batch 2300  Training Loss Date 0.1576 Training Loss Amount 0.1948 Training Loss Country 0.1850 Accuracy Date 0.9408 Accuracy Amount 0.5588 Accuracy Country 0.9598\n",
            "Epoch 7 Batch 2400  Training Loss Date 0.1572 Training Loss Amount 0.1947 Training Loss Country 0.1838 Accuracy Date 0.9409 Accuracy Amount 0.5599 Accuracy Country 0.9599\n",
            "Epoch 7 Batch 2500  Training Loss Date 0.1570 Training Loss Amount 0.1944 Training Loss Country 0.1833 Accuracy Date 0.9410 Accuracy Amount 0.5606 Accuracy Country 0.9600\n",
            "Epoch 7 Batch 2600  Training Loss Date 0.1571 Training Loss Amount 0.1943 Training Loss Country 0.1838 Accuracy Date 0.9410 Accuracy Amount 0.5603 Accuracy Country 0.9599\n",
            "Epoch 7 Batch 2700  Training Loss Date 0.1571 Training Loss Amount 0.1938 Training Loss Country 0.1837 Accuracy Date 0.9410 Accuracy Amount 0.5604 Accuracy Country 0.9597\n",
            "Epoch 7 Batch 2800  Training Loss Date 0.1570 Training Loss Amount 0.1937 Training Loss Country 0.1843 Accuracy Date 0.9411 Accuracy Amount 0.5606 Accuracy Country 0.9597\n",
            "Epoch 7 Batch 2900  Training Loss Date 0.1567 Training Loss Amount 0.1936 Training Loss Country 0.1842 Accuracy Date 0.9412 Accuracy Amount 0.5608 Accuracy Country 0.9597\n",
            "Epoch 7 Batch 3000  Training Loss Date 0.1566 Training Loss Amount 0.1937 Training Loss Country 0.1835 Accuracy Date 0.9412 Accuracy Amount 0.5610 Accuracy Country 0.9598\n",
            "Epoch 7 Batch 3100  Training Loss Date 0.1565 Training Loss Amount 0.1933 Training Loss Country 0.1835 Accuracy Date 0.9412 Accuracy Amount 0.5605 Accuracy Country 0.9598\n",
            "Epoch 7 Batch 3200  Training Loss Date 0.1564 Training Loss Amount 0.1932 Training Loss Country 0.1831 Accuracy Date 0.9413 Accuracy Amount 0.5608 Accuracy Country 0.9599\n",
            "Epoch 7 Batch 3300  Training Loss Date 0.1562 Training Loss Amount 0.1931 Training Loss Country 0.1827 Accuracy Date 0.9414 Accuracy Amount 0.5611 Accuracy Country 0.9600\n",
            "Epoch 7 Batch 3400  Training Loss Date 0.1562 Training Loss Amount 0.1931 Training Loss Country 0.1821 Accuracy Date 0.9414 Accuracy Amount 0.5611 Accuracy Country 0.9601\n",
            "Epoch 7 Batch 3500  Training Loss Date 0.1561 Training Loss Amount 0.1928 Training Loss Country 0.1816 Accuracy Date 0.9414 Accuracy Amount 0.5610 Accuracy Country 0.9601\n",
            "Epoch 7 Batch 3600  Training Loss Date 0.1560 Training Loss Amount 0.1925 Training Loss Country 0.1813 Accuracy Date 0.9415 Accuracy Amount 0.5607 Accuracy Country 0.9601\n",
            "Epoch 7 Batch 3700  Training Loss Date 0.1559 Training Loss Amount 0.1924 Training Loss Country 0.1807 Accuracy Date 0.9415 Accuracy Amount 0.5607 Accuracy Country 0.9602\n",
            "Epoch 7 Batch 3800  Training Loss Date 0.1559 Training Loss Amount 0.1922 Training Loss Country 0.1808 Accuracy Date 0.9415 Accuracy Amount 0.5607 Accuracy Country 0.9603\n",
            "Epoch 7 Batch 3900  Training Loss Date 0.1559 Training Loss Amount 0.1920 Training Loss Country 0.1809 Accuracy Date 0.9415 Accuracy Amount 0.5601 Accuracy Country 0.9602\n",
            "Epoch 7 Batch 4000  Training Loss Date 0.1558 Training Loss Amount 0.1918 Training Loss Country 0.1807 Accuracy Date 0.9415 Accuracy Amount 0.5600 Accuracy Country 0.9602\n",
            "Epoch 7 Batch 4100  Training Loss Date 0.1556 Training Loss Amount 0.1917 Training Loss Country 0.1806 Accuracy Date 0.9416 Accuracy Amount 0.5601 Accuracy Country 0.9603\n",
            "Epoch 7 Batch 4200  Training Loss Date 0.1555 Training Loss Amount 0.1914 Training Loss Country 0.1801 Accuracy Date 0.9416 Accuracy Amount 0.5599 Accuracy Country 0.9603\n",
            "Epoch 7 Batch 4300  Training Loss Date 0.1555 Training Loss Amount 0.1913 Training Loss Country 0.1801 Accuracy Date 0.9416 Accuracy Amount 0.5599 Accuracy Country 0.9603\n",
            "Epoch 7 Batch 4400  Training Loss Date 0.1554 Training Loss Amount 0.1914 Training Loss Country 0.1799 Accuracy Date 0.9417 Accuracy Amount 0.5602 Accuracy Country 0.9603\n",
            "Saving checkpoint for epoch 7 at ./checkpoints/train/ckpt-7\n",
            "Epoch 7 Training Loss Date 0.1554 Training Loss Amount 0.1914 Training Loss Country 0.1800 Accuracy Date 0.9417 Accuracy Amount 0.5603 Accuracy Country 0.9603\n",
            "Epoch 7 Validation Loss Date 0.1430 Validation Loss Amount 0.1822 Validation Loss Country 0.4131 Accuracy Date 0.9470 Accuracy Amount 0.5636 Accuracy Country 0.9307\n",
            "Time taken for 1 epoch: 1415.14 secs\n",
            "\n",
            "Epoch 8 Batch 0  Training Loss Date 0.1378 Training Loss Amount 0.1404 Training Loss Country 0.0949 Accuracy Date 0.9443 Accuracy Amount 0.5898 Accuracy Country 0.9688\n",
            "Epoch 8 Batch 100  Training Loss Date 0.1516 Training Loss Amount 0.1849 Training Loss Country 0.1891 Accuracy Date 0.9434 Accuracy Amount 0.5417 Accuracy Country 0.9573\n",
            "Epoch 8 Batch 200  Training Loss Date 0.1514 Training Loss Amount 0.1863 Training Loss Country 0.1887 Accuracy Date 0.9436 Accuracy Amount 0.5465 Accuracy Country 0.9597\n",
            "Epoch 8 Batch 300  Training Loss Date 0.1507 Training Loss Amount 0.1858 Training Loss Country 0.1836 Accuracy Date 0.9438 Accuracy Amount 0.5520 Accuracy Country 0.9604\n",
            "Epoch 8 Batch 400  Training Loss Date 0.1500 Training Loss Amount 0.1857 Training Loss Country 0.1741 Accuracy Date 0.9441 Accuracy Amount 0.5555 Accuracy Country 0.9625\n",
            "Epoch 8 Batch 500  Training Loss Date 0.1497 Training Loss Amount 0.1858 Training Loss Country 0.1755 Accuracy Date 0.9441 Accuracy Amount 0.5582 Accuracy Country 0.9625\n",
            "Epoch 8 Batch 600  Training Loss Date 0.1504 Training Loss Amount 0.1861 Training Loss Country 0.1758 Accuracy Date 0.9438 Accuracy Amount 0.5604 Accuracy Country 0.9620\n",
            "Epoch 8 Batch 700  Training Loss Date 0.1503 Training Loss Amount 0.1859 Training Loss Country 0.1730 Accuracy Date 0.9438 Accuracy Amount 0.5605 Accuracy Country 0.9624\n",
            "Epoch 8 Batch 800  Training Loss Date 0.1499 Training Loss Amount 0.1855 Training Loss Country 0.1741 Accuracy Date 0.9440 Accuracy Amount 0.5610 Accuracy Country 0.9620\n",
            "Epoch 8 Batch 900  Training Loss Date 0.1497 Training Loss Amount 0.1857 Training Loss Country 0.1742 Accuracy Date 0.9439 Accuracy Amount 0.5614 Accuracy Country 0.9617\n",
            "Epoch 8 Batch 1000  Training Loss Date 0.1497 Training Loss Amount 0.1851 Training Loss Country 0.1742 Accuracy Date 0.9439 Accuracy Amount 0.5599 Accuracy Country 0.9617\n",
            "Epoch 8 Batch 1100  Training Loss Date 0.1493 Training Loss Amount 0.1849 Training Loss Country 0.1733 Accuracy Date 0.9440 Accuracy Amount 0.5604 Accuracy Country 0.9621\n",
            "Epoch 8 Batch 1200  Training Loss Date 0.1494 Training Loss Amount 0.1850 Training Loss Country 0.1732 Accuracy Date 0.9439 Accuracy Amount 0.5609 Accuracy Country 0.9620\n",
            "Epoch 8 Batch 1300  Training Loss Date 0.1492 Training Loss Amount 0.1858 Training Loss Country 0.1740 Accuracy Date 0.9440 Accuracy Amount 0.5612 Accuracy Country 0.9619\n",
            "Epoch 8 Batch 1400  Training Loss Date 0.1490 Training Loss Amount 0.1854 Training Loss Country 0.1731 Accuracy Date 0.9440 Accuracy Amount 0.5622 Accuracy Country 0.9620\n",
            "Epoch 8 Batch 1500  Training Loss Date 0.1495 Training Loss Amount 0.1853 Training Loss Country 0.1731 Accuracy Date 0.9439 Accuracy Amount 0.5624 Accuracy Country 0.9619\n",
            "Epoch 8 Batch 1600  Training Loss Date 0.1495 Training Loss Amount 0.1853 Training Loss Country 0.1735 Accuracy Date 0.9439 Accuracy Amount 0.5624 Accuracy Country 0.9618\n",
            "Epoch 8 Batch 1700  Training Loss Date 0.1493 Training Loss Amount 0.1849 Training Loss Country 0.1730 Accuracy Date 0.9440 Accuracy Amount 0.5622 Accuracy Country 0.9618\n",
            "Epoch 8 Batch 1800  Training Loss Date 0.1490 Training Loss Amount 0.1846 Training Loss Country 0.1717 Accuracy Date 0.9441 Accuracy Amount 0.5619 Accuracy Country 0.9621\n",
            "Epoch 8 Batch 1900  Training Loss Date 0.1491 Training Loss Amount 0.1845 Training Loss Country 0.1710 Accuracy Date 0.9440 Accuracy Amount 0.5622 Accuracy Country 0.9622\n",
            "Epoch 8 Batch 2000  Training Loss Date 0.1488 Training Loss Amount 0.1845 Training Loss Country 0.1706 Accuracy Date 0.9441 Accuracy Amount 0.5618 Accuracy Country 0.9622\n",
            "Epoch 8 Batch 2100  Training Loss Date 0.1488 Training Loss Amount 0.1844 Training Loss Country 0.1701 Accuracy Date 0.9441 Accuracy Amount 0.5623 Accuracy Country 0.9623\n",
            "Epoch 8 Batch 2200  Training Loss Date 0.1489 Training Loss Amount 0.1841 Training Loss Country 0.1707 Accuracy Date 0.9441 Accuracy Amount 0.5622 Accuracy Country 0.9622\n",
            "Epoch 8 Batch 2300  Training Loss Date 0.1487 Training Loss Amount 0.1840 Training Loss Country 0.1709 Accuracy Date 0.9442 Accuracy Amount 0.5628 Accuracy Country 0.9622\n",
            "Epoch 8 Batch 2400  Training Loss Date 0.1485 Training Loss Amount 0.1837 Training Loss Country 0.1705 Accuracy Date 0.9443 Accuracy Amount 0.5627 Accuracy Country 0.9622\n",
            "Epoch 8 Batch 2500  Training Loss Date 0.1485 Training Loss Amount 0.1836 Training Loss Country 0.1699 Accuracy Date 0.9443 Accuracy Amount 0.5632 Accuracy Country 0.9623\n",
            "Epoch 8 Batch 2600  Training Loss Date 0.1486 Training Loss Amount 0.1831 Training Loss Country 0.1707 Accuracy Date 0.9442 Accuracy Amount 0.5631 Accuracy Country 0.9621\n",
            "Epoch 8 Batch 2700  Training Loss Date 0.1487 Training Loss Amount 0.1830 Training Loss Country 0.1698 Accuracy Date 0.9442 Accuracy Amount 0.5635 Accuracy Country 0.9623\n",
            "Epoch 8 Batch 2800  Training Loss Date 0.1488 Training Loss Amount 0.1830 Training Loss Country 0.1706 Accuracy Date 0.9442 Accuracy Amount 0.5636 Accuracy Country 0.9621\n",
            "Epoch 8 Batch 2900  Training Loss Date 0.1486 Training Loss Amount 0.1829 Training Loss Country 0.1701 Accuracy Date 0.9443 Accuracy Amount 0.5639 Accuracy Country 0.9622\n",
            "Epoch 8 Batch 3000  Training Loss Date 0.1485 Training Loss Amount 0.1826 Training Loss Country 0.1701 Accuracy Date 0.9443 Accuracy Amount 0.5636 Accuracy Country 0.9623\n",
            "Epoch 8 Batch 3100  Training Loss Date 0.1484 Training Loss Amount 0.1826 Training Loss Country 0.1701 Accuracy Date 0.9443 Accuracy Amount 0.5635 Accuracy Country 0.9623\n",
            "Epoch 8 Batch 3200  Training Loss Date 0.1484 Training Loss Amount 0.1824 Training Loss Country 0.1702 Accuracy Date 0.9443 Accuracy Amount 0.5633 Accuracy Country 0.9622\n",
            "Epoch 8 Batch 3300  Training Loss Date 0.1483 Training Loss Amount 0.1823 Training Loss Country 0.1702 Accuracy Date 0.9444 Accuracy Amount 0.5637 Accuracy Country 0.9623\n",
            "Epoch 8 Batch 3400  Training Loss Date 0.1483 Training Loss Amount 0.1821 Training Loss Country 0.1700 Accuracy Date 0.9444 Accuracy Amount 0.5638 Accuracy Country 0.9623\n",
            "Epoch 8 Batch 3500  Training Loss Date 0.1481 Training Loss Amount 0.1819 Training Loss Country 0.1699 Accuracy Date 0.9444 Accuracy Amount 0.5636 Accuracy Country 0.9622\n",
            "Epoch 8 Batch 3600  Training Loss Date 0.1481 Training Loss Amount 0.1818 Training Loss Country 0.1695 Accuracy Date 0.9445 Accuracy Amount 0.5634 Accuracy Country 0.9623\n",
            "Epoch 8 Batch 3700  Training Loss Date 0.1479 Training Loss Amount 0.1818 Training Loss Country 0.1691 Accuracy Date 0.9445 Accuracy Amount 0.5636 Accuracy Country 0.9624\n",
            "Epoch 8 Batch 3800  Training Loss Date 0.1479 Training Loss Amount 0.1817 Training Loss Country 0.1688 Accuracy Date 0.9445 Accuracy Amount 0.5631 Accuracy Country 0.9625\n",
            "Epoch 8 Batch 3900  Training Loss Date 0.1479 Training Loss Amount 0.1816 Training Loss Country 0.1684 Accuracy Date 0.9445 Accuracy Amount 0.5629 Accuracy Country 0.9626\n",
            "Epoch 8 Batch 4000  Training Loss Date 0.1479 Training Loss Amount 0.1815 Training Loss Country 0.1687 Accuracy Date 0.9445 Accuracy Amount 0.5627 Accuracy Country 0.9626\n",
            "Epoch 8 Batch 4100  Training Loss Date 0.1479 Training Loss Amount 0.1814 Training Loss Country 0.1681 Accuracy Date 0.9445 Accuracy Amount 0.5627 Accuracy Country 0.9627\n",
            "Epoch 8 Batch 4200  Training Loss Date 0.1478 Training Loss Amount 0.1813 Training Loss Country 0.1679 Accuracy Date 0.9445 Accuracy Amount 0.5626 Accuracy Country 0.9628\n",
            "Epoch 8 Batch 4300  Training Loss Date 0.1478 Training Loss Amount 0.1812 Training Loss Country 0.1675 Accuracy Date 0.9445 Accuracy Amount 0.5627 Accuracy Country 0.9628\n",
            "Epoch 8 Batch 4400  Training Loss Date 0.1478 Training Loss Amount 0.1811 Training Loss Country 0.1673 Accuracy Date 0.9445 Accuracy Amount 0.5627 Accuracy Country 0.9629\n",
            "Saving checkpoint for epoch 8 at ./checkpoints/train/ckpt-8\n",
            "Epoch 8 Training Loss Date 0.1478 Training Loss Amount 0.1811 Training Loss Country 0.1675 Accuracy Date 0.9446 Accuracy Amount 0.5627 Accuracy Country 0.9629\n",
            "Epoch 8 Validation Loss Date 0.1420 Validation Loss Amount 0.1752 Validation Loss Country 0.4179 Accuracy Date 0.9476 Accuracy Amount 0.5644 Accuracy Country 0.9328\n",
            "Time taken for 1 epoch: 1412.46 secs\n",
            "\n",
            "Epoch 9 Batch 0  Training Loss Date 0.1178 Training Loss Amount 0.1544 Training Loss Country 0.1461 Accuracy Date 0.9551 Accuracy Amount 0.5100 Accuracy Country 0.9688\n",
            "Epoch 9 Batch 100  Training Loss Date 0.1443 Training Loss Amount 0.1714 Training Loss Country 0.1736 Accuracy Date 0.9460 Accuracy Amount 0.5469 Accuracy Country 0.9612\n",
            "Epoch 9 Batch 200  Training Loss Date 0.1447 Training Loss Amount 0.1782 Training Loss Country 0.1833 Accuracy Date 0.9455 Accuracy Amount 0.5518 Accuracy Country 0.9592\n",
            "Epoch 9 Batch 300  Training Loss Date 0.1442 Training Loss Amount 0.1792 Training Loss Country 0.1743 Accuracy Date 0.9457 Accuracy Amount 0.5616 Accuracy Country 0.9612\n",
            "Epoch 9 Batch 400  Training Loss Date 0.1441 Training Loss Amount 0.1784 Training Loss Country 0.1710 Accuracy Date 0.9457 Accuracy Amount 0.5598 Accuracy Country 0.9622\n",
            "Epoch 9 Batch 500  Training Loss Date 0.1441 Training Loss Amount 0.1778 Training Loss Country 0.1662 Accuracy Date 0.9455 Accuracy Amount 0.5619 Accuracy Country 0.9632\n",
            "Epoch 9 Batch 600  Training Loss Date 0.1444 Training Loss Amount 0.1775 Training Loss Country 0.1643 Accuracy Date 0.9455 Accuracy Amount 0.5608 Accuracy Country 0.9636\n",
            "Epoch 9 Batch 700  Training Loss Date 0.1446 Training Loss Amount 0.1784 Training Loss Country 0.1644 Accuracy Date 0.9455 Accuracy Amount 0.5617 Accuracy Country 0.9633\n",
            "Epoch 9 Batch 800  Training Loss Date 0.1441 Training Loss Amount 0.1783 Training Loss Country 0.1626 Accuracy Date 0.9457 Accuracy Amount 0.5640 Accuracy Country 0.9636\n",
            "Epoch 9 Batch 900  Training Loss Date 0.1437 Training Loss Amount 0.1778 Training Loss Country 0.1629 Accuracy Date 0.9459 Accuracy Amount 0.5660 Accuracy Country 0.9638\n",
            "Epoch 9 Batch 1000  Training Loss Date 0.1439 Training Loss Amount 0.1775 Training Loss Country 0.1628 Accuracy Date 0.9458 Accuracy Amount 0.5656 Accuracy Country 0.9638\n",
            "Epoch 9 Batch 1100  Training Loss Date 0.1436 Training Loss Amount 0.1772 Training Loss Country 0.1613 Accuracy Date 0.9459 Accuracy Amount 0.5654 Accuracy Country 0.9642\n",
            "Epoch 9 Batch 1200  Training Loss Date 0.1432 Training Loss Amount 0.1772 Training Loss Country 0.1629 Accuracy Date 0.9460 Accuracy Amount 0.5645 Accuracy Country 0.9636\n",
            "Epoch 9 Batch 1300  Training Loss Date 0.1434 Training Loss Amount 0.1769 Training Loss Country 0.1623 Accuracy Date 0.9460 Accuracy Amount 0.5641 Accuracy Country 0.9635\n",
            "Epoch 9 Batch 1400  Training Loss Date 0.1435 Training Loss Amount 0.1770 Training Loss Country 0.1610 Accuracy Date 0.9460 Accuracy Amount 0.5643 Accuracy Country 0.9638\n",
            "Epoch 9 Batch 1500  Training Loss Date 0.1436 Training Loss Amount 0.1766 Training Loss Country 0.1611 Accuracy Date 0.9459 Accuracy Amount 0.5643 Accuracy Country 0.9638\n",
            "Epoch 9 Batch 1600  Training Loss Date 0.1436 Training Loss Amount 0.1767 Training Loss Country 0.1603 Accuracy Date 0.9459 Accuracy Amount 0.5643 Accuracy Country 0.9639\n",
            "Epoch 9 Batch 1700  Training Loss Date 0.1434 Training Loss Amount 0.1767 Training Loss Country 0.1602 Accuracy Date 0.9460 Accuracy Amount 0.5648 Accuracy Country 0.9640\n",
            "Epoch 9 Batch 1800  Training Loss Date 0.1433 Training Loss Amount 0.1766 Training Loss Country 0.1606 Accuracy Date 0.9460 Accuracy Amount 0.5646 Accuracy Country 0.9639\n",
            "Epoch 9 Batch 1900  Training Loss Date 0.1432 Training Loss Amount 0.1767 Training Loss Country 0.1599 Accuracy Date 0.9460 Accuracy Amount 0.5644 Accuracy Country 0.9640\n",
            "Epoch 9 Batch 2000  Training Loss Date 0.1431 Training Loss Amount 0.1765 Training Loss Country 0.1605 Accuracy Date 0.9460 Accuracy Amount 0.5642 Accuracy Country 0.9639\n",
            "Epoch 9 Batch 2100  Training Loss Date 0.1432 Training Loss Amount 0.1765 Training Loss Country 0.1597 Accuracy Date 0.9460 Accuracy Amount 0.5642 Accuracy Country 0.9642\n",
            "Epoch 9 Batch 2200  Training Loss Date 0.1430 Training Loss Amount 0.1759 Training Loss Country 0.1587 Accuracy Date 0.9461 Accuracy Amount 0.5638 Accuracy Country 0.9645\n",
            "Epoch 9 Batch 2300  Training Loss Date 0.1431 Training Loss Amount 0.1760 Training Loss Country 0.1588 Accuracy Date 0.9460 Accuracy Amount 0.5638 Accuracy Country 0.9645\n",
            "Epoch 9 Batch 2400  Training Loss Date 0.1432 Training Loss Amount 0.1760 Training Loss Country 0.1586 Accuracy Date 0.9461 Accuracy Amount 0.5637 Accuracy Country 0.9646\n",
            "Epoch 9 Batch 2500  Training Loss Date 0.1432 Training Loss Amount 0.1759 Training Loss Country 0.1583 Accuracy Date 0.9461 Accuracy Amount 0.5642 Accuracy Country 0.9646\n",
            "Epoch 9 Batch 2600  Training Loss Date 0.1431 Training Loss Amount 0.1756 Training Loss Country 0.1586 Accuracy Date 0.9461 Accuracy Amount 0.5644 Accuracy Country 0.9646\n",
            "Epoch 9 Batch 2700  Training Loss Date 0.1429 Training Loss Amount 0.1756 Training Loss Country 0.1588 Accuracy Date 0.9462 Accuracy Amount 0.5650 Accuracy Country 0.9646\n",
            "Epoch 9 Batch 2800  Training Loss Date 0.1429 Training Loss Amount 0.1757 Training Loss Country 0.1591 Accuracy Date 0.9462 Accuracy Amount 0.5652 Accuracy Country 0.9646\n",
            "Epoch 9 Batch 2900  Training Loss Date 0.1428 Training Loss Amount 0.1755 Training Loss Country 0.1588 Accuracy Date 0.9462 Accuracy Amount 0.5649 Accuracy Country 0.9647\n",
            "Epoch 9 Batch 3000  Training Loss Date 0.1427 Training Loss Amount 0.1753 Training Loss Country 0.1583 Accuracy Date 0.9463 Accuracy Amount 0.5655 Accuracy Country 0.9647\n",
            "Epoch 9 Batch 3100  Training Loss Date 0.1426 Training Loss Amount 0.1752 Training Loss Country 0.1583 Accuracy Date 0.9463 Accuracy Amount 0.5654 Accuracy Country 0.9647\n",
            "Epoch 9 Batch 3200  Training Loss Date 0.1426 Training Loss Amount 0.1752 Training Loss Country 0.1577 Accuracy Date 0.9463 Accuracy Amount 0.5655 Accuracy Country 0.9649\n",
            "Epoch 9 Batch 3300  Training Loss Date 0.1425 Training Loss Amount 0.1751 Training Loss Country 0.1573 Accuracy Date 0.9464 Accuracy Amount 0.5657 Accuracy Country 0.9650\n",
            "Epoch 9 Batch 3400  Training Loss Date 0.1424 Training Loss Amount 0.1750 Training Loss Country 0.1576 Accuracy Date 0.9464 Accuracy Amount 0.5657 Accuracy Country 0.9650\n",
            "Epoch 9 Batch 3500  Training Loss Date 0.1424 Training Loss Amount 0.1749 Training Loss Country 0.1573 Accuracy Date 0.9464 Accuracy Amount 0.5654 Accuracy Country 0.9650\n",
            "Epoch 9 Batch 3600  Training Loss Date 0.1425 Training Loss Amount 0.1749 Training Loss Country 0.1569 Accuracy Date 0.9464 Accuracy Amount 0.5652 Accuracy Country 0.9650\n",
            "Epoch 9 Batch 3700  Training Loss Date 0.1424 Training Loss Amount 0.1750 Training Loss Country 0.1568 Accuracy Date 0.9464 Accuracy Amount 0.5654 Accuracy Country 0.9651\n",
            "Epoch 9 Batch 3800  Training Loss Date 0.1424 Training Loss Amount 0.1750 Training Loss Country 0.1568 Accuracy Date 0.9464 Accuracy Amount 0.5651 Accuracy Country 0.9652\n",
            "Epoch 9 Batch 3900  Training Loss Date 0.1424 Training Loss Amount 0.1749 Training Loss Country 0.1568 Accuracy Date 0.9464 Accuracy Amount 0.5648 Accuracy Country 0.9652\n",
            "Epoch 9 Batch 4000  Training Loss Date 0.1424 Training Loss Amount 0.1748 Training Loss Country 0.1563 Accuracy Date 0.9464 Accuracy Amount 0.5646 Accuracy Country 0.9653\n",
            "Epoch 9 Batch 4100  Training Loss Date 0.1425 Training Loss Amount 0.1748 Training Loss Country 0.1566 Accuracy Date 0.9464 Accuracy Amount 0.5645 Accuracy Country 0.9653\n",
            "Epoch 9 Batch 4200  Training Loss Date 0.1425 Training Loss Amount 0.1748 Training Loss Country 0.1566 Accuracy Date 0.9464 Accuracy Amount 0.5645 Accuracy Country 0.9653\n",
            "Epoch 9 Batch 4300  Training Loss Date 0.1425 Training Loss Amount 0.1747 Training Loss Country 0.1564 Accuracy Date 0.9463 Accuracy Amount 0.5644 Accuracy Country 0.9653\n",
            "Epoch 9 Batch 4400  Training Loss Date 0.1424 Training Loss Amount 0.1746 Training Loss Country 0.1560 Accuracy Date 0.9464 Accuracy Amount 0.5649 Accuracy Country 0.9654\n",
            "Saving checkpoint for epoch 9 at ./checkpoints/train/ckpt-9\n",
            "Epoch 9 Training Loss Date 0.1424 Training Loss Amount 0.1747 Training Loss Country 0.1560 Accuracy Date 0.9464 Accuracy Amount 0.5649 Accuracy Country 0.9654\n",
            "Epoch 9 Validation Loss Date 0.1381 Validation Loss Amount 0.1762 Validation Loss Country 0.4577 Accuracy Date 0.9491 Accuracy Amount 0.5655 Accuracy Country 0.9309\n",
            "Time taken for 1 epoch: 1411.15 secs\n",
            "\n",
            "Epoch 10 Batch 0  Training Loss Date 0.1563 Training Loss Amount 0.2611 Training Loss Country 0.0248 Accuracy Date 0.9434 Accuracy Amount 0.7188 Accuracy Country 1.0000\n",
            "Epoch 10 Batch 100  Training Loss Date 0.1417 Training Loss Amount 0.1769 Training Loss Country 0.1532 Accuracy Date 0.9457 Accuracy Amount 0.5835 Accuracy Country 0.9650\n",
            "Epoch 10 Batch 200  Training Loss Date 0.1392 Training Loss Amount 0.1738 Training Loss Country 0.1488 Accuracy Date 0.9471 Accuracy Amount 0.5753 Accuracy Country 0.9670\n",
            "Epoch 10 Batch 300  Training Loss Date 0.1387 Training Loss Amount 0.1734 Training Loss Country 0.1542 Accuracy Date 0.9478 Accuracy Amount 0.5722 Accuracy Country 0.9661\n",
            "Epoch 10 Batch 400  Training Loss Date 0.1393 Training Loss Amount 0.1735 Training Loss Country 0.1525 Accuracy Date 0.9477 Accuracy Amount 0.5693 Accuracy Country 0.9663\n",
            "Epoch 10 Batch 500  Training Loss Date 0.1391 Training Loss Amount 0.1731 Training Loss Country 0.1515 Accuracy Date 0.9478 Accuracy Amount 0.5655 Accuracy Country 0.9664\n",
            "Epoch 10 Batch 600  Training Loss Date 0.1391 Training Loss Amount 0.1737 Training Loss Country 0.1540 Accuracy Date 0.9477 Accuracy Amount 0.5646 Accuracy Country 0.9664\n",
            "Epoch 10 Batch 700  Training Loss Date 0.1392 Training Loss Amount 0.1726 Training Loss Country 0.1533 Accuracy Date 0.9478 Accuracy Amount 0.5640 Accuracy Country 0.9660\n",
            "Epoch 10 Batch 800  Training Loss Date 0.1391 Training Loss Amount 0.1727 Training Loss Country 0.1522 Accuracy Date 0.9477 Accuracy Amount 0.5646 Accuracy Country 0.9662\n",
            "Epoch 10 Batch 900  Training Loss Date 0.1394 Training Loss Amount 0.1727 Training Loss Country 0.1523 Accuracy Date 0.9477 Accuracy Amount 0.5634 Accuracy Country 0.9663\n",
            "Epoch 10 Batch 1000  Training Loss Date 0.1389 Training Loss Amount 0.1724 Training Loss Country 0.1519 Accuracy Date 0.9478 Accuracy Amount 0.5644 Accuracy Country 0.9662\n",
            "Epoch 10 Batch 1100  Training Loss Date 0.1392 Training Loss Amount 0.1725 Training Loss Country 0.1524 Accuracy Date 0.9476 Accuracy Amount 0.5652 Accuracy Country 0.9663\n",
            "Epoch 10 Batch 1200  Training Loss Date 0.1392 Training Loss Amount 0.1728 Training Loss Country 0.1521 Accuracy Date 0.9476 Accuracy Amount 0.5653 Accuracy Country 0.9662\n",
            "Epoch 10 Batch 1300  Training Loss Date 0.1392 Training Loss Amount 0.1720 Training Loss Country 0.1517 Accuracy Date 0.9476 Accuracy Amount 0.5654 Accuracy Country 0.9663\n",
            "Epoch 10 Batch 1400  Training Loss Date 0.1389 Training Loss Amount 0.1716 Training Loss Country 0.1529 Accuracy Date 0.9477 Accuracy Amount 0.5652 Accuracy Country 0.9662\n",
            "Epoch 10 Batch 1500  Training Loss Date 0.1392 Training Loss Amount 0.1714 Training Loss Country 0.1527 Accuracy Date 0.9475 Accuracy Amount 0.5651 Accuracy Country 0.9662\n",
            "Epoch 10 Batch 1600  Training Loss Date 0.1390 Training Loss Amount 0.1712 Training Loss Country 0.1523 Accuracy Date 0.9476 Accuracy Amount 0.5653 Accuracy Country 0.9663\n",
            "Epoch 10 Batch 1700  Training Loss Date 0.1390 Training Loss Amount 0.1710 Training Loss Country 0.1518 Accuracy Date 0.9476 Accuracy Amount 0.5648 Accuracy Country 0.9663\n",
            "Epoch 10 Batch 1800  Training Loss Date 0.1389 Training Loss Amount 0.1711 Training Loss Country 0.1503 Accuracy Date 0.9476 Accuracy Amount 0.5655 Accuracy Country 0.9666\n",
            "Epoch 10 Batch 1900  Training Loss Date 0.1388 Training Loss Amount 0.1711 Training Loss Country 0.1503 Accuracy Date 0.9476 Accuracy Amount 0.5653 Accuracy Country 0.9666\n",
            "Epoch 10 Batch 2000  Training Loss Date 0.1388 Training Loss Amount 0.1707 Training Loss Country 0.1502 Accuracy Date 0.9477 Accuracy Amount 0.5649 Accuracy Country 0.9667\n",
            "Epoch 10 Batch 2100  Training Loss Date 0.1389 Training Loss Amount 0.1708 Training Loss Country 0.1496 Accuracy Date 0.9477 Accuracy Amount 0.5654 Accuracy Country 0.9668\n",
            "Epoch 10 Batch 2200  Training Loss Date 0.1390 Training Loss Amount 0.1708 Training Loss Country 0.1499 Accuracy Date 0.9476 Accuracy Amount 0.5654 Accuracy Country 0.9669\n",
            "Epoch 10 Batch 2300  Training Loss Date 0.1390 Training Loss Amount 0.1710 Training Loss Country 0.1499 Accuracy Date 0.9476 Accuracy Amount 0.5653 Accuracy Country 0.9668\n",
            "Epoch 10 Batch 2400  Training Loss Date 0.1390 Training Loss Amount 0.1710 Training Loss Country 0.1497 Accuracy Date 0.9476 Accuracy Amount 0.5654 Accuracy Country 0.9668\n",
            "Epoch 10 Batch 2500  Training Loss Date 0.1389 Training Loss Amount 0.1709 Training Loss Country 0.1489 Accuracy Date 0.9476 Accuracy Amount 0.5660 Accuracy Country 0.9670\n",
            "Epoch 10 Batch 2600  Training Loss Date 0.1388 Training Loss Amount 0.1708 Training Loss Country 0.1486 Accuracy Date 0.9477 Accuracy Amount 0.5666 Accuracy Country 0.9671\n",
            "Epoch 10 Batch 2700  Training Loss Date 0.1388 Training Loss Amount 0.1708 Training Loss Country 0.1486 Accuracy Date 0.9477 Accuracy Amount 0.5661 Accuracy Country 0.9670\n",
            "Epoch 10 Batch 2800  Training Loss Date 0.1389 Training Loss Amount 0.1708 Training Loss Country 0.1483 Accuracy Date 0.9476 Accuracy Amount 0.5665 Accuracy Country 0.9669\n",
            "Epoch 10 Batch 2900  Training Loss Date 0.1388 Training Loss Amount 0.1707 Training Loss Country 0.1482 Accuracy Date 0.9477 Accuracy Amount 0.5666 Accuracy Country 0.9670\n",
            "Epoch 10 Batch 3000  Training Loss Date 0.1388 Training Loss Amount 0.1706 Training Loss Country 0.1481 Accuracy Date 0.9477 Accuracy Amount 0.5666 Accuracy Country 0.9671\n",
            "Epoch 10 Batch 3100  Training Loss Date 0.1388 Training Loss Amount 0.1705 Training Loss Country 0.1489 Accuracy Date 0.9477 Accuracy Amount 0.5665 Accuracy Country 0.9670\n",
            "Epoch 10 Batch 3200  Training Loss Date 0.1387 Training Loss Amount 0.1704 Training Loss Country 0.1482 Accuracy Date 0.9477 Accuracy Amount 0.5667 Accuracy Country 0.9671\n",
            "Epoch 10 Batch 3300  Training Loss Date 0.1387 Training Loss Amount 0.1702 Training Loss Country 0.1481 Accuracy Date 0.9477 Accuracy Amount 0.5667 Accuracy Country 0.9671\n",
            "Epoch 10 Batch 3400  Training Loss Date 0.1385 Training Loss Amount 0.1702 Training Loss Country 0.1483 Accuracy Date 0.9478 Accuracy Amount 0.5668 Accuracy Country 0.9671\n",
            "Epoch 10 Batch 3500  Training Loss Date 0.1386 Training Loss Amount 0.1701 Training Loss Country 0.1480 Accuracy Date 0.9478 Accuracy Amount 0.5665 Accuracy Country 0.9672\n",
            "Epoch 10 Batch 3600  Training Loss Date 0.1386 Training Loss Amount 0.1699 Training Loss Country 0.1475 Accuracy Date 0.9478 Accuracy Amount 0.5659 Accuracy Country 0.9672\n",
            "Epoch 10 Batch 3700  Training Loss Date 0.1386 Training Loss Amount 0.1700 Training Loss Country 0.1474 Accuracy Date 0.9477 Accuracy Amount 0.5660 Accuracy Country 0.9673\n",
            "Epoch 10 Batch 3800  Training Loss Date 0.1386 Training Loss Amount 0.1698 Training Loss Country 0.1477 Accuracy Date 0.9477 Accuracy Amount 0.5655 Accuracy Country 0.9673\n",
            "Epoch 10 Batch 3900  Training Loss Date 0.1386 Training Loss Amount 0.1697 Training Loss Country 0.1477 Accuracy Date 0.9477 Accuracy Amount 0.5653 Accuracy Country 0.9673\n",
            "Epoch 10 Batch 4000  Training Loss Date 0.1384 Training Loss Amount 0.1695 Training Loss Country 0.1472 Accuracy Date 0.9478 Accuracy Amount 0.5656 Accuracy Country 0.9674\n",
            "Epoch 10 Batch 4100  Training Loss Date 0.1385 Training Loss Amount 0.1695 Training Loss Country 0.1469 Accuracy Date 0.9478 Accuracy Amount 0.5654 Accuracy Country 0.9674\n",
            "Epoch 10 Batch 4200  Training Loss Date 0.1384 Training Loss Amount 0.1693 Training Loss Country 0.1463 Accuracy Date 0.9478 Accuracy Amount 0.5652 Accuracy Country 0.9675\n",
            "Epoch 10 Batch 4300  Training Loss Date 0.1384 Training Loss Amount 0.1694 Training Loss Country 0.1464 Accuracy Date 0.9478 Accuracy Amount 0.5652 Accuracy Country 0.9675\n",
            "Epoch 10 Batch 4400  Training Loss Date 0.1385 Training Loss Amount 0.1694 Training Loss Country 0.1468 Accuracy Date 0.9478 Accuracy Amount 0.5652 Accuracy Country 0.9674\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-10\n",
            "Epoch 10 Training Loss Date 0.1385 Training Loss Amount 0.1694 Training Loss Country 0.1469 Accuracy Date 0.9478 Accuracy Amount 0.5652 Accuracy Country 0.9674\n",
            "Epoch 10 Validation Loss Date 0.1335 Validation Loss Amount 0.1741 Validation Loss Country 0.4930 Accuracy Date 0.9496 Accuracy Amount 0.5659 Accuracy Country 0.9291\n",
            "Time taken for 1 epoch: 1418.99 secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QfcsSWswSdGV"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y6APsFrgImLW"
      },
      "source": [
        "The following steps are used for evaluation:\n",
        "\n",
        "* Encode the input sentence using the ocr tokenizer (`tokenizer_ocr`). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.\n",
        "* The decoder input is the `start token == tokenizer_en.vocab_size`.\n",
        "* Calculate the padding masks and the look ahead masks.\n",
        "* The `decoder` then outputs the predictions by looking at the `encoder output` and its own output (self-attention).\n",
        "* Select the last word and calculate the argmax of that.\n",
        "* Concatentate the predicted word to the decoder input as pass it to the decoder.\n",
        "* In this approach, the decoder predicts the next word based on the previous words it predicted.\n",
        "\n",
        "Note: The model used here has less capacity to keep the example relatively faster so the predictions maybe less right. To reproduce the results in the paper, use the entire dataset and base transformer model or transformer XL, by changing the hyperparameters above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.586Z"
        },
        "colab_type": "code",
        "id": "5buvMlnvyrFm",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def inference_fast(inp_sentence):\n",
        "  training = False\n",
        "\n",
        "  start_token = [tokenizer_ocr.vocab_size]\n",
        "  end_token = [tokenizer_ocr.vocab_size + 1]\n",
        "  \n",
        "  # inp sentence is portuguese, hence adding the start and end token\n",
        "  inp_sentence = start_token + tokenizer_ocr.encode(inp_sentence) + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "  decoder_input_dt = [tokenizer_date.vocab_size]\n",
        "  output_dt = tf.expand_dims(decoder_input_dt, 0)\n",
        "    \n",
        "  decoder_input_amt = [tokenizer_amount.vocab_size]\n",
        "  output_amt = tf.expand_dims(decoder_input_amt, 0)\n",
        "  \n",
        "  enc_padding_mask, combined_mask_dt, dec_padding_mask, combined_mask_amt = create_masks(\n",
        "        encoder_input, output_dt, output_amt)\n",
        "\n",
        "  encoder_out = transformer.encoder(encoder_input, training, enc_padding_mask)\n",
        "\n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask_dt, dec_padding_mask, combined_mask_amt = create_masks(\n",
        "        encoder_input, output_dt, output_amt)\n",
        "    \n",
        "    dec_output_dt, attention_weights_dt = transformer.decoder_dt(\n",
        "        output_dt, encoder_out, training, combined_mask_dt, dec_padding_mask)\n",
        "    \n",
        "    predictions_dt = transformer.final_layer_dt(dec_output_dt)\n",
        "    predictions_dt = predictions_dt[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "    predicted_id_dt = tf.cast(tf.argmax(predictions_dt, axis=-1), tf.int32)\n",
        "    \n",
        "    if predicted_id_dt == tokenizer_date.vocab_size+1:\n",
        "        break\n",
        "    \n",
        "    output_dt = tf.concat([output_dt, predicted_id_dt], axis=-1)\n",
        "    \n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask_dt, dec_padding_mask, combined_mask_amt = create_masks(\n",
        "        encoder_input, output_dt, output_amt)\n",
        "    \n",
        "    dec_output_amt, attention_weights_amt = transformer.decoder_amt(\n",
        "        output_amt, encoder_out, training, combined_mask_amt, dec_padding_mask)\n",
        "    \n",
        "    predictions_amt = transformer.final_layer_amt(dec_output_amt)\n",
        "    predictions_amt = predictions_amt[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "    predicted_id_amt = tf.cast(tf.argmax(predictions_amt, axis=-1), tf.int32)\n",
        "    \n",
        "    if predicted_id_amt == tokenizer_amount.vocab_size+1:\n",
        "        break\n",
        "    \n",
        "    output_amt = tf.concat([output_amt, predicted_id_amt], axis=-1)\n",
        "    \n",
        "  predictions_country = transformer.country_classifier(encoder_out, training)\n",
        "  output_country = tf.squeeze(tf.math.argmax(predictions_country, 1), 0)\n",
        "    \n",
        "  return tf.squeeze(output_dt, axis=0), \\\n",
        "         attention_weights_dt, \\\n",
        "         tf.squeeze(output_amt, axis=0), \\\n",
        "         attention_weights_amt, \\\n",
        "         output_country\n",
        "\n",
        "def inference(inp_sentence):\n",
        "  start_token = [tokenizer_ocr.vocab_size]\n",
        "  end_token = [tokenizer_ocr.vocab_size + 1]\n",
        "  \n",
        "  # inp sentence is portuguese, hence adding the start and end token\n",
        "  inp_sentence = start_token + tokenizer_ocr.encode(inp_sentence) + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "  decoder_input_dt = [tokenizer_date.vocab_size]\n",
        "  output_dt = tf.expand_dims(decoder_input_dt, 0)\n",
        "  decoder_input_amt = [tokenizer_amount.vocab_size]\n",
        "  output_amt = tf.expand_dims(decoder_input_amt, 0)\n",
        "  is_decoded_dt = False\n",
        "  is_decoded_amt = False\n",
        "    \n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask_dt, dec_padding_mask, combined_mask_amt = create_masks(\n",
        "        encoder_input, output_dt, output_amt)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions_dt, \\\n",
        "    attention_weights_dt, \\\n",
        "    predictions_amt, \\\n",
        "    attention_weights_amt, \\\n",
        "    predictions_country = transformer(encoder_input, \n",
        "                                      output_dt,\n",
        "                                      output_amt,\n",
        "                                      False,\n",
        "                                      enc_padding_mask,\n",
        "                                      combined_mask_dt,\n",
        "                                      combined_mask_amt,\n",
        "                                      dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    if not is_decoded_dt:\n",
        "      predictions_dt = predictions_dt[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "      predicted_id_dt = tf.cast(tf.argmax(predictions_dt, axis=-1), tf.int32)\n",
        "\n",
        "    if not is_decoded_amt:\n",
        "      predictions_amt = predictions_amt[: ,-1:, :]\n",
        "      predicted_id_amt = tf.cast(tf.argmax(predictions_amt, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id_dt == tokenizer_date.vocab_size+1:\n",
        "      is_decoded_dt = True\n",
        "    \n",
        "    if predicted_id_amt == tokenizer_amount.vocab_size+1:\n",
        "      is_decoded_amt = True\n",
        "\n",
        "    if is_decoded_dt and is_decoded_amt:\n",
        "      return tf.squeeze(output_dt, axis=0), \\\n",
        "       attention_weights_dt, \\\n",
        "       tf.squeeze(output_amt, axis=0), \\\n",
        "       attention_weights_amt, \\\n",
        "       tf.squeeze(tf.math.argmax(predictions_country, 1), 0)\n",
        "      \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    if not is_decoded_dt:\n",
        "      output_dt = tf.concat([output_dt, predicted_id_dt], axis=-1)\n",
        "\n",
        "    if not is_decoded_amt:\n",
        "      output_amt = tf.concat([output_amt, predicted_id_amt], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output_dt, axis=0), attention_weights_dt, tf.squeeze(output_amt, axis=0), attention_weights_amt, tf.math.argmax(predictions_country, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.589Z"
        },
        "colab_type": "code",
        "id": "CN-BV43FMBej",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def plot_attention_weights(attention, sentence, result, result_tokenizer, layer):\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "  \n",
        "  sentence = tokenizer_ocr.encode(sentence)\n",
        "  \n",
        "  attention = tf.squeeze(attention[layer], axis=0)\n",
        "  \n",
        "  for head in range(attention.shape[0]):\n",
        "    ax = fig.add_subplot(2, 4, head+1)\n",
        "    \n",
        "    # plot the attention weights\n",
        "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 10}\n",
        "    \n",
        "    ax.set_xticks(range(len(sentence)+2))\n",
        "    ax.set_yticks(range(len(result)))\n",
        "    \n",
        "    ax.set_ylim(len(result)-1.5, -0.5)\n",
        "        \n",
        "    ax.set_xticklabels(\n",
        "        ['<start>']+[tokenizer_ocr.decode([i]) for i in sentence]+['<end>'], \n",
        "        fontdict=fontdict, rotation=90)\n",
        "    \n",
        "    ax.set_yticklabels([result_tokenizer.decode([i]) for i in result \n",
        "                        if i < result_tokenizer.vocab_size], \n",
        "                       fontdict=fontdict)\n",
        "    \n",
        "    ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.591Z"
        },
        "colab_type": "code",
        "id": "lU2_yG_vBGza",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def translate(ocr, plot='', log=True):\n",
        "  enc_result_dt, attention_weights_dt, enc_result_amt, attention_weights_amt, enc_result_country = inference_fast(ocr)\n",
        "  \n",
        "  result_dt = tokenizer_date.decode([i for i in enc_result_dt \n",
        "                                            if i < tokenizer_date.vocab_size])  \n",
        "  result_amt = tokenizer_amount.decode([i for i in enc_result_amt \n",
        "                                            if i < tokenizer_amount.vocab_size])\n",
        "  result_country = country_classes[enc_result_country]\n",
        "\n",
        "  if plot:\n",
        "    plot_attention_weights(attention_weights_dt, ocr, enc_result_dt,tokenizer_date, plot)\n",
        "    plot_attention_weights(attention_weights_amt, ocr, enc_result_amt,tokenizer_amount, plot)\n",
        "\n",
        "  if log:\n",
        "    print('Input: {}'.format(ocr))\n",
        "    print('Predicted date: {}'.format(result_dt))\n",
        "    print('Predicted amount: {}'.format(result_amt))\n",
        "    print('Predicted country: {}'.format(result_country))\n",
        "  else:\n",
        "    return result_dt, result_amt, result_country"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.593Z"
        },
        "colab_type": "code",
        "id": "YsxrAlvFG8SZ",
        "outputId": "881b4522-2423-4b14-e1e3-72494a14cb9a",
        "pycharm": {
          "is_executing": false
        },
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "translate(\"* 0KQ8 OK DETALJHANDEL AB ORG NR: 556029-4588 37548 Servogatan 12 SE-19560 TEL NR: 08-59113055  Arlandastad  Belopp 18,00 19,00 37.00 kr 37,00 kr BRUTTO 37,00  Antal  Beskrivning #w#m #W# â– ** Aria Kvarg Mild 0,2% 250g Varm Dryck Li ten  1  1  EuroCard:  NETTO MOMS 33,04 3,96  MOMS %  12%  20:30 SEK 37,00  2017-12-18  KOP  EuroCard Butiksnr: 134890 Cal 7 000 SWE 746784 REF.NR :  Termid: 37548122  Personlig kod 375481223988 A0000000041010 0000001000  AID  TVR  6800  TSI  KASSÃ–RENS NAMN: F, Erik DATUM: 2017-12-18 TRAN: 245569  110:20:30:36 KASSA NR:  2  TACK FOR BESOKET VAEKOMMEN ATER\")"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: * 0KQ8 OK DETALJHANDEL AB ORG NR: 556029-4588 37548 Servogatan 12 SE-19560 TEL NR: 08-59113055  Arlandastad  Belopp 18,00 19,00 37.00 kr 37,00 kr BRUTTO 37,00  Antal  Beskrivning #w#m #W# â– ** Aria Kvarg Mild 0,2% 250g Varm Dryck Li ten  1  1  EuroCard:  NETTO MOMS 33,04 3,96  MOMS %  12%  20:30 SEK 37,00  2017-12-18  KOP  EuroCard Butiksnr: 134890 Cal 7 000 SWE 746784 REF.NR :  Termid: 37548122  Personlig kod 375481223988 A0000000041010 0000001000  AID  TVR  6800  TSI  KASSÃ–RENS NAMN: F, Erik DATUM: 2017-12-18 TRAN: 245569  110:20:30:36 KASSA NR:  2  TACK FOR BESOKET VAEKOMMEN ATER\n",
            "Predicted date: 2ï¿­ 0ï¿­ 1ï¿­ 8 ï¿­-ï¿­ 0ï¿­ 7 ï¿­-ï¿­ 1ï¿­ 2\n",
            "Predicted amount: 1ï¿­ 0ï¿­ 0 ï¿­.ï¿­ 0ï¿­ 0\n",
            "Predicted country: ESP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_1MxkSZvz0jX"
      },
      "source": [
        "You can pass different layers and attention blocks of the decoder to the `plot` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ubyeeQ-sOMXw",
        "outputId": "1d33dfea-65f1-44f0-bf9f-8ba7ecea2e67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "source": [
        "translate(\"EURMWST 19,00% A9,04 EUR 1,99 EUR# 29/02/2021\", plot='decoder_layer2_block2')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 65517 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 65517 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x576 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABHgAAAB+CAYAAABf5KZNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZgdV3nv+++7u9UaLGuyhCd5xAYMDoMRxgZCAJNATBIghBMHEg7zSU4c4HLICTfJSUxuLicTgQO5NsdhCgkOIQGCkwDmQCAMNsbG8yiEjWXhQR4ka1ar937vH1Ut7e56V/eu7l21d6l/n+fpR+rVq1etql3121XVu9Yyd0dERERERERERJqrNegOiIiIiIiIiIjI/OgGj4iIiIiIiIhIw+kGj4iIiIiIiIhIw+kGj4iIiIiIiIhIw+kGj4iIiIiIiIhIw+kGj4iIiIiIiIhIw+kGj4iIiIiIiIhIw+kGj4iIiIiIiIhIw+kGj4iIiIiIiIhIww3lDR7L/LOZnTHovojI4U+ZIyJ1UuaISJ2UOSILx1De4AF+BngW8OZBd0REFgRljojUSZkjInVS5ogsEMN6g+dNZAH082Y2OujOiMhhT5kjInVS5ohInZQ5IgvE0N3gMbO1wFPc/UvAV4FXDLhLInIYU+aISJ2UOSJSJ2WOyMIydDd4gF8D/j7//8fRRwlFGsHMXmlmywfdjzlQ5og0kDJHROrW0NxR5og01FwyZxhv8LyRLHxw92uAY83shMF2SURmYmaPBz4D/Oqg+zIHyhyRhlHmiEjdGpw7yhyRBppr5gzVDR4zWwX8lbv/uKv4XcDaAXVJRHrzBuBPyU4iGkOZI9JYyhwRqVvjckeZI9Joc8qcobrB4+7bgVumlf0fYNlgeiQiszGzEeDVZAH0mJk9bcBd6pkyR6R5lDkiUrem5o4yR6SZ5pM5Q3WDJ/ehHstEZDicD3zX3XcCHyObqaFJlDkizaLMEZG6NTl3lDkizTPnzBmaafLM7FzgOcA6M3tn149WACOD6ZWI9OBNwF/m//888Mdm9i53Hx9gn2alzBFpLGWOiNStcbmjzBFptDlnzjB9gmcMWE520+nIrq8dwC8NsF8ikpA/273K3b8J4O77gH8CXjTQjvVGmSPSMMocEalbg3NHmSPSQPPNHHP3CrtXTv6s2Wfc/VWD7ouIHP6UOSJSJ2WOiNRJmSOy8AzNI1oA7t42s+MG3Q8RmZ2ZnTXTz939urr6MlfKHJHmUOaISN2anjvKHJFm6UfmDNUneADM7BLgeOAfgd2T5e7+uYF1SkQKzOzr+X+XABuAGwEDngpc6+7nDqpvZShzRJpBmSMidTscckeZI9Ic/cicofoET24J8AhTnzFzQCEkMkTc/YUAZvY54Cx3vzn//kzgogF2rSxljkgDKHNEpG6HSe4oc0Qaoh+ZM3Sf4BGRZjGzW939KbOViYj0gzJHROqm3BGROs0nc4buEzxmtoRsWrCnkN1xBsDd3ziwTonITG4ys48Af5d//1rgpgH2pxRljkjjKHNEpG6NzR1ljkgjzTlzhmma9El/CxwDvAT4D2A9sHOgPRKRmbwBuBV4e/51W17WFMockWZR5ohI3ZqcO8ockeaZc+YM3SNaZna9uz/DzG5y96ea2SLgW+5+zqD7JiKHH2WOiNRJmSMidVLmiCwsQ/eIFnAg/3d7PpjQA8DjBtgfEZmBmT2XbNCvk+jKFHc/dVB9KkmZI9IgyhwRqVvDc0eZI9Iw88mcYbzBc6mZrQZ+H7gcWA78j8F2SURm8FHg/wK+D7QH3Je5UOaINIsyR0Tq1uTcUeaINM+cM2cYH9E6xd3vnq1MRIaDmV3t7s8edD/mSpkj0izKHBGpW5NzR5kj0jzzyZxhvMFznbufNa3s++7+zEH1SUTSzOxPgBHgc8D+yXJ3v25gnSpBmSPSLMocEalbk3NHmSPSPPPJnKF5RMvMnkQ2fd9KM/vFrh+toGtKPxEZOpN3lzd0lTnwogH0pWfKHJHGUuaISN0alzvKHJFGm3PmDM0NHuCJwM8Bq4Cf7yrfCbxlID0SkVm5+wsH3Yc5UuaINJAyR0Tq1tDcUeaINNR8MmcYH9E6192vGnQ/RKQ3ZnY08F7gOHf/WTN7MnCuu390wF3riTJHpFmUOSJStybnjjJHpHnmkzmtyntX3ivNbIWZLTKzr5nZQ2b2q4PulIgkfQK4Ajgu/34j8I6B9aY8ZY5Is3wCZY6I1OsTNDd3lDkizfMJ5pg5w3iD52fcfQfZRwp/BJwG/PZAeyQiM1nr7p8BOgDuPkGzphBV5og0izJHROrW5NxR5og0z5wzZxhv8CzK/30Z8I/u/li/F2Bm7zCzs81smMYgEmmq3WZ2FNnAX5jZOUDfj9sKKXNEmkWZ0wPljkhfNTl3lDkizTPnzBnGA/BfzOwOYC/wG2a2DtjX52WsBz4APMnMbga+A1wJXOnuj/Z5WbXKX/yLyEbH/4C7//NgeyQLwDuBy4HHm9l3gHXALw22S6Uoc+ZBmSMDoMzpzWGZO8ocGZAm544yZ56UOzIAc86coRtkGcDM1gCPuXvbzI4AjnT3BypYzhjZ1GPPAc7Nv7a7+5O76ozmH4kaSmZ2TPe2MbPPAP8ZMOBqd/+JgXVuATGz33f3P87/v9jd9w+6T3XK/1rzRLL97k53PzDgLpWizOmdMmc4KHOUOSWW1ejcUeYMh4WeOdDs3FHmlKPcGQ4LPXfmmjlD9QkeM1sGnO7uN3YVH8W0583M7J3TftWBh4Fvu/vd0+pePsMiR4EPAivzr/uAm6fV+R5wVq/rMAAfNrPrgD9z933AdrK7ex1gR3dFM3se8Cvu/pt1da7MazUsSu5fvwN8k2yb/3FefBXDvc/0zbRj9ta87EQza7v7jwfbu9kpc+ak58wB5U6veu2zMkeZE+3Dh3nuKHMqoMzpXZNzp9fMyevqXOcQXV/1ma6vejffzBmqGzzAAeBzZvZUd9+dl30E+F2ge2WODH73ZOD3zOwid/90V/m5wC7gYuBqsjtg78rr78l/fiXwl2SDjt07+Ytm9jrgVDP7IHDRfD9eaGanAr8InEAWrBuBy/KBz6bXfRLwcuD4vOjHZB/T8mnlV5EFzb+a2SfJRtd+DbAMeIWZPSP//tXA3WTbd979cPfbu+p80t1fl1jtMq8VZtYiC8lPJdqbVWL9rgR+hh7WuWSf7yDbtk8ws2/l3x9lZpN3W48nu9O/q6t/rwfW9tKXWfaDqO2XuvuXgzaium8me5Ofc9v0fswOq6oy516yfeHDLLDMyduanjv/YWbvmk8/SmQOlHi9+pE5eTt15c5k5pyaf+T+W+SZ4+53Rsd73rf/m2z/qzxzutqZ3o8nkQ3sOXmsKXMy880cGJJznTKZk9eP9rebgZ+gpsyZoR8618lUmjl5/UrOdfqUOdDs3CnTd11f6foqSddX87q+uhnY30u7uXllzlANspx/7OjzwH8CMLP3Auvc/Voze3VXvfcAi939PZNfwP8APgN80Mx+xjK/RRbqDwBnAv8L+GlgNdlGvp1sI20huzP7v4HxfNnPB/6EbAd5EvB1M3vn9K9e183M3kZ2sfcc4FnAYrId8Ltm9oJpdX8H+DTZDvy9/MuArwH/HpT/l/xnK/Pttydfx28BHwI2A+buL8zrf5jsGdK59OM/53VvM7PLzexfgF/M/39r1+++GrLXKni93g68BHifmf3VtNfrLvLXv8ft+obEdu5ev1eSBdAZwTp/eD59Bl4EvIpsv3kB2T4G8DfA9cBvAbeY2cu7+vfBeWx/A/4P2Ws7pe3ce4Pt8YWgH7+T92PObefb6gBZgE4esyeSH7PT6w6j+WTODPvE/0MW6s9j4WTORuDLwH/NLwAO5k7+8+dQU+ZA6feIUpmTL2s+ufODyfWeyz5Gtt/cm7d3J4cy591mdhfTjveuvr2MGjKna3tM78dk2y+dT9uHYeacCDxlsv/zeO8c+LlOmczJ60f72wuAz+b/Vp45M/QjlTu3Wv7Jhbm+RzTtXIcKM2eG7T/vc51+ZU7uVyket43InV4zJ6/b1Oury5nfebWur4rbVddX/bu+ei7wuby8nsxx96H6IjvYv5n//z7gbfn/r5tWb/r3XyCbL34zWRB9A/gP4OlddRYDrwceAi4kC6W35r93LbATeE9e9/8jG0zrfuAP8n//cPpXifW6GRjJ+7cM+EZefiJw/bS6G4FFQRsbgR9MK/uFfF33AC8EVpHdfdwKvKir3l3d/cj/X7ofwHXAZWSh/VNkB939+f83zvD6RK/XI2ThGb5ePW7XzdF27l6/vOxkskCYvs575tnne8j+cvEYWfi8CbgtX+by/PdOzvevt+fl10/r37kl94NbgE3T2n4/WZhMbyPVj43T687Q9tvz7wv1u47TyWP298mP2aZ8McfM6WU/ZuFkzvvIcudq4LSuuncxXJnzCWZ4jyixbeeTO90ZMJd97B5gW77vHMycrranH+/35fvBwX5QYebM0I+tZDO5TF/mQs+c3wfuLbkfz/jeyYByhxKZk9rf8rJldOUOFWbODP1I5c5G4KfmcfwWXq8et+0gz3Uqy5zU9s/Lp2fD7SSyIdGPfmbOZorHbWNyJ+h72cz5BMN9fbW5+1jo2u91fdXMzPkGh9H1VV53NfADasqcgYdOYqW+BTyBbLT31dEGSGy8F5LdgR0hOwCX5D9bTPaRsn8EriG7G318/rP1wC/nO89+skHAIPso2PMnd0LglhL9vyn42pvvOPvzF/na7h1q2u//gOy5u+ntbgJ+GCzrCfnvfC8vewXwr2Sh9NfAecDdXdtpcf7/0v0g+9TXH5HduZ+8iJ0Mt+u76s32et0F/Hv+/ymvV4/b86Z8XfYH+8FJ+Wu+Oj+Ibs6/vyVY57396DNwI9mAcm8le4PbDfxL1+8uJ/tr40PAjd3bH7iELNi3k71BHtO1D54UbI+NZANtdbe9GXiQ7CSsu41bp/3uZD8enf6zGdqe7Pfe1GvAoWP2FvJjtklfzCFzZtonaF7m3JHvv4unlfeUOXn5b5C9Id9LV+5QLnMK/aB/mZN8jyixTVO5czvZgJUH15FE7pCdCN3Sh31seuZ8G9gZHO+TF2U3UEPm5D+PcmdXvl/c0EPbCylzbgFuKrEfz/Q+VFvuJI6NnjMnL4vOMe4g+/TNndOWVUnmzNCPMHcokTmzvV49btN5Z05eZ97nOlSQOV2v+UnB9pieDX9NdrF3B1n+1Jo5wXHbqNxh7pkzFNdXMxwf+7peH11f6frq4PZnSK6vyMbQOWmybWrInGEbg2fSR8meM9vr7tvyMp/8oWVT751mZjd1/c4TgT8FXufZCPFb3H2fZc9Nngl8kezu8S35x6reZ2bPIXvG7cr861LgLDP7AtkG/1a2ODuNHuedzx1N9nGzbV1lbyALugmy5zr/NF+XdWQ7Q7ddwG1mdi/w1bzsRLI7kpjZlzj0LOt6skGo/h44HcCzqfv+2bJR8l9O9tzo48zsknydrjGzq4GfnGM/TiO7y/p7ZvYgh8Zy8q7f9bzdm/P/n971eq0hO/jOz/t78PWaviFz0faE7I7qldPKPkJ2EDjZa/suYAXZAb6G4rbvHsF/Pn2+wrOPzV1rZr+R9/V9Bxt232VmP5f391lm9tfk29/dP5735Yv5Mj5hZivJTo6+k/dn8vU+kew1f9e0tk8FPga8dlobR5jZG4G/cfd2Vz++DJw3bV9Ktf1zZCdsY8DPJ16DyWP25q5jtknmkjnhPkGWI03LnHeQ5Qhm9gOyv5r0nDn5+l8CXDI9d8g+pn27mX2N2TMn1Y+eMyf/3Z7fI0grkzst4BHLHhUZy5cT5g7Zycvkes9pH8vf26Zkjrs/z8y+ZWZPd/cb8vq7zOwPyP5KunKyHxVnzteBfWZ2lrtf11X/P5F9JPpIM7t0lrYXUubcTLZvTprT+9AAznXmmzkQn2PsIvvL641d+0mVmZPqRyp3ymROlec6PWdOv851Ksqcr5MdB1/LM3+m3HlLfn71T2Sfrqg7c6DZuTNj5sDQX1+ljo83Ar/bfV6dr4uur3R9NSzXV8eQ7R9vndbnyjJnWKdJX0b2sbTlZHfqDFhKdscUsje2xWQHAmQ7y90cGkStu/7kgE67OHSALCYbBX3C3acM+GRm5wDHAl9x992WTSu4luxjWNf12P+PAh93929PK38K2fOLb3H3O2Zpo0V2ojSeF/2Y7O64A2dzaOCmXWThO87MA9itJhus6peBt5E9L3nLXPvh7u385y8Dnuvuv2tmbYqvl+Vfk6+Xk30Mbwfx62WAu/uKrj6E2zP/2WXu/pppZU/J12+fu//rtLIp61xFn83saXn9CQ+moDSzXyX7i0Ny+5vZUrK/mPws8GKyj+ZBtv3vA8YTbT/X3b/T1cYvkX3M82nuvmFa3eeRBXD3IGBh2/lrcG1+Qj19mZcBbyY7Zl/l7l+dXmfYzTFzUvvESFfTO/N/m5I5ZwPnkL0pzStz8jYnc+eNwF/Qe+YU+lEic6Dce0Qhc/LlzCV3fhr48uQ6RrlTss895Y6ZPc3dbzSz9QS5k/fjFcBna8icF5Llzlnu/vRpdVtkWfHIbG0voMx5FXAF838fqvVcpx+Zk9ePzjGuJfuLbS2ZM0M/CrkD/A59Pn67+lBJ5uTlfT3XqSBzziV7tGfW85H8d5/r7t+pM3Pc/TXdx23TcqeHzIEhvr6a5fj4N+DjzOO6Bl1fTe+frq+m/t58rq/Gge+7+33T6lWWOUN5g0dERERERERERHo3VLNoiYiIiIiIiIhIebrBIyIiIiIiIiLScEN9g8fM3tpLWT/qqg21sVDaGESfm2JYtpfaUBvD3saw97kphmV7qQ21MezLG/Y2mmJYtpfaUBuDWN7h1kaSVzAVX7++6JpubaayftRVG2pjobQxiD435WtYtpfaUBvD3saw97kpX8OyvdSG2hj25Q17G035GpbtpTbURlP7PExtpL4q/QSPmX3MzLaa2S1VLkdEZJJyR0TqpMwRkTopc0RkJpXOomVmzyebZu6T7n5mL78zZot9CUcAcID9LGLxlJ9HZf2oW0UbtngMgPH2XsZGlh6qlG/y8c4exlrLsqLx8biNRaN53b2MtaI2DpX7xESpdfGV2bIPjO9m0Vi2ze2xPWF9G2nly9vHWGvJoTY6WUcO+D4WWV7etU/1fZsuyf4dn9jD2OiyQ/3Ytz9sY//6bL3au3czckT2/8Vbds+7H4NYl371o6713sduxn2/FSpVrGzu9DNzBt3GxOOOOFg+sXc3o0uz7xc9lmXDeHsPYyN55uzfH7Zho4dmeZ9yvE9mju9lzPLMabdL9c/GFuX9OJSJPn4gbsMO7TpRvgx6m45ujXNk/Lhi5ozdt/vgz2vbP/LNd8D3s8gmczx+bX2iPf/l9aPP82yjMZkzusyXjq0CYHxiN2Oj+T7W9R5eeM+fXuaJckuUh214V3nXsd517B2sPzp6qKz7PWuiXawL0JUN476PMVsCI61E3U6x7jSztpFYR+/q32SOTM24rja614X9jOX7lQfnNJPnZr32Y7byKWXdx2lXZk/ZpkG/u/s85TXs3qap7RecUyb3pVar2DcOncd2Z073fjr1fSY+t508j+3p/aQPbcxUVlje0kP7ZfcxsGPv/Q+7+7rCL1doztdXdsTU1wcOvkZDmf+J84DxY7Jt396zm5Flh96jx3ZmWXLgwG4WLcrLd+2N216e7TPjB3YztuhQGzaRtTHlfGlffL6UKrPJY6Tr2PNOJ6w/+b48Pfsmz6/qel06q/Lz4P27WLR4+cE6re3xuU6/rq/muy/ZSKtwbTq53xSuWVPnOmZTzzO7lze9fIZz0GG6vhqd6Yfz5e7fNLOTy/zOEo7g2XZeNR2q2chJp4bl1nUSMWni7nvCuqNrjw7Loxtz7Qe3lugd7P/JZxXKFv/bNWHdkeUrwvLO3n3Fvh0YD2r2x8jJp4Xl7Ts3heU/fOc5hbLHv+vquPEKb3bSGikUjZx8Slg1tS7db3SzqnJdenS1f20gyy2bO0s4gme3Xhw11L9O9VuwPwE8+CvPDsuP+/KDhbL2xh+GdUdWHxUvs+vi82Ab2x9LdDA2esz6YrP3bgnr2qKxsLz7ptJBnaCsTx684Dlh+dEfujIsv+fXi/VPek8icyrst40W395bq1aGddsPP1Ki4UQO1X28BP24uvPVevuQK5s5S8dWcc4T3lQob23dVmaZYbmVeJ/wAwfCchsrHnudo9eEdVsPbQ/LO9uK5a0VR8Z1d+xMdbHnNlL7ZfvR4jYdWbM67kfQZ4BO183wSaNrH5fo4fz56vh8y7bH2yns96JFYd3WEcvC8mh/Su5Ly4o3rQDam4tZHuY1MLI2vh8Snscm+pFsY+tDxcJUPqXW0YoPOLSe9MSw6leu/6P4BL5Cc7q+siM4Z/QlxbaC9/ZhkToP2PyWDWH5+n/fUyiz79wQ1u2c9YywfNHDuwpl7dt/kOpiqNV10+ng8nbvDmrCyMo4j9rbg+O6wvfZPefF54/LPhefv4TXV78dX0NWea4TXp+OxfmXOtexxcUbKnTibZ28xo3Oy71TLINyr2PQ7tXtr8z+a70vQUREREREREREhlGln+DpVT4y9FsBlhD/dUFEpF+UOSJSpymZsyj+hIaISD/pXEdkYRqKT/C4+6XuvsHdN0TPnYmI9JMyR0Tq1J05B8fcERGp0JRzHdO5jshCMRQ3eEREREREREREZO76/oiWmV0EnANMAM8AjgIWmdkW4A/d/aP9XubAJQY8nRyRfLo731QcoO4Jvx6P0eb748GcOrviwbrKWPzFa3uu296xY97L64fOj+4tVb+9ojiwl43Gg29VOTh0NMBYcjDllGEe9HeApmUOwFnAkcCSnnMnGFARr25QuPlqLYn/ErdqUzxo6h1vX1soO/0340GWbWk8gGan5IDKkXDwy5Tk4HSJ8oocd9kdYXlq7+gsLh6nI8vj94K+5GpikNBocNP2o/EgsqUMSQ51zwh2UE1dm3fmjB/ANt9fKG734X29DE8MImmt4j5l61aFdVd+Zm9Y/thri5nT3nJfWLe1Oh5kNHzffCQxEHUiF6K22w89HNc9MjGA83gxV0sNTJ4Sve8Ad77nxLD8ib95d1jeeuLjC2WdjXHd9t749Sqjtaz3R33C4xToPPLovPvRjzbSgy8Xi1oPz/89cK76cp6TMHrSCX3pY8/avb+Hdx6L3yNP/J/fC8ujwXJbifWb+HY8+HI72CdGjooHmU+dL01s+XGhbHT98T3XTUm+VoksJ8hyT2zTZZ+Pt+noCcXJMQBO/71g+yUGN24dfVzcvxJ8R3Hwa4D2tt4nJ0itS2qyj7CN1P50T4nr0xKTVYQZ2pl9MoWqxuC5wN23A5jZKuAd7n5RRcsSEVHmiEidlDkiUidljoj0RIMsi8iCo8wRkTpNyZyWxuARkerpXEdkYRqKMXg04KmI1EmZIyJ1mjLIssUf6xcR6ScNsiyyMNVxg+c84EIz22Rm765heSKysClzRKRup5nZncodEamJMkdEQpU+omVmI8BfAJ8Cfhu4xswud/fbqlxu7YKBAAHsjh+F5euuOrPnpm1Z/Je+VjSAZslBgqOBm3xiIqhJqQGhquTj5dbxkhd+slD2/okn96s785MYnDu1P8ns5pw5DdvmnT17wvJlP4oHgVxxZ3HA0xRPDfR6IB7AuQw/kMiXqG4qi2pWdkD7ja+7pFD2kt99Zr+6U1Qmg2cfl68xwv1jcOM/t4DzgQ3AFmbLnVYLO6L4mJbt29+HngR/t+ukBixPDLIcnBu0l8d//X/0bfHgmdufWxywePXn48GN/eijwvLWY8VBNZPbKLEunWAAThsbi9sIzquyRoLJGsaWxHXLiF4r4Iw/T2ynRL/v+YXi9jvhq3Fdu/kHPXZuBomBVAnellIDebcSbUSDwyf309H4EibMhtT5VpnB/EeG4qGHSeUyB8DjbVNqcNi6Ja497Onxefydby8el6e//vth3dbTzojb3vxAoazdhwG9k4Mpl7i+qvK1skVxZqQGIL7ivuIgyy896ey4jSr3sWj7JQawTw6mHLWRyJ2+rEuJ87ZwAqAefr/qtDobuAvY5u7jwKeBl1e8TBFZuJQ5IlK3ZwKPuvtdyh0RqYEyR0SSqr7BczzQfctyS14mIlIFZY6I1O1YoHvuWeWOiFRJmSMiSVU8orUV+KSZdYDjgKOB7870CxrlXUTmQZkjInXqzhzILqz2zvQLUzJnpPj4kojIDEpnDuhcR2Sh6vsNHne/GLgYwMzOBS7KywDWM/Wv65O/cylwKcAKWzO4p+hFpHGUOSJSp+7MgUO501WlkDvdmbNy7Ghljoj0bC6Zk/+eznVEFqCqH9G6BjjdzE4xszHgAuDyipcpIguXMkdE6qbcEZE6KXNEJKnSWbTcfcLMLgSuAEaAj7n7rbP+YonRrIdCYgT01vLiLBkAyx4qMZPMirgNVgUf8b51R7FsBtsv2FAoW/l38ZMtrcXxDBoWrGP74UdK9aOM1OwXvj+eWeOii95QKFs1cm3cRpWz9gQzONiixAwQ+xMzeQRttJbGM3l0dscz/1jwOiZnJkuMQh/OOBUes/GvV2kumWNmtJYUt2Op/SG1rcJOJmbtKNGGLUkcjzvi133djcGMPYkZE2xl/PiIrVheKPMHtsYdTMxGM/G8pxbKRr9zS7y8xDq21qwqLu7H98f9KKPMa0h6BpeXPfOlhbLWkjiby84KGHck0e9WcEwmZrQJ6ya0jloTlncSs4yMrC3O8NN+pDizEUArse91HttZKLNgFh7bM5hZbsrmjk9M0N4azJSUyoaKpGY4io7fRfc8FFbtrCsejwAPP724T636h3h/b22+L+5G9L6emukqobWs+FhKZ+++sG5n76xPvBxU5fmCPRwfS6lZCG/9rYsLZS/7+18I67b70e/EzILR/jSyemVYt/NY4ny1zOwyZdalDzNldh6q7ty2rH5eX42efGK0gJ5/H4hn6kvMEpdsI9i/JxLv7X59vKqnv6HH9QMmbrw97kdgJPG+F10DAUxsLs7WNLo+HiIpObNTYPSUk+IflHi9fNv2sFlWpRUAABQGSURBVGp7ezwL6+hJJ4TlL4kmUGzFx1my3yV0ovdMEtc7nuhHal/40eZiYWI/TbZxT/A6pmbZTpx/hzNmzfH6qu83eMzsIuAcYPJIHQUuc/eL+r0sERFljojUKZE533X3JwysUyJy2FLmiEgZVX2C5wJ33w5gZquAd1S0HBERUOaISL2UOSJSJ2WOiPSk0ke0eqVR3kWkTlMyxxKPQYqI9InOc0SkbsodkYWpjgfWzwMuNLNNZvbuqIK7X+ruG9x9wyLicRdERHpUKnPGlDkiMn+nmdmdqdyZcp5j8dhpIiIlzJg5oOsrkYWq0hs8ZjYC/AXwKeDJwK+Y2ZOrXKaILFzKHBEZgBZwPvCzKHdEpHrKHBFJqvoRrbOBu4Bt7j5uZp8GXg7cNuNvDfOMWZFEfyfufyAsX7qjOCNIau6MzqZ7wvLUDDNlfPfPPlwoe8nfPT3ux/iBsLx1zLpiYYWzaKVmy0o56hv3FsomqpwtKyUaSb3d+8w1yTZSsxQkWDSifup4S4xCH9cdmmO2dOa4O5198awqwyocaZ/0zE5jPyzOdjWRaMMfjWdYsCOKH+8uezw+8/3XFcpueEaicmJGIV/8uGJZhcd0NAsPQCex7tuef3Kh7Mh/iGcnrFRwrKdmQ+zsS82kF2RUYhaj1GvQ2b0nqBu/n7QTM3GFywv2X695FqouzwQedfe7AGbLHWu1wlk226mZhaqSOsY6xfcVXx4fB+1lxdnMAJY+GLzXpGbtOv7osHhkZ3HfaT/wYFg32XZUN7EPp4+P4vtDmeWlWGr2uuOPCYv99k1h+Ye2FWeo8dHirJvQp36nyoP16ZSYcaus1OtYSmpGp+Ccxo5IPM4dr2LVSmXOQcF6TdwdX2cMs2h2RgBfX8ySiRviTTJyxulhefvI4ics29+7Oe5I4j1r098WT2xO+7Xrw7rRLK4AdmpxtqaJ2zbG/eiD1MxOE/cUr6MA9vziswtlyz7/vbiNCvex8Lw3MbtoOFsW8brv++mnxQv84jVxG8/6iWLhTfHrFV2LAfhE/2YRr/oRreOBH3d9vyUvExGpgjJHROp2LNB9d0a5IyJVUuaISJIGWRaRBUeZIyJ1mpI5reUD7o2ILAQ61xFZmKq4wbMV+KSZdYDVwBnARfnP1jP1r+tANggYcCnAClszNM96iEgjKHNEpE7dmQNwFND9OftC7nRnzsrRdcocESmjdOaAznVEFqq+3+Bx94uBiwHMbBTYCHzJzMaAC4DX9HuZIrJwKXNEpE7dmQOHcsfMTiG7yFLuiEjfKHNEpIxKH9Fy9wkzuxC4AhgBPubut876i9HgQ8MziGvPUgNWlRkQ1MbiAQxt2dJi4c7i4M0zOe3rbyiUPZ7EIGBL40HAxo8uDjo3ekupbpSSGkQ2tU19167qOlNGqzjgYWrw1vaOeKDNsP6p6+Pl3Xh73Ma6tYUyf6A4AG/2g9QAnMGxGNUdwCE7l8wxs3CQu1IDmScGdEttw/m20dkbDwqdHKQ1KB9ZtTJuY0d8zNie4oCnqTZSbjm/mFsjq/aGdVPraDuLI1qW7Ue8wHiHbSdyNbXMcEDlxIB6IytX9Na3GfiBRPYFg0CnBstPrUtnd/G1scSAkKk2LCj3ffEA1a01q8Jy3xPsI8EAq7a36mEFY6Vzxz18fUZPOK5Ytw+D0ZYWnG/5w/FgoiP33heWn/hAcTB0ovUDJu68K9GPYvaNHLUmrJraLyfuK052MXr8sXHde7fE/Qik2kgKBiD2RNa2E4Mph/sH8MXXPrG4uH2PlWoj3M8Sg0D7njibfXtxma0Vccal3l8nthQ+hJI0emJ8/jOxOXgdU9cQqffdYOoTH6LJGPp5fTV60gm9Lzg1KHUnONdJTALiI3G57S++P6X2h3ZqMpegfPTk4mDFABO3/yBuIzCyenVYbivix22jAZVHT0jsr6ncCfo3ekpxQHVghv07yJ1t8UQa7eD4hfT+sexzV8fLjNpI9TuS2Mc6Dz4Ul+/ufZTzVD+iQaAXf+nauI1TT47b+H4wmHc0OQ5A4lo2FN4Tmf3Xel6Cmb0eeCNTB/W6393f0lXnZODfgLu76qx19yf0uhwREVDmiEj9lDsiUidljoj0W9lP8LzN3W+Y/MbMPhDU+Wt3/8AsdUREeqHMEZG6KXdEpE7KHBHpG82iJSILzpTMseJjhiIi/aTMEZG66fpKZGEazAPr07j7pe6+wd03LKLEuBciInPQnTljyhwRqdiUzLF4vBgRkX7S9ZXIwlTHDZ6TzOxOM9tkZu+uYXkisrApc0SkbsodEamTMkdEQpU+omVmI8ALgbOALcA1Zna5uwdDTXdp4IxZET8wPu82kqODJ2aYKePiZ/9doex9PCWsG834AbBzfXGmsHis+f4oMwMZzDCrUN2CkdRTs/MkmwhmMRrdui2um2jDx4v7pE/EM+skDfHxOZfMcfd4dqEhmjGjIDWTRYmZuFIzJkQzvgHYWPFYT7aR0PlscUaV1nnFWW4ARhKzr2x96SmFsjUfv6pUP8pIzjaWWPdoRrZOYl8qu/1KCfYRG41nZSyzL7SPjWcx8mvi2U5aJx9frLs5nlnCU3kdzMbSCWbi8mgmlxqUzp1Fo9jxxxSK25t7n0GoL1IzdAWzJ7VOOzmsaoltPv644uwyI9+NN0frzNPjtvcX3+/9nnjGmc7OeEaqaNat9tZ4NpbRY44Oy6MZe1JtlGEjcda2fiIeN7d9Rzzb2H2vKs5yc8I/JGbKSc2aWUJrcfxJkGi21dRsWant11pe3G+i8xZIr0uUc7YoMQNrYhZCPMjPY4OZ4WDq0Mg16ef11cSPNlfRxWolzlOic53U+llqPz6yuA8mZ+3aFp+Dn3h18THczc+Osyt1jrH1l55cKDvqI9Wd66S2x8Q998b1g5miU9e90SxVfRPtC4kZbJP9CNp4+M1nh1XXXhq/BjsvOKdQtuKz14V1UzNkE1xrl73undSXT/CY2UfMbEPwo7OB7e5+l7uPA58GXt6PZYrIwqXMEZG6KXdEpE7KHBGZi77c4HH3N7t7NGH88UD3n1W25GUiInOmzBGRuil3RKROyhwRmYsyj2htA95rZt2fv7ppWp1x4OVm9oL8++OAWaeL0CjvIhJQ5ohI3SrJnSmZMxo/figiC5LOdUSkr3q+wePuXwC+MEud+8ieCQXAzM4FLuqqsh4oPGTu7pcClwKssDXDO8CHiNRGmSMidasqd7ozZ+WSY5Q5IgLoXEdE+q/qWbSuAU43s1PMbAy4ALi84mWKyMKlzBGRuil3RKROyhwRSer7LFpmdhFwDjA57POjwLXAduBj7n7rjL/fatFaWvwYoY1WOuFXgSdmCrJgZpJk3ZHE/bNoppt2YlaR1IxF0cwVwUwjM3n/mc8sNnFkYmTvRP9W/+33CmWp2W9KbdPE8pIS6x7NvhAtD4BFiXUPpNpIvl7B+qRmhmgtiz9G295VnFGt/fCjYd3Ua9B5NJhZIzHzUmrWibBuMBuI7a36/nG+nH5kTjDTR+r1GQbRjFYAfkZxhimA1t7iLGGdjfGMLHZmPIOLHSjuw7bx7riDiRkMxl4XzCaXyHYLZrIAOOqyYFaCCt8ffP2xYbkFxyOAnXBcoax134Nx24nZCUtJHL+tlUf23EQnMYtWNKuG37Qxrpt4DfzWTcW6idmDkrNFBPVbwSwUti+R7X0WZM4ocBVwBTDCbLlzYAJ/oDiLUGq7VMYSs45F76cPPhxW9ePjmYXGNhffmzqpWaO2xTNgMRrUT2QfiX1n9znFTFz2tVvCur4izpyRxcVldh5KzKpTRmJ72ETidUnUP+GzxVluPDFzX1/2sWCWNQA7ovjEUOp9tEw/RtatDcs7iZmNWmuLc7l2dsSzliZzMsrVrfExUId5Zw7ZNh9ZUZyxyYMZCUuLMqPsrIZBG8lrv+PjGe+iNvzuxCxQwXt11kYwi+CevXHdhB/9t+LMgKPL4vfO1Hv44z5TfDk9mKWztMT1Umtl4rphWzwjX2v1qmLd1EyY/ZjhMnWNG11/JGap8t3F2YhTbRz92TvjNhKvwaorivU9dR2Veg2i679gf7Qds+dnVWfFF7j7dgAzWwW8w90vqmhZIiLKHBGpU5Q5rx1wn0Tk8KXMEZGe1PuxmIQpg4DZrGOGiYjMizJHROqkzBGRuk3JnZZyR2ShqOcZilm4+6XuvsHdN4xZHz5+JiIyA2WOiNRJmSMidZuaO0sH3R0RqUkdN3jOAy40s01m9u4aliciC5syR0TqdpqZ3ancEZGaKHNEJFTpI1pmNgL8BfAp4LeBa8zscne/LfU73unQ2ZMYBEn66q4/O7dQdup/vyqu3IoHdDr6O8WPfD54bmKQrSGRnCcyMShhZRLbtL0jsf2CgZ1HjooHH5x4IB7UdeSoNcXlPbotrJscZDgYSDrapp4YaLdKc86cvcEAeqkBs4dAchDy6+8Ii9ud3gct9xtvj38QDQRYol2AR3/qxELZisvuC+u2UwNaXrGuUOQvKswO2z93xoNRpwYEbm8KBp4ewL7UfqQ4yK2NJgYeTA1uHBh/0dPD8kVfuTYs7/zkMwplI1fdHNZNDqgZDIDfCQZLTA5yX70WcD6wAdjCbLkztgg7sTjAZyc1aHmpngQD4HZKbpdgO9rq4uCsALYnMUDreHFg91RujZ8Uv49Z0O2R+xMDlide+yP+o5iJnUQ/OmviR1hGbisOiO0Hej9mUlKTNRxYG0+0MLIp7vfE0cXBTkcSg8D3o9/RuQgQn0OlJqRI9KN1XHHw3M79W+M22vE5RjTQa2tNceBlgPaDxdcW4klS7PEnhXWJT6GqVi5zyI6/dmJA/cbZGQ+aXeq9duMPw+JoAPAy75EAW84rHsMnfju+trXEsXDgS8cUykZfvLlUP8ropAbbTpzndaJjp+Q5YV8EGVN2IodoQoktb3xaWPe4P78yLN/5iuK50YrPXx/WTU6sFLxX+4HitZj77Nu56k/wnA3cBWxz93Hg08DLK16miCxcyhwRqdszgUfd/S7ljojUQJkjIklV3+A5Huj+0+qWvExEpArKHBGp27FA98cGlDsiUiVljogkVfGI1lbgk2bWAY4Djga+O9MvTBnlnfijqSIiCcocEalTd+ZAdmEVPOd5yJTMWbSi2t6JyOGmdOaAznVEFqq+3+Bx94uBiwHM7FzgorwMYD1T/7o++TuXApcCrLA1wzvwhYgMHWWOiNSpO3PgUO50VSnkTnfmrFx6rDJHRHo2l8zJf0/nOiILUNWPaF0DnG5mp5jZGHABcHnFyxSRhUuZIyJ1U+6ISJ2UOSKSZFXPOmFm5wMfAEaAj7n7/ztL/YeAe/Jv1wLTp1KJyvpRV22ojYXSRl3LO8ndi1MeVWzAmaM21MZCaWMY+zyQzIFyudND5qTKh3Gbq42F20YT+1xFG0071xn09lIbamOQyzsc2pg9c9x9aL+Aa3sp60ddtaE2Fkobg+hzU76GZXupDbUx7G0Me5+b8jUs20ttqI1hX96wt9GUr2HZXmpDbTS1z8PURuqr6ke0RERERERERESkYrrBIyIiIiIiIiLScMN+g+fSHsv6UVdt9NiGme3qLjCz1wM7+9EPM/uGmW0Iyi80s01m5ma2tmSfS/fjMG9jEH1uimHZXmpjqjO7v5lD5iT7MUPmfMrM7jSzW8zsY2a2KNVGP/rRwDaGvc9NMSzbS21MVcgcM/urPvXjyETmfNTMbjSzm8zsn8xsecm2lQGDbaMphmV7qY2pymRO2X5EmXOwvpl9sOvaTjnSrDZClQ+yLIcfM9vl7su7vn89sMHdL+xD298A3uXu104rfwawDfhGvqxoACoROQwNKHPOB76Uf3sZ8E13v2S+yxOR4TegzFnh7jvy//8lsNXd/2S+yxOR4TeIzMl/tgF4O/DK7uVLsw37J3ikYcxsnZl91syuyb+em5efbWZXmdn1ZnalmT0xL19qZp82s9vN7PPA0qhdd7/e3X9U35qISBNUmDlf9BzwPWB9bSslIkOrwsyZvLljeR39BVZEKsscMxsB/hz477WtjNRidNAdkEZaamY3dH2/Brg8////At7v7t82sxOBK4AzgDuAn3T3CTN7MfBe4FXAbwB73P0MM3sqcF1tayEiTTGwzMkfzfo1sr9wicjCMJDMMbOPA+cDtwH/rd8rJSJDaxCZcyFwubvfn91XlsOFbvDIXOx196dPfjP5McL82xcDT+4KihX5c+Qrgb8xs9PJ/iq1KP/584EPArj7TWZ2U/XdF5GGGWTmXEz2eNa3+rEiItIIA8kcd39D/lf1DwG/DHy8b2skIsOs1swxs+OAVwMv6PuayMDpBo/0Wws4x933dRfmA4V93d1faWYnk42lIyIyX5Vljpn9IbAO+C/z76aIHCYqPc9x97aZfZrssQnd4BGRKjLnGcBpwKb8xtEyM9vk7qf1pccyUBqDR/rtK8BvTX5jZpN3o1cCP87///qu+t8EXpPXPRN4avVdFJHDSCWZY2ZvBl4C/Iq7d/rbZRFpsL5njmVOm/w/8Atkj1+IiPQ9c9z939z9GHc/2d1PJnukSzd3DhO6wSP99jZgg2XTfN4G/Hpe/mfA/zSz65n6ybFLgOVmdjvwR8D3o0bN7G1mtoVsoNObzOwjla2BiDRJJZkDfBg4GrjKzG4wsz+opvsi0jBVZI6RPWpxM3AzcGxeV0SkqvMcOUxpmnQRERERERERkYbTJ3hERERERERERBpON3hERERERERERBpON3hERERERERERBpON3hERERERERERBpON3hERERERERERBpON3hERERERERERBpON3hERERERERERBpON3hERERERERERBru/wcDPme+wEZ8IAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1152x576 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Input: EURMWST 19,00% A9,04 EUR 1,99 EUR# 29/02/2021\n",
            "Predicted date: 2ï¿­ 0ï¿­ 1ï¿­ 9 ï¿­-ï¿­ 0ï¿­ 9 ï¿­-ï¿­ 1ï¿­ 9\n",
            "Predicted amount: 1ï¿­ 0ï¿­ 0 ï¿­.ï¿­ 0ï¿­ 0\n",
            "Predicted country: ESP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQvCvGQSxA97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer.save_weights(\"weights.combo.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.596Z"
        },
        "colab_type": "code",
        "id": "t-kFyiOLH0xg",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "f74be712-f70c-4130-bd56-8cdb7ff91fb0"
      },
      "source": [
        "!tar -czvf checkpoints.tar.gz checkpoints"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoints/\n",
            "checkpoints/train/\n",
            "checkpoints/train/ckpt-8.index\n",
            "checkpoints/train/ckpt-7.index\n",
            "checkpoints/train/ckpt-6.data-00001-of-00002\n",
            "checkpoints/train/ckpt-7.data-00001-of-00002\n",
            "checkpoints/train/ckpt-10.index\n",
            "checkpoints/train/ckpt-10.data-00000-of-00002\n",
            "checkpoints/train/ckpt-9.data-00001-of-00002\n",
            "checkpoints/train/checkpoint\n",
            "checkpoints/train/ckpt-9.data-00000-of-00002\n",
            "checkpoints/train/ckpt-8.data-00000-of-00002\n",
            "checkpoints/train/ckpt-7.data-00000-of-00002\n",
            "checkpoints/train/ckpt-6.data-00000-of-00002\n",
            "checkpoints/train/ckpt-6.index\n",
            "checkpoints/train/ckpt-9.index\n",
            "checkpoints/train/.DS_Store\n",
            "checkpoints/train/ckpt-8.data-00001-of-00002\n",
            "checkpoints/train/ckpt-10.data-00001-of-00002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FsKGQ8277SXg",
        "colab": {}
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "def eval_dataset(df):\n",
        "    date_pred = []\n",
        "    date_gt = []\n",
        "    amount_pred = []\n",
        "    amount_gt = []\n",
        "    country_pred = []\n",
        "    country_gt = []\n",
        "    ocr_text = []\n",
        "    pbar = tqdm(total=df.shape[0])\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "      try:\n",
        "        date, amount, country = translate(row[\"ocr_text\"], log=False)\n",
        "      except Exception:\n",
        "        continue\n",
        "\n",
        "      date_pred.append(date)\n",
        "      amount_pred.append(amount)\n",
        "      country_pred.append(country)\n",
        "      date_gt.append(row[\"date\"])\n",
        "      amount_gt.append(row[\"amount\"])\n",
        "      country_gt.append(row[\"country\"])\n",
        "      ocr_text.append(row[\"ocr_text\"])\n",
        "\n",
        "      pbar.update()\n",
        "        \n",
        "    df_result = pd.DataFrame()\n",
        "    df_result[\"dt_pred\"] = date_pred\n",
        "    df_result[\"dt_gt\"] = date_gt\n",
        "    df_result[\"ocr_text\"] = ocr_text\n",
        "    df_result[\"amt_pred\"] = amount_pred\n",
        "    df_result[\"amt_gt\"] = amount_gt\n",
        "    df_result[\"country_gt\"] = country_gt\n",
        "    df_result[\"country_pred\"] = country_pred\n",
        "    \n",
        "    df_result[\"country_correct\"] = df_result[\"country_pred\"] == df_result[\"country_gt\"]\n",
        "    df_result[\"amt_correct\"] = df_result[\"amt_pred\"] == df_result[\"amt_gt\"]\n",
        "    df_result[\"dt_correct\"] = df_result[\"dt_pred\"] == df_result[\"dt_gt\"]\n",
        "    df_result[\"dt_correct_mnt_day\"] = df_result.apply(lambda x: \"-\".join(x[\"dt_pred\"].split(\"-\")[1:3]) == \"-\".join(x[\"dt_gt\"].split(\"-\")[1:3]), axis=1)\n",
        "    df_result[\"dt_correct_day\"] = df_result.apply(lambda x: \"-\".join(x[\"dt_pred\"].split(\"-\")[2:3]) == \"-\".join(x[\"dt_gt\"].split(\"-\")[2:3]), axis=1)\n",
        "    df_result[\"dt_correct_month\"] = df_result.apply(lambda x: \"-\".join(x[\"dt_pred\"].split(\"-\")[1:2]) == \"-\".join(x[\"dt_gt\"].split(\"-\")[1:2]), axis=1)\n",
        "    df_result[\"dt_correct_year\"] = df_result.apply(lambda x: \"-\".join(x[\"dt_pred\"].split(\"-\")[0:1]) == \"-\".join(x[\"dt_gt\"].split(\"-\")[0:1]), axis=1)\n",
        "    \n",
        "    print(\"Country Accuracy {0:.2%}\".format(df_result['country_correct'].mean()))\n",
        "    print(\"Amount Accuracy {0:.2%}\".format(df_result['amt_correct'].mean()))\n",
        "    print(\"Date Accuracy {0:.2%}\".format(df_result['dt_correct'].mean()))\n",
        "    print(\"Month-Day Accuracy {0:.2%}\".format(df_result['dt_correct_mnt_day'].mean()))\n",
        "    print(\"Day Accuracy {0:.2%}\".format(df_result['dt_correct_day'].mean()))\n",
        "    print(\"Month Accuracy {0:.2%}\".format(df_result['dt_correct_month'].mean()))\n",
        "    print(\"Year Accuracy {0:.2%}\".format(df_result['dt_correct_year'].mean()))\n",
        "    \n",
        "    return df_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TOVY-GVzu7Y3",
        "colab": {}
      },
      "source": [
        "transformer.save_weights(\"combo.weights.14.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjJxKjTftP1x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "303799afa5e449008b0cee8aaf4f594d",
            "3db39f9896674cccaf8e887ed86429a7",
            "1be5f6ff8d614fccb0d0852e4b98937b",
            "48f38c0c4a4a40f59e2bb666ac60b260",
            "bf518af2ff5b49dcab27b6c22afc0225",
            "af8d9a20fa464a6dbf716582957ffc8e",
            "edf7928d9c1e42a1b17f895a43fe8aac",
            "2db29f5a3f43416299ccbc8362d70959"
          ]
        },
        "outputId": "c0e3ee79-2207-4086-af2a-f52c94a888b3"
      },
      "source": [
        "eval_result_df = eval_dataset(testing_df.sample(n=2000, random_state=0))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "303799afa5e449008b0cee8aaf4f594d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Country Accuracy 91.30%\n",
            "Amount Accuracy 64.45%\n",
            "Date Accuracy 63.00%\n",
            "Month-Day Accuracy 64.40%\n",
            "Day Accuracy 68.10%\n",
            "Month Accuracy 76.35%\n",
            "Year Accuracy 88.20%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95-GkuvIQXP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "N=2000\n",
        "eval_df = pd.read_csv(\"eval.tsv\",\n",
        "                          sep='\\t',\n",
        "                          float_precision='round_trip',\n",
        "                          dtype={'amount': str,\n",
        "                                 'date': str,\n",
        "                                 'country': str,\n",
        "                                 'currency': str,\n",
        "                                 'ocr_text': object})\n",
        "\n",
        "eval_df[\"ocr_text\"] = eval_df[\"ocr_text\"].apply(lambda x: x.lower())\n",
        "eval_df = eval_df.sample(n=N, random_state=0)\n",
        "eval_result_df = eval_dataset(eval_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_HSBTh9R83i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}