{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transformer_dates_amounts.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {
        "height": "359px",
        "width": "160px"
      },
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/renjithsasidharan/JSON-Airlines/blob/master/transformer_dates_amounts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWfG3BLaOcjN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a3c7502a-abbd-46a2-af0d-941687ae1660"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zJjX6ljOYHW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "630779aa-565c-4773-f8de-6e7f6abdcc99"
      },
      "source": [
        "!unzip ./drive/My\\ Drive/training_data/er_transformer/training_8_with_dates__amt.zip\n",
        "!cp -r ./training_8_with_dates__amt/* ./"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  ./drive/My Drive/training_data/er_transformer/training_8_with_dates__amt.zip\n",
            "   creating: training_8_with_dates__amt/\n",
            "  inflating: training_8_with_dates__amt/transformer_dates.py  \n",
            "  inflating: training_8_with_dates__amt/train.tsv.amount.subwords  \n",
            "  inflating: training_8_with_dates__amt/ocr_text.py  \n",
            "  inflating: training_8_with_dates__amt/training_helper.py  \n",
            "  inflating: training_8_with_dates__amt/.DS_Store  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/training_8_with_dates__amt/\n",
            "  inflating: __MACOSX/training_8_with_dates__amt/._.DS_Store  \n",
            "  inflating: training_8_with_dates__amt/train.tsv.ocr.subwords  \n",
            "  inflating: training_8_with_dates__amt/transformer_dates.ipynb  \n",
            "  inflating: training_8_with_dates__amt/train.tsv  \n",
            "  inflating: training_8_with_dates__amt/checkpoints.tar.gz  \n",
            "  inflating: __MACOSX/training_8_with_dates__amt/._checkpoints.tar.gz  \n",
            "  inflating: training_8_with_dates__amt/create_training_data_test.py  \n",
            "  inflating: training_8_with_dates__amt/training_log.txt  \n",
            "  inflating: training_8_with_dates__amt/config.json  \n",
            "  inflating: training_8_with_dates__amt/training_loss.png  \n",
            "   creating: training_8_with_dates__amt/__pycache__/\n",
            "  inflating: training_8_with_dates__amt/__pycache__/training_helper.cpython-36.pyc  \n",
            "  inflating: training_8_with_dates__amt/__pycache__/ocr_text.cpython-36.pyc  \n",
            "  inflating: training_8_with_dates__amt/currency_classes.json  \n",
            "  inflating: training_8_with_dates__amt/data.json  \n",
            "  inflating: training_8_with_dates__amt/train.tsv.date.subwords  \n",
            "  inflating: training_8_with_dates__amt/.rsyncignore  \n",
            "  inflating: training_8_with_dates__amt/test.tsv  \n",
            "  inflating: training_8_with_dates__amt/transformer_dates_amounts.ipynb  \n",
            "  inflating: training_8_with_dates__amt/country_classes.json  \n",
            "   creating: training_8_with_dates__amt/.ipynb_checkpoints/\n",
            "  inflating: training_8_with_dates__amt/.ipynb_checkpoints/transformer_dates-checkpoint.ipynb  \n",
            "  inflating: training_8_with_dates__amt/.ipynb_checkpoints/transformer_dates_amounts-checkpoint.ipynb  \n",
            "  inflating: training_8_with_dates__amt/training_accuracy.png  \n",
            "  inflating: training_8_with_dates__amt/create_training_data.py  \n",
            "  inflating: training_8_with_dates__amt/analyze_logs.py  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJieQZPgCrdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf ./checkpoints/train/*"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J0Qjg6vuaHNt"
      },
      "source": [
        "# Transformer model for language understanding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:21.831641Z",
          "start_time": "2019-11-10T06:32:19.420135Z"
        },
        "colab_type": "code",
        "id": "JjJJyJTZYebt",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:21.837459Z",
          "start_time": "2019-11-10T06:32:21.833884Z"
        },
        "colab_type": "code",
        "id": "y7hAaZD7uYZa",
        "outputId": "91b57222-ddd6-4f24-b426-abb25e7c1bc4",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fd1NWMxjfsDd"
      },
      "source": [
        "## Setup input pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:21.842399Z",
          "start_time": "2019-11-10T06:32:21.839958Z"
        },
        "colab_type": "code",
        "id": "w4cXX4eKuYZh",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "train_file_path='./train.tsv'\n",
        "test_file_path='./test.tsv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:23.718935Z",
          "start_time": "2019-11-10T06:32:21.844334Z"
        },
        "colab_type": "code",
        "id": "89PSUjtfuYZj",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "training_df = pd.read_csv(train_file_path,\n",
        "                          sep='\\t',\n",
        "                          float_precision='round_trip',\n",
        "                          dtype={'amount': str, 'date': str, 'ocr_text': object})\n",
        "\n",
        "testing_df = pd.read_csv(test_file_path,\n",
        "                          sep='\\t',\n",
        "                          dtype={'amount': str, 'date': str, 'ocr_text': object})\n",
        "\n",
        "\n",
        "training_examples = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(training_df['ocr_text'].values, tf.string),\n",
        "            tf.cast(training_df['amount'].values, tf.string),\n",
        "            tf.cast(training_df['date'].values, tf.string)\n",
        "        )\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "val_examples = (\n",
        "    tf.data.Dataset.from_tensor_slices(\n",
        "        (\n",
        "            tf.cast(testing_df['ocr_text'].values, tf.string),\n",
        "            tf.cast(testing_df['amount'].values, tf.string),\n",
        "            tf.cast(testing_df['date'].values, tf.string)\n",
        "        )\n",
        "    )\n",
        ")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RCEKotqosGfq"
      },
      "source": [
        "Create a custom subwords tokenizer from the training dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:23.812888Z",
          "start_time": "2019-11-10T06:32:23.720666Z"
        },
        "colab_type": "code",
        "id": "KVBg5Q8tBk5z",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "06cbd087-d630-45b9-8477-37e43cf9029c"
      },
      "source": [
        "print(\"{}.ocr.subwords\".format(train_file_path))\n",
        "if os.path.isfile(\"{}.ocr.subwords\".format(train_file_path)):\n",
        "  print('Loading input tokenizer from disk')\n",
        "  tokenizer_ocr = tfds.features.text.SubwordTextEncoder.load_from_file(train_file_path + \".ocr\")\n",
        "else:\n",
        "  print('Creating new tokenizer from training set')\n",
        "  tokenizer_ocr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "      (ocr.numpy() for ocr, _, _ in training_examples), target_vocab_size=2**13)\n",
        "  tokenizer_ocr.save_to_file(train_file_path + \".ocr\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./train.tsv.ocr.subwords\n",
            "Loading input tokenizer from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.021201Z",
          "start_time": "2019-11-10T06:32:23.814923Z"
        },
        "colab_type": "code",
        "id": "-jxOS89-uYZp",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "tokenizer_date = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (date.numpy() for _, _, date in training_examples), target_vocab_size=2**9)\n",
        "tokenizer_date.save_to_file(train_file_path + \".date\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0mYFVU5OMVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_amount = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "    (amount.numpy() for _, amount, _ in training_examples), target_vocab_size=2**9)\n",
        "tokenizer_amount.save_to_file(train_file_path + \".amount\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlV1E42iCVzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_amount = tokenizer_ocr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.026527Z",
          "start_time": "2019-11-10T06:32:42.023029Z"
        },
        "pycharm": {
          "is_executing": false
        },
        "id": "VTiBw2AsOMVg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "657f5eda-eb7e-4802-b0d8-d8e222efeaa8"
      },
      "source": [
        "date_str = \"13/11/2019\"\n",
        "print('Original date {}'.format(date_str))\n",
        "print('Tokenized date {}'.format(tokenizer_date.encode(date_str)))\n",
        "print('Tokens {}'.format([tokenizer_date.decode([x]) for x in tokenizer_date.encode(date_str)]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original date 13/11/2019\n",
            "Tokenized date [21, 89, 5, 89, 1]\n",
            "Tokens ['13', '/', '11', '/', '2019']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MxO2Zpl8OMVj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c44f9238-ead2-490e-f554-88fe19ec9bd3"
      },
      "source": [
        "amount_str = \"2185.90\"\n",
        "print('Original amount {}'.format(amount_str))\n",
        "print('Tokenized amount {}'.format(tokenizer_amount.encode(amount_str)))\n",
        "print('Tokens {}'.format([tokenizer_amount.decode([x]) for x in tokenizer_amount.encode(amount_str)]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original amount 2185.90\n",
            "Tokenized amount [2011, 7727, 6346, 55]\n",
            "Tokens ['218', '5', '.', '90']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.035344Z",
          "start_time": "2019-11-10T06:32:42.029953Z"
        },
        "colab_type": "code",
        "id": "4DYWukNFkGQN",
        "outputId": "6925cc3f-8cec-4409-a87c-efcb20905f82",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "sample_string = 'The target date is 13.11.2019 and amount is 199.55'\n",
        "\n",
        "tokenized_string = tokenizer_ocr.encode(sample_string)\n",
        "print ('Tokenized string is {}'.format(tokenized_string))\n",
        "\n",
        "original_string = tokenizer_ocr.decode(tokenized_string)\n",
        "print ('The original string: {}'.format(original_string))\n",
        "\n",
        "print('Tokens {}'.format([tokenizer_ocr.decode([x]) for x in tokenizer_ocr.encode(original_string)]))\n",
        "\n",
        "assert original_string == sample_string"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized string is [2733, 1306, 3562, 7706, 1891, 574, 25, 6346, 17, 6346, 237, 401, 3354, 574, 1872, 6346, 127]\n",
            "The original string: The target date is 13.11.2019 and amount is 199.55\n",
            "Tokens ['The ', 'tar', 'get', ' ', 'date ', 'is ', '13', '.', '11', '.', '2019 ', 'and ', 'amount ', 'is ', '199', '.', '55']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.042778Z",
          "start_time": "2019-11-10T06:32:42.038651Z"
        },
        "colab_type": "code",
        "id": "sfY5_rr-CnVF",
        "outputId": "2c08fb7a-7344-46fa-aadf-1b7300e82012",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(tokenizer_amount.vocab_size)\n",
        "print(tokenizer_date.vocab_size)\n",
        "print(tokenizer_ocr.vocab_size)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7930\n",
            "298\n",
            "7930\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o9KJWJjrsZ4Y"
      },
      "source": [
        "The tokenizer encodes the string by breaking it into subwords if the word is not in its dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.051074Z",
          "start_time": "2019-11-10T06:32:42.045507Z"
        },
        "colab_type": "code",
        "id": "bf2ntBxjkqK6",
        "outputId": "d4a668b1-beb5-4ddf-ec1b-9e40131f1088",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_ocr.decode([ts])))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2733 ----> The \n",
            "1306 ----> tar\n",
            "3562 ----> get\n",
            "7706 ---->  \n",
            "1891 ----> date \n",
            "574 ----> is \n",
            "25 ----> 13\n",
            "6346 ----> .\n",
            "17 ----> 11\n",
            "6346 ----> .\n",
            "237 ----> 2019 \n",
            "401 ----> and \n",
            "3354 ----> amount \n",
            "574 ----> is \n",
            "1872 ----> 199\n",
            "6346 ----> .\n",
            "127 ----> 55\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.056061Z",
          "start_time": "2019-11-10T06:32:42.053148Z"
        },
        "colab_type": "code",
        "id": "bcRp7VcQ5m6g",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kGi4PoVakxdc"
      },
      "source": [
        "Add a start and end token to the input and target. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.062005Z",
          "start_time": "2019-11-10T06:32:42.057738Z"
        },
        "colab_type": "code",
        "id": "UZwnPr4R055s",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def encode(ocr_param, amount_param, date_param):\n",
        "  ocr = [tokenizer_ocr.vocab_size] + tokenizer_ocr.encode(\n",
        "      ocr_param.numpy()) + [tokenizer_ocr.vocab_size+1]\n",
        "\n",
        "  date = [tokenizer_date.vocab_size] + tokenizer_date.encode(\n",
        "      date_param.numpy()) + [tokenizer_date.vocab_size+1]\n",
        "    \n",
        "  amount = [tokenizer_amount.vocab_size] + tokenizer_amount.encode(\n",
        "      amount_param.numpy()) + [tokenizer_amount.vocab_size+1]\n",
        "  \n",
        "  return ocr, amount, date"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6JrGp5Gek6Ql"
      },
      "source": [
        "Note: To keep this example small and relatively fast, drop examples with a length of over 40 tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.067042Z",
          "start_time": "2019-11-10T06:32:42.064342Z"
        },
        "colab_type": "code",
        "id": "2QEgbjntk6Yf",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 800"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.072228Z",
          "start_time": "2019-11-10T06:32:42.069216Z"
        },
        "colab_type": "code",
        "id": "c081xPGv1CPI",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def filter_max_length(x, y1, y2, max_length=MAX_LENGTH):\n",
        "  return tf.logical_and(tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y1) <= max_length), tf.size(y2) <= max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tx1sFbR-9fRs"
      },
      "source": [
        "Operations inside `.map()` run in graph mode and receive a graph tensor that do not have a numpy attribute. The `tokenizer` expects a string or Unicode symbol to encode it into integers. Hence, you need to run the encoding inside a `tf.py_function`, which receives an eager tensor having a numpy attribute that contains the string value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.077875Z",
          "start_time": "2019-11-10T06:32:42.074724Z"
        },
        "colab_type": "code",
        "id": "Mah1cS-P70Iz",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def tf_encode(ocr, amount, date):\n",
        "  return tf.py_function(encode, [ocr, amount, date], [tf.int64, tf.int64, tf.int64])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.088195Z",
          "start_time": "2019-11-10T06:32:42.079882Z"
        },
        "colab_type": "code",
        "id": "8iWeIrANuYZ9",
        "outputId": "033a8d5a-e9cc-4adf-93b8-d578e9fd927b",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_examples"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: ((), (), ()), types: (tf.string, tf.string, tf.string)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.368141Z",
          "start_time": "2019-11-10T06:32:42.089872Z"
        },
        "colab_type": "code",
        "id": "9mk9AZdZ5bcS",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "train_dataset = training_examples \\\n",
        "                .map(tf_encode) \\\n",
        "                .filter(filter_max_length) \\\n",
        "                .cache() \\\n",
        "                .shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1], [-1])) \\\n",
        "                .prefetch(tf.data.experimental.AUTOTUNE) \\\n",
        "\n",
        "valid_dataset = val_examples.map(tf_encode) \\\n",
        "                .filter(filter_max_length).padded_batch(BATCH_SIZE, padded_shapes=([-1], [-1], [-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nBQuibYA4n0n"
      },
      "source": [
        "## Positional encoding\n",
        "\n",
        "Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence. \n",
        "\n",
        "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the *similarity of their meaning and their position in the sentence*, in the d-dimensional space.\n",
        "\n",
        "See the notebook on [positional encoding](https://github.com/tensorflow/examples/blob/master/community/en/position_encoding.ipynb) to learn more about it. The formula for calculating the positional encoding is as follows:\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = cos(pos / 10000^{2i / d_{model}})} $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.372964Z",
          "start_time": "2019-11-10T06:32:42.369990Z"
        },
        "colab_type": "code",
        "id": "WhIOZjMNKujn",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.379237Z",
          "start_time": "2019-11-10T06:32:42.374655Z"
        },
        "colab_type": "code",
        "id": "1Rz82wEs5biZ",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.622473Z",
          "start_time": "2019-11-10T06:32:42.381024Z"
        },
        "colab_type": "code",
        "id": "1kLCla68EloE",
        "outputId": "8157b257-2f06-4b82-94aa-5e616e653d52",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        }
      },
      "source": [
        "pos_encoding = positional_encoding(50, 512)\n",
        "print (pos_encoding.shape)\n",
        "\n",
        "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.xlim((0, 512))\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 50, 512)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "a_b4ou4TYqUN"
      },
      "source": [
        "## Masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s42Uydjkv0hF"
      },
      "source": [
        "Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input. The mask indicates where pad value `0` is present: it outputs a `1` at those locations, and a `0` otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.627869Z",
          "start_time": "2019-11-10T06:32:42.624456Z"
        },
        "colab_type": "code",
        "id": "U2i8-e1s8ti9",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.635845Z",
          "start_time": "2019-11-10T06:32:42.629584Z"
        },
        "colab_type": "code",
        "id": "A7BYeBCNvi7n",
        "outputId": "cf8de804-73f8-4f02-a41c-651ec87bb024",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
        "create_padding_mask(x)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
              "array([[[[0., 0., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z0hzukDBgVom"
      },
      "source": [
        "The look-ahead mask is used to mask the future tokens in a sequence. In other words, the mask indicates which entries should not be used.\n",
        "\n",
        "This means that to predict the third word, only the first and second word will be used. Similarly to predict the fourth word, only the first, second and the third word will be used and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-11-10T06:32:42.643086Z",
          "start_time": "2019-11-10T06:32:42.639838Z"
        },
        "colab_type": "code",
        "id": "dVxS8OPI9uI0",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.481Z"
        },
        "colab_type": "code",
        "id": "yxKGuXxaBeeE",
        "outputId": "09df852f-bf7e-433a-b83b-e69518953609",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "x = tf.random.uniform((1, 3))\n",
        "temp = create_look_ahead_mask(x.shape[1])\n",
        "temp"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
              "array([[0., 1., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xluDl5cXYy4y"
      },
      "source": [
        "## Scaled dot product attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vsxEE_-Wa1gF"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png\" width=\"500\" alt=\"scaled_dot_product_attention\">\n",
        "\n",
        "The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n",
        "\n",
        "$$\\Large{Attention(Q, K, V) = softmax_k(\\frac{QK^T}{\\sqrt{d_k}}) V} $$\n",
        "\n",
        "The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax. \n",
        "\n",
        "For example, consider that `Q` and `K` have a mean of 0 and variance of 1. Their matrix multiplication will have a mean of 0 and variance of `dk`. Hence, *square root of `dk`* is used for scaling (and not any other number) because the matmul of `Q` and `K` should have a mean of 0 and variance of 1, and you get a gentler softmax.\n",
        "\n",
        "The mask is multiplied with -1e9 (close to negative infinity). This is done because the mask is summed with the scaled matrix multiplication of Q and K and is applied immediately before a softmax. The goal is to zero out these cells, and large negative inputs to softmax are near zero in the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.485Z"
        },
        "colab_type": "code",
        "id": "LazzUq3bJ5SH",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FiqETnhCkoXh"
      },
      "source": [
        "As the softmax normalization is done on K, its values decide the amount of importance given to Q.\n",
        "\n",
        "The output represents the multiplication of the attention weights and the V (value) vector. This ensures that the words you want to focus on are kept as-is and the irrelevant words are flushed out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.488Z"
        },
        "colab_type": "code",
        "id": "n90YjClyInFy",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def print_out(q, k, v):\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\n",
        "      q, k, v, None)\n",
        "  print ('Attention weights are:')\n",
        "  print (temp_attn)\n",
        "  print ('Output is:')\n",
        "  print (temp_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.491Z"
        },
        "colab_type": "code",
        "id": "yAzUAf2DPlNt",
        "outputId": "48e7e929-852d-4431-ba53-55453688b51b",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "\n",
        "temp_k = tf.constant([[10,0,0],\n",
        "                      [0,10,0],\n",
        "                      [0,0,10],\n",
        "                      [0,0,10]], dtype=tf.float32)  # (4, 3)\n",
        "\n",
        "temp_v = tf.constant([[   1,0],\n",
        "                      [  10,0],\n",
        "                      [ 100,5],\n",
        "                      [1000,6]], dtype=tf.float32)  # (4, 2)\n",
        "\n",
        "# This `query` aligns with the second `key`,\n",
        "# so the second `value` is returned.\n",
        "temp_q = tf.constant([[0, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[10.  0.]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.494Z"
        },
        "colab_type": "code",
        "id": "zg6k-fGhgXra",
        "outputId": "d226715b-f3d1-4309-c473-f512ef9109fa",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# This query aligns with a repeated key (third and fourth), \n",
        "# so all associated values get averaged.\n",
        "temp_q = tf.constant([[0, 0, 10]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0.  0.  0.5 0.5]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[550.    5.5]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.496Z"
        },
        "colab_type": "code",
        "id": "UAq3YOzUgXhb",
        "outputId": "90b29a0b-ff0a-4620-99b3-73e8a87ad19e",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# This query aligns equally with the first and second key, \n",
        "# so their values get averaged.\n",
        "temp_q = tf.constant([[10, 10, 0]], dtype=tf.float32)  # (1, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor([[0.5 0.5 0.  0. ]], shape=(1, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor([[5.5 0. ]], shape=(1, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aOz-4_XIhaTP"
      },
      "source": [
        "Pass all the queries together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.499Z"
        },
        "colab_type": "code",
        "id": "6dlU8Tm-hYrF",
        "outputId": "dd507b2f-c628-4991-f9fa-e3bb8c594511",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "temp_q = tf.constant([[0, 0, 10], [0, 10, 0], [10, 10, 0]], dtype=tf.float32)  # (3, 3)\n",
        "print_out(temp_q, temp_k, temp_v)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention weights are:\n",
            "tf.Tensor(\n",
            "[[0.  0.  0.5 0.5]\n",
            " [0.  1.  0.  0. ]\n",
            " [0.5 0.5 0.  0. ]], shape=(3, 4), dtype=float32)\n",
            "Output is:\n",
            "tf.Tensor(\n",
            "[[550.    5.5]\n",
            " [ 10.    0. ]\n",
            " [  5.5   0. ]], shape=(3, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kmzGPEy64qmA"
      },
      "source": [
        "## Multi-head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fz5BMC8Kaoqo"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png\" width=\"500\" alt=\"multi-head attention\">\n",
        "\n",
        "\n",
        "Multi-head attention consists of four parts:\n",
        "*    Linear layers and split into heads.\n",
        "*    Scaled dot-product attention.\n",
        "*    Concatenation of heads.\n",
        "*    Final linear layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JPmbr6F1C-v_"
      },
      "source": [
        "Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through linear (Dense) layers and split up into multiple heads. \n",
        "\n",
        "The `scaled_dot_product_attention` defined above is applied to each head (broadcasted for efficiency). An appropriate mask must be used in the attention step.  The attention output for each head is then concatenated (using `tf.transpose`, and `tf.reshape`) and put through a final `Dense` layer.\n",
        "\n",
        "Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.503Z"
        },
        "colab_type": "code",
        "id": "BSV3PPKsYecw",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0D8FJue5lDyZ"
      },
      "source": [
        "Create a `MultiHeadAttention` layer to try out. At each location in the sequence, `y`, the `MultiHeadAttention` runs all 8 attention heads across all other locations in the sequence, returning a new vector of the same length at each location."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.507Z"
        },
        "colab_type": "code",
        "id": "Hu94p-_-2_BX",
        "outputId": "25039d79-a026-4673-a426-45457e2195d4",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
        "out.shape, attn.shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RdDqGayx67vv"
      },
      "source": [
        "## Point wise feed forward network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gBqzJXGfHK3X"
      },
      "source": [
        "Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.511Z"
        },
        "colab_type": "code",
        "id": "ET7xLt0yCT6Z",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.514Z"
        },
        "colab_type": "code",
        "id": "mytb1lPyOHLB",
        "outputId": "2a70cc35-012d-48a2-d582-478d843a7576",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_ffn = point_wise_feed_forward_network(512, 2048)\n",
        "sample_ffn(tf.random.uniform((64, 50, 512))).shape"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7e7hKcxn6-zd"
      },
      "source": [
        "## Encoder and decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yScbC0MUH8dS"
      },
      "source": [
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\" width=\"600\" alt=\"transformer\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MfYJG-Kvgwy2"
      },
      "source": [
        "The transformer model follows the same general pattern as a standard [sequence to sequence with attention model](nmt_with_attention.ipynb). \n",
        "\n",
        "* The input sentence is passed through `N` encoder layers that generates an output for each word/token in the sequence.\n",
        "* The decoder attends on the encoder's output and its own input (self-attention) to predict the next word. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QFv-FNYUmvpn"
      },
      "source": [
        "### Encoder layer\n",
        "\n",
        "Each encoder layer consists of sublayers:\n",
        "\n",
        "1.   Multi-head attention (with padding mask) \n",
        "2.    Point wise feed forward networks. \n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n",
        "\n",
        "The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis. There are N encoder layers in the transformer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.519Z"
        },
        "colab_type": "code",
        "id": "ncyS-Ms3i2x_",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.521Z"
        },
        "colab_type": "code",
        "id": "AzZRXdO0mI48",
        "outputId": "e54ae69e-64cc-420c-8878-1019df391ae1",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_encoder_layer = EncoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_encoder_layer_output = sample_encoder_layer(\n",
        "    tf.random.uniform((64, 43, 512)), False, None)\n",
        "\n",
        "sample_encoder_layer_output.shape  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 43, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6LO_48Owmx_o"
      },
      "source": [
        "### Decoder layer\n",
        "\n",
        "Each decoder layer consists of sublayers:\n",
        "\n",
        "1.   Masked multi-head attention (with look ahead mask and padding mask)\n",
        "2.   Multi-head attention (with padding mask). V (value) and K (key) receive the *encoder output* as inputs. Q (query) receives the *output from the masked multi-head attention sublayer.*\n",
        "3.   Point wise feed forward networks\n",
        "\n",
        "Each of these sublayers has a residual connection around it followed by a layer normalization. The output of each sublayer is `LayerNorm(x + Sublayer(x))`. The normalization is done on the `d_model` (last) axis.\n",
        "\n",
        "There are N decoder layers in the transformer.\n",
        "\n",
        "As Q receives the output from decoder's first attention block, and K receives the encoder output, the attention weights represent the importance given to the decoder's input based on the encoder's output. In other words, the decoder predicts the next word by looking at the encoder output and self-attending to its own output. See the demonstration above in the scaled dot product attention section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.524Z"
        },
        "colab_type": "code",
        "id": "9SoX0-vd1hue",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.527Z"
        },
        "colab_type": "code",
        "id": "Ne2Bqx8k71l0",
        "outputId": "5eb0135a-e669-4008-8aaa-4ee41e9b88dd",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_decoder_layer = DecoderLayer(512, 8, 2048)\n",
        "\n",
        "sample_decoder_layer_output, _, _ = sample_decoder_layer(\n",
        "    tf.random.uniform((64, 50, 512)), sample_encoder_layer_output, \n",
        "    False, None, None)\n",
        "\n",
        "sample_decoder_layer_output.shape  # (batch_size, target_seq_len, d_model)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 50, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SE1H51Ajm0q1"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "The `Encoder` consists of:\n",
        "1.   Input Embedding\n",
        "2.   Positional Encoding\n",
        "3.   N encoder layers\n",
        "\n",
        "The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.530Z"
        },
        "colab_type": "code",
        "id": "jpEox7gJ8FCI",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.533Z"
        },
        "colab_type": "code",
        "id": "8QG9nueFQKXx",
        "outputId": "ed5f4408-2a53-4846-9710-1130d053e30c",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, input_vocab_size=8500,\n",
        "                         maximum_position_encoding=10000)\n",
        "\n",
        "sample_encoder_output = sample_encoder(tf.random.uniform((64, 62)), \n",
        "                                       training=False, mask=None)\n",
        "\n",
        "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 62, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "p-uO6ls8m2O5"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZtT7PKzrXkNr"
      },
      "source": [
        " The `Decoder` consists of:\n",
        "1.   Output Embedding\n",
        "2.   Positional Encoding\n",
        "3.   N decoder layers\n",
        "\n",
        "The target is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the decoder layers. The output of the decoder is the input to the final linear layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.537Z"
        },
        "colab_type": "code",
        "id": "d5_d5-PLQXwY",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.540Z"
        },
        "colab_type": "code",
        "id": "a1jXoAMRZyvu",
        "outputId": "3a27ff3e-2e8a-4ca8-a354-86f69596d3ef",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, target_vocab_size=8000,\n",
        "                         maximum_position_encoding=5000)\n",
        "\n",
        "output, attn = sample_decoder(tf.random.uniform((64, 26)), \n",
        "                              enc_output=sample_encoder_output, \n",
        "                              training=False, look_ahead_mask=None, \n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y54xnJnuYgJ7"
      },
      "source": [
        "## Create the Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uERO1y54cOKq"
      },
      "source": [
        "Transformer consists of the encoder, decoder and a final linear layer. The output of the decoder is the input to the linear layer and its output is returned."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.543Z"
        },
        "colab_type": "code",
        "id": "PED3bIpOYkBu",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self,\n",
        "               num_layers,\n",
        "               d_model,\n",
        "               num_heads,\n",
        "               dff,\n",
        "               input_vocab_size, \n",
        "               target_vocab_size_dt,\n",
        "               target_vocab_size_amt,\n",
        "               pe_input,\n",
        "               pe_target_dt,\n",
        "               pe_target_amt,\n",
        "               rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                           input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder_dt = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size_dt, pe_target_dt, rate)\n",
        "    \n",
        "    self.decoder_amt = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size_amt, pe_target_amt, rate)\n",
        "\n",
        "    self.final_layer_dt = tf.keras.layers.Dense(target_vocab_size_dt)\n",
        "    self.final_layer_amt = tf.keras.layers.Dense(target_vocab_size_amt)\n",
        "    \n",
        "  def call(self,\n",
        "           inp,\n",
        "           tar_dt,\n",
        "           tar_amt,\n",
        "           training,\n",
        "           enc_padding_mask, \n",
        "           look_ahead_mask_dt,\n",
        "           look_ahead_mask_amt,\n",
        "           dec_padding_mask):\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output_dt, attention_weights_dt = self.decoder_dt(\n",
        "        tar_dt, enc_output, training, look_ahead_mask_dt, dec_padding_mask)\n",
        "    \n",
        "    dec_output_amt, attention_weights_amt = self.decoder_amt(\n",
        "        tar_amt, enc_output, training, look_ahead_mask_amt, dec_padding_mask)\n",
        "    \n",
        "    final_output_dt = self.final_layer_dt(dec_output_dt)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    final_output_amt = self.final_layer_amt(dec_output_amt)\n",
        "    \n",
        "    return final_output_dt, attention_weights_dt, final_output_amt, attention_weights_amt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.546Z"
        },
        "colab_type": "code",
        "id": "tJ4fbQcIkHW1",
        "outputId": "73466dbe-a4b8-4e82-c964-9feb58d2a346",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
        "    input_vocab_size=8500,\n",
        "    target_vocab_size_dt=2000,\n",
        "    target_vocab_size_amt=1000, \n",
        "    pe_input=10000,\n",
        "    pe_target_dt=6000,\n",
        "    pe_target_amt=6000)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 62))\n",
        "temp_target_dt = tf.random.uniform((64, 26))\n",
        "temp_target_amt = tf.random.uniform((64, 32))\n",
        "\n",
        "fn_out_dt, _, fn_out_amt, _ = sample_transformer(temp_input,\n",
        "                               temp_target_dt,\n",
        "                               temp_target_amt,\n",
        "                               training=False, \n",
        "                               enc_padding_mask=None, \n",
        "                               look_ahead_mask_dt=None,\n",
        "                               look_ahead_mask_amt=None,\n",
        "                               dec_padding_mask=None)\n",
        "\n",
        "print(fn_out_dt.shape)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "print(fn_out_amt.shape)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 26, 2000)\n",
            "(64, 32, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wsINyf1VEQLC"
      },
      "source": [
        "## Set hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zVjWCxFNcgbt"
      },
      "source": [
        "To keep this example small and relatively fast, the values for *num_layers, d_model, and dff* have been reduced. \n",
        "\n",
        "The values used in the base model of transformer were; *num_layers=6*, *d_model = 512*, *dff = 2048*. See the [paper](https://arxiv.org/abs/1706.03762) for all the other versions of the transformer.\n",
        "\n",
        "Note: By changing the values below, you can get the model that achieved state of the art on many tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.549Z"
        },
        "colab_type": "code",
        "id": "lnJn5SLA2ahP",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "num_layers = 2\n",
        "d_model = 64\n",
        "dff = 256\n",
        "num_heads = 4\n",
        "\n",
        "input_vocab_size = tokenizer_ocr.vocab_size + 2\n",
        "target_vocab_size_dt = tokenizer_date.vocab_size + 2\n",
        "target_vocab_size_amt = tokenizer_amount.vocab_size + 2\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xYEGhEOtzn5W"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GOmWW--yP3zx"
      },
      "source": [
        "Use the Adam optimizer with a custom learning rate scheduler according to the formula in the [paper](https://arxiv.org/abs/1706.03762).\n",
        "\n",
        "$$\\Large{lrate = d_{model}^{-0.5} * min(step{\\_}num^{-0.5}, step{\\_}num * warmup{\\_}steps^{-1.5})}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.553Z"
        },
        "colab_type": "code",
        "id": "iYQdOO1axwEI",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.555Z"
        },
        "colab_type": "code",
        "id": "7r4scdulztRx",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.558Z"
        },
        "colab_type": "code",
        "id": "f33ZCgvHpPdG",
        "outputId": "ff0d2f04-a499-441c-cfae-abb1a08b797f",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YgkDE7hzo8r5"
      },
      "source": [
        "## Loss and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oxGJtoDuYIHL"
      },
      "source": [
        "Since the target sequences are padded, it is important to apply a padding mask when calculating the loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.561Z"
        },
        "colab_type": "code",
        "id": "MlhsJMm0TW_B",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.564Z"
        },
        "colab_type": "code",
        "id": "67oqVHiT0Eiu",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.566Z"
        },
        "colab_type": "code",
        "id": "phlyxMnm-Tpx",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "train_loss_dt = tf.keras.metrics.Mean(name='train_loss_dt')\n",
        "train_loss_amt = tf.keras.metrics.Mean(name='train_loss_amt')\n",
        "train_accuracy_dt = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy_dt')\n",
        "train_accuracy_amt = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy_amt')\n",
        "\n",
        "valid_loss_dt = tf.keras.metrics.Mean(name='valid_loss_dt')\n",
        "valid_loss_amt = tf.keras.metrics.Mean(name='valid_loss_amt')\n",
        "valid_accuracy_dt = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='valid_accuracy_dt')\n",
        "valid_accuracy_amt = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='valid_accuracy_amt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aeHumfr7zmMa"
      },
      "source": [
        "## Training and checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.569Z"
        },
        "colab_type": "code",
        "id": "UiysUa--4tOU",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size,\n",
        "                          target_vocab_size_dt,\n",
        "                          target_vocab_size_amt,\n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target_dt=target_vocab_size_dt,\n",
        "                          pe_target_amt=target_vocab_size_amt,\n",
        "                          rate=dropout_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.571Z"
        },
        "colab_type": "code",
        "id": "ZOJUSB1T8GjM",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar_dt, tar_amt):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by \n",
        "  # the decoder.\n",
        "  look_ahead_mask_dt = create_look_ahead_mask(tf.shape(tar_dt)[1])\n",
        "  dec_target_padding_mask_dt = create_padding_mask(tar_dt)\n",
        "  combined_mask_dt = tf.maximum(dec_target_padding_mask_dt, look_ahead_mask_dt)\n",
        "\n",
        "  look_ahead_mask_amt = create_look_ahead_mask(tf.shape(tar_amt)[1])\n",
        "  dec_target_padding_mask_amt = create_padding_mask(tar_amt)\n",
        "  combined_mask_amt = tf.maximum(dec_target_padding_mask_amt, look_ahead_mask_amt)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask_dt, dec_padding_mask, combined_mask_amt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fzuf06YZp66w"
      },
      "source": [
        "Create the checkpoint path and the checkpoint manager. This will be used to save checkpoints every `n` epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.574Z"
        },
        "colab_type": "code",
        "id": "hNhuYfllndLZ",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"./checkpoints/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Di_Yaa1gf9r"
      },
      "source": [
        "The target is divided into tar_inp and tar_real. tar_inp is passed as an input to the decoder. `tar_real` is that same input shifted by 1: At each location in `tar_input`, `tar_real` contains the  next token that should be predicted.\n",
        "\n",
        "For example, `sentence` = \"SOS A lion in the jungle is sleeping EOS\"\n",
        "\n",
        "`tar_inp` =  \"SOS A lion in the jungle is sleeping\"\n",
        "\n",
        "`tar_real` = \"A lion in the jungle is sleeping EOS\"\n",
        "\n",
        "The transformer is an auto-regressive model: it makes predictions one part at a time, and uses its output so far to decide what to do next. \n",
        "\n",
        "During training this example uses teacher-forcing (like in the [text generation tutorial](./text_generation.ipynb)). Teacher forcing is passing the true output to the next time step regardless of what the model predicts at the current time step.\n",
        "\n",
        "As the transformer predicts each word, *self-attention* allows it to look at the previous words in the input sequence to better predict the next word.\n",
        "\n",
        "To prevent the model from peaking at the expected output the model uses a look-ahead mask."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.577Z"
        },
        "colab_type": "code",
        "id": "LKpoA6q1sJFj",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.579Z"
        },
        "colab_type": "code",
        "id": "iJwmp9OE29oj",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar_dt, tar_amt):\n",
        "  tar_inp_dt = tar_dt[:, :-1]\n",
        "  tar_real_dt = tar_dt[:, 1:]\n",
        "  tar_inp_amt = tar_amt[:, :-1]\n",
        "  tar_real_amt = tar_amt[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask_dt, dec_padding_mask, combined_mask_amt = create_masks(inp, tar_inp_dt, tar_inp_amt)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions_dt, _, predictions_amt, _ = transformer(inp,\n",
        "                                 tar_inp_dt,\n",
        "                                 tar_inp_amt,\n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask_dt,\n",
        "                                 combined_mask_amt,\n",
        "                                 dec_padding_mask)\n",
        "    loss_dt = loss_function(tar_real_dt, predictions_dt)\n",
        "    loss_amt = loss_function(tar_real_amt, predictions_amt)\n",
        "    loss = loss_dt + loss_amt\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss_dt(loss_dt)\n",
        "  train_loss_amt(loss_amt)\n",
        "  train_accuracy_dt(tar_real_dt, predictions_dt)\n",
        "  train_accuracy_amt(tar_real_amt, predictions_amt)\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def valid_epoch(inp, tar_dt, tar_amt):\n",
        "  tar_inp_dt = tar_dt[:, :-1]\n",
        "  tar_real_dt = tar_dt[:, 1:]\n",
        "  tar_inp_amt = tar_amt[:, :-1]\n",
        "  tar_real_amt = tar_amt[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask_dt, dec_padding_mask, combined_mask_amt = create_masks(inp, tar_inp_dt, tar_inp_amt)\n",
        "  \n",
        "  predictions_dt, _, predictions_amt, _ = transformer(inp, \n",
        "                               tar_inp_dt,\n",
        "                               tar_inp_amt, \n",
        "                               False, \n",
        "                               enc_padding_mask, \n",
        "                               combined_mask_dt,\n",
        "                               combined_mask_amt, \n",
        "                               dec_padding_mask)\n",
        "  loss_dt = loss_function(tar_real_dt, predictions_dt)\n",
        "  loss_amt = loss_function(tar_real_amt, predictions_amt)\n",
        "  \n",
        "  valid_loss_dt(loss_dt)\n",
        "  valid_loss_amt(loss_amt)\n",
        "  valid_accuracy_dt(tar_real_dt, predictions_dt)\n",
        "  valid_accuracy_amt(tar_real_amt, predictions_amt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qM2PDWGDJ_8V"
      },
      "source": [
        "Ocr text is used as the input language and amount is the target language."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.582Z"
        },
        "colab_type": "code",
        "id": "bbvmaKNiznHZ",
        "outputId": "48517999-4257-4d88-8db0-1d017388c163",
        "pycharm": {
          "is_executing": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss_dt.reset_states()\n",
        "  train_accuracy_dt.reset_states()\n",
        "  train_loss_amt.reset_states()\n",
        "  train_accuracy_amt.reset_states()\n",
        "\n",
        "  valid_loss_dt.reset_states()\n",
        "  valid_accuracy_dt.reset_states()\n",
        "  valid_loss_amt.reset_states()\n",
        "  valid_accuracy_amt.reset_states()\n",
        "  \n",
        "  # inp -> ocr, tar -> amount\n",
        "  for (batch, (inp, tar_amt , tar_dt)) in enumerate(train_dataset):\n",
        "    train_step(inp, tar_dt, tar_amt)\n",
        "    \n",
        "    if batch % 50 == 0:\n",
        "      print ('Epoch {} Batch {} Training Loss Date {:.4f} Training Loss Amount {:.4f} Accuracy Date {:.4f} Accuracy Amount {:.4f}'.format(\n",
        "          epoch + 1, batch, train_loss_dt.result(), train_loss_amt.result(), train_accuracy_dt.result(), train_accuracy_amt.result()))\n",
        "      \n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
        "                                                         ckpt_save_path))\n",
        "    \n",
        "  print ('Epoch {} Training Loss Date {:.4f} Training Loss Amount {:.4f} Accuracy Date {:.4f} Accuracy Amount {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss_dt.result(), \n",
        "                                                train_loss_amt.result(), \n",
        "                                                train_accuracy_dt.result(),\n",
        "                                                train_accuracy_amt.result()))\n",
        "  \n",
        "  for (batch, (inp, tar_amt, tar_dt)) in enumerate(valid_dataset):\n",
        "    valid_epoch(inp, tar_dt, tar_amt)\n",
        "\n",
        "  print ('Epoch {} Validation Loss Date {:.4f} Validation Loss Amount {:.4f} Accuracy Date {:.4f} Accuracy Amount {:.4f}'.format(epoch + 1, \n",
        "                                                valid_loss_dt.result(), \n",
        "                                                valid_loss_amt.result(), \n",
        "                                                valid_accuracy_dt.result(),\n",
        "                                                valid_accuracy_amt.result()))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {:.2f} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Training Loss Date 5.9205 Training Loss Amount 7.1656 Accuracy Date 0.0000 Accuracy Amount 0.0000\n",
            "Epoch 1 Batch 50 Training Loss Date 5.6994 Training Loss Amount 7.9923 Accuracy Date 0.0074 Accuracy Amount 0.0007\n",
            "Epoch 1 Batch 100 Training Loss Date 5.2314 Training Loss Amount 7.7749 Accuracy Date 0.1561 Accuracy Amount 0.0948\n",
            "Epoch 1 Batch 150 Training Loss Date 4.8610 Training Loss Amount 7.6078 Accuracy Date 0.2161 Accuracy Amount 0.1411\n",
            "Epoch 1 Batch 200 Training Loss Date 4.5512 Training Loss Amount 7.4072 Accuracy Date 0.2768 Accuracy Amount 0.1633\n",
            "Epoch 1 Batch 250 Training Loss Date 4.2457 Training Loss Amount 7.2142 Accuracy Date 0.3224 Accuracy Amount 0.1892\n",
            "Epoch 1 Batch 300 Training Loss Date 3.9524 Training Loss Amount 6.9405 Accuracy Date 0.3593 Accuracy Amount 0.2179\n",
            "Epoch 1 Batch 350 Training Loss Date 3.6819 Training Loss Amount 6.6893 Accuracy Date 0.3875 Accuracy Amount 0.2434\n",
            "Epoch 1 Batch 400 Training Loss Date 3.4472 Training Loss Amount 6.3868 Accuracy Date 0.4096 Accuracy Amount 0.2645\n",
            "Epoch 1 Batch 450 Training Loss Date 3.2457 Training Loss Amount 6.0669 Accuracy Date 0.4269 Accuracy Amount 0.2819\n",
            "Epoch 1 Batch 500 Training Loss Date 3.0729 Training Loss Amount 5.7627 Accuracy Date 0.4406 Accuracy Amount 0.2968\n",
            "Epoch 1 Batch 550 Training Loss Date 2.9242 Training Loss Amount 5.4850 Accuracy Date 0.4526 Accuracy Amount 0.3095\n",
            "Epoch 1 Batch 600 Training Loss Date 2.7969 Training Loss Amount 5.2211 Accuracy Date 0.4624 Accuracy Amount 0.3223\n",
            "Epoch 1 Batch 650 Training Loss Date 2.6863 Training Loss Amount 4.9781 Accuracy Date 0.4708 Accuracy Amount 0.3359\n",
            "Epoch 1 Batch 700 Training Loss Date 2.5900 Training Loss Amount 4.7589 Accuracy Date 0.4785 Accuracy Amount 0.3481\n",
            "Epoch 1 Batch 750 Training Loss Date 2.5043 Training Loss Amount 4.5601 Accuracy Date 0.4859 Accuracy Amount 0.3590\n",
            "Epoch 1 Batch 800 Training Loss Date 2.4250 Training Loss Amount 4.3795 Accuracy Date 0.4950 Accuracy Amount 0.3680\n",
            "Epoch 1 Batch 850 Training Loss Date 2.3504 Training Loss Amount 4.2209 Accuracy Date 0.5054 Accuracy Amount 0.3768\n",
            "Epoch 1 Batch 900 Training Loss Date 2.2803 Training Loss Amount 4.0773 Accuracy Date 0.5156 Accuracy Amount 0.3843\n",
            "Epoch 1 Batch 950 Training Loss Date 2.2151 Training Loss Amount 3.9499 Accuracy Date 0.5253 Accuracy Amount 0.3910\n",
            "Epoch 1 Batch 1000 Training Loss Date 2.1548 Training Loss Amount 3.8328 Accuracy Date 0.5346 Accuracy Amount 0.3966\n",
            "Epoch 1 Batch 1050 Training Loss Date 2.0992 Training Loss Amount 3.7284 Accuracy Date 0.5430 Accuracy Amount 0.4024\n",
            "Epoch 1 Batch 1100 Training Loss Date 2.0477 Training Loss Amount 3.6338 Accuracy Date 0.5510 Accuracy Amount 0.4081\n",
            "Epoch 1 Batch 1150 Training Loss Date 2.0001 Training Loss Amount 3.5455 Accuracy Date 0.5585 Accuracy Amount 0.4131\n",
            "Epoch 1 Batch 1200 Training Loss Date 1.9558 Training Loss Amount 3.4640 Accuracy Date 0.5654 Accuracy Amount 0.4176\n",
            "Epoch 1 Batch 1250 Training Loss Date 1.9149 Training Loss Amount 3.3895 Accuracy Date 0.5718 Accuracy Amount 0.4220\n",
            "Epoch 1 Batch 1300 Training Loss Date 1.8769 Training Loss Amount 3.3180 Accuracy Date 0.5778 Accuracy Amount 0.4255\n",
            "Epoch 1 Batch 1350 Training Loss Date 1.8408 Training Loss Amount 3.2516 Accuracy Date 0.5837 Accuracy Amount 0.4290\n",
            "Epoch 1 Batch 1400 Training Loss Date 1.8073 Training Loss Amount 3.1912 Accuracy Date 0.5890 Accuracy Amount 0.4327\n",
            "Epoch 1 Batch 1450 Training Loss Date 1.7759 Training Loss Amount 3.1345 Accuracy Date 0.5940 Accuracy Amount 0.4362\n",
            "Epoch 1 Batch 1500 Training Loss Date 1.7468 Training Loss Amount 3.0798 Accuracy Date 0.5988 Accuracy Amount 0.4395\n",
            "Epoch 1 Batch 1550 Training Loss Date 1.7192 Training Loss Amount 3.0284 Accuracy Date 0.6032 Accuracy Amount 0.4428\n",
            "Epoch 1 Batch 1600 Training Loss Date 1.6927 Training Loss Amount 2.9801 Accuracy Date 0.6075 Accuracy Amount 0.4459\n",
            "Epoch 1 Batch 1650 Training Loss Date 1.6682 Training Loss Amount 2.9334 Accuracy Date 0.6115 Accuracy Amount 0.4491\n",
            "Epoch 1 Batch 1700 Training Loss Date 1.6441 Training Loss Amount 2.8883 Accuracy Date 0.6154 Accuracy Amount 0.4523\n",
            "Epoch 1 Batch 1750 Training Loss Date 1.6217 Training Loss Amount 2.8461 Accuracy Date 0.6191 Accuracy Amount 0.4554\n",
            "Epoch 1 Batch 1800 Training Loss Date 1.5999 Training Loss Amount 2.8053 Accuracy Date 0.6228 Accuracy Amount 0.4583\n",
            "Epoch 1 Batch 1850 Training Loss Date 1.5795 Training Loss Amount 2.7661 Accuracy Date 0.6262 Accuracy Amount 0.4612\n",
            "Epoch 1 Batch 1900 Training Loss Date 1.5594 Training Loss Amount 2.7284 Accuracy Date 0.6296 Accuracy Amount 0.4640\n",
            "Epoch 1 Batch 1950 Training Loss Date 1.5406 Training Loss Amount 2.6916 Accuracy Date 0.6328 Accuracy Amount 0.4671\n",
            "Epoch 1 Batch 2000 Training Loss Date 1.5227 Training Loss Amount 2.6559 Accuracy Date 0.6358 Accuracy Amount 0.4700\n",
            "Epoch 1 Batch 2050 Training Loss Date 1.5052 Training Loss Amount 2.6228 Accuracy Date 0.6388 Accuracy Amount 0.4729\n",
            "Epoch 1 Batch 2100 Training Loss Date 1.4885 Training Loss Amount 2.5902 Accuracy Date 0.6417 Accuracy Amount 0.4761\n",
            "Epoch 1 Batch 2150 Training Loss Date 1.4723 Training Loss Amount 2.5583 Accuracy Date 0.6445 Accuracy Amount 0.4790\n",
            "Epoch 1 Batch 2200 Training Loss Date 1.4567 Training Loss Amount 2.5285 Accuracy Date 0.6472 Accuracy Amount 0.4822\n",
            "Epoch 1 Batch 2250 Training Loss Date 1.4417 Training Loss Amount 2.4982 Accuracy Date 0.6500 Accuracy Amount 0.4848\n",
            "Epoch 1 Batch 2300 Training Loss Date 1.4271 Training Loss Amount 2.4688 Accuracy Date 0.6526 Accuracy Amount 0.4876\n",
            "Epoch 1 Batch 2350 Training Loss Date 1.4130 Training Loss Amount 2.4416 Accuracy Date 0.6552 Accuracy Amount 0.4905\n",
            "Epoch 1 Batch 2400 Training Loss Date 1.3991 Training Loss Amount 2.4145 Accuracy Date 0.6577 Accuracy Amount 0.4933\n",
            "Epoch 1 Batch 2450 Training Loss Date 1.3859 Training Loss Amount 2.3885 Accuracy Date 0.6601 Accuracy Amount 0.4959\n",
            "Epoch 1 Batch 2500 Training Loss Date 1.3728 Training Loss Amount 2.3635 Accuracy Date 0.6625 Accuracy Amount 0.4989\n",
            "Epoch 1 Batch 2550 Training Loss Date 1.3598 Training Loss Amount 2.3381 Accuracy Date 0.6650 Accuracy Amount 0.5016\n",
            "Epoch 1 Batch 2600 Training Loss Date 1.3474 Training Loss Amount 2.3143 Accuracy Date 0.6673 Accuracy Amount 0.5044\n",
            "Epoch 1 Batch 2650 Training Loss Date 1.3351 Training Loss Amount 2.2900 Accuracy Date 0.6696 Accuracy Amount 0.5070\n",
            "Epoch 1 Batch 2700 Training Loss Date 1.3230 Training Loss Amount 2.2669 Accuracy Date 0.6719 Accuracy Amount 0.5096\n",
            "Epoch 1 Batch 2750 Training Loss Date 1.3114 Training Loss Amount 2.2436 Accuracy Date 0.6741 Accuracy Amount 0.5121\n",
            "Epoch 1 Batch 2800 Training Loss Date 1.2999 Training Loss Amount 2.2213 Accuracy Date 0.6763 Accuracy Amount 0.5148\n",
            "Epoch 1 Batch 2850 Training Loss Date 1.2888 Training Loss Amount 2.1996 Accuracy Date 0.6784 Accuracy Amount 0.5171\n",
            "Epoch 1 Batch 2900 Training Loss Date 1.2778 Training Loss Amount 2.1784 Accuracy Date 0.6806 Accuracy Amount 0.5195\n",
            "Epoch 1 Batch 2950 Training Loss Date 1.2669 Training Loss Amount 2.1571 Accuracy Date 0.6827 Accuracy Amount 0.5218\n",
            "Epoch 1 Batch 3000 Training Loss Date 1.2561 Training Loss Amount 2.1371 Accuracy Date 0.6849 Accuracy Amount 0.5245\n",
            "Epoch 1 Batch 3050 Training Loss Date 1.2459 Training Loss Amount 2.1178 Accuracy Date 0.6869 Accuracy Amount 0.5269\n",
            "Epoch 1 Training Loss Date 1.2395 Training Loss Amount 2.1055 Accuracy Date 0.6882 Accuracy Amount 0.5285\n",
            "Epoch 1 Validation Loss Date 0.5713 Validation Loss Amount 0.8574 Accuracy Date 0.8249 Accuracy Amount 0.6870\n",
            "Time taken for 1 epoch: 791.54 secs\n",
            "\n",
            "Epoch 2 Batch 0 Training Loss Date 0.6045 Training Loss Amount 1.0649 Accuracy Date 0.8125 Accuracy Amount 0.7695\n",
            "Epoch 2 Batch 50 Training Loss Date 0.6162 Training Loss Amount 0.8923 Accuracy Date 0.8120 Accuracy Amount 0.6894\n",
            "Epoch 2 Batch 100 Training Loss Date 0.6166 Training Loss Amount 0.9063 Accuracy Date 0.8127 Accuracy Amount 0.6776\n",
            "Epoch 2 Batch 150 Training Loss Date 0.6127 Training Loss Amount 0.9030 Accuracy Date 0.8137 Accuracy Amount 0.6775\n",
            "Epoch 2 Batch 200 Training Loss Date 0.6097 Training Loss Amount 0.8973 Accuracy Date 0.8151 Accuracy Amount 0.6798\n",
            "Epoch 2 Batch 250 Training Loss Date 0.6082 Training Loss Amount 0.8906 Accuracy Date 0.8156 Accuracy Amount 0.6747\n",
            "Epoch 2 Batch 300 Training Loss Date 0.6052 Training Loss Amount 0.8875 Accuracy Date 0.8163 Accuracy Amount 0.6789\n",
            "Epoch 2 Batch 350 Training Loss Date 0.6012 Training Loss Amount 0.8830 Accuracy Date 0.8172 Accuracy Amount 0.6821\n",
            "Epoch 2 Batch 400 Training Loss Date 0.6003 Training Loss Amount 0.8804 Accuracy Date 0.8175 Accuracy Amount 0.6841\n",
            "Epoch 2 Batch 450 Training Loss Date 0.5971 Training Loss Amount 0.8742 Accuracy Date 0.8184 Accuracy Amount 0.6857\n",
            "Epoch 2 Batch 500 Training Loss Date 0.5953 Training Loss Amount 0.8684 Accuracy Date 0.8188 Accuracy Amount 0.6895\n",
            "Epoch 2 Batch 550 Training Loss Date 0.5927 Training Loss Amount 0.8634 Accuracy Date 0.8195 Accuracy Amount 0.6912\n",
            "Epoch 2 Batch 600 Training Loss Date 0.5898 Training Loss Amount 0.8577 Accuracy Date 0.8203 Accuracy Amount 0.6925\n",
            "Epoch 2 Batch 650 Training Loss Date 0.5878 Training Loss Amount 0.8547 Accuracy Date 0.8209 Accuracy Amount 0.6922\n",
            "Epoch 2 Batch 700 Training Loss Date 0.5859 Training Loss Amount 0.8511 Accuracy Date 0.8216 Accuracy Amount 0.6937\n",
            "Epoch 2 Batch 750 Training Loss Date 0.5831 Training Loss Amount 0.8466 Accuracy Date 0.8225 Accuracy Amount 0.6943\n",
            "Epoch 2 Batch 800 Training Loss Date 0.5806 Training Loss Amount 0.8418 Accuracy Date 0.8233 Accuracy Amount 0.6947\n",
            "Epoch 2 Batch 850 Training Loss Date 0.5784 Training Loss Amount 0.8396 Accuracy Date 0.8239 Accuracy Amount 0.6955\n",
            "Epoch 2 Batch 900 Training Loss Date 0.5771 Training Loss Amount 0.8367 Accuracy Date 0.8244 Accuracy Amount 0.6965\n",
            "Epoch 2 Batch 950 Training Loss Date 0.5753 Training Loss Amount 0.8332 Accuracy Date 0.8251 Accuracy Amount 0.6969\n",
            "Epoch 2 Batch 1000 Training Loss Date 0.5727 Training Loss Amount 0.8284 Accuracy Date 0.8259 Accuracy Amount 0.6970\n",
            "Epoch 2 Batch 1050 Training Loss Date 0.5710 Training Loss Amount 0.8256 Accuracy Date 0.8263 Accuracy Amount 0.6974\n",
            "Epoch 2 Batch 1100 Training Loss Date 0.5691 Training Loss Amount 0.8231 Accuracy Date 0.8269 Accuracy Amount 0.6979\n",
            "Epoch 2 Batch 1150 Training Loss Date 0.5669 Training Loss Amount 0.8180 Accuracy Date 0.8277 Accuracy Amount 0.6989\n",
            "Epoch 2 Batch 1200 Training Loss Date 0.5654 Training Loss Amount 0.8157 Accuracy Date 0.8281 Accuracy Amount 0.7000\n",
            "Epoch 2 Batch 1250 Training Loss Date 0.5634 Training Loss Amount 0.8117 Accuracy Date 0.8287 Accuracy Amount 0.7008\n",
            "Epoch 2 Batch 1300 Training Loss Date 0.5619 Training Loss Amount 0.8085 Accuracy Date 0.8292 Accuracy Amount 0.7018\n",
            "Epoch 2 Batch 1350 Training Loss Date 0.5602 Training Loss Amount 0.8051 Accuracy Date 0.8296 Accuracy Amount 0.7028\n",
            "Epoch 2 Batch 1400 Training Loss Date 0.5585 Training Loss Amount 0.8012 Accuracy Date 0.8301 Accuracy Amount 0.7028\n",
            "Epoch 2 Batch 1450 Training Loss Date 0.5561 Training Loss Amount 0.7982 Accuracy Date 0.8308 Accuracy Amount 0.7044\n",
            "Epoch 2 Batch 1500 Training Loss Date 0.5543 Training Loss Amount 0.7958 Accuracy Date 0.8314 Accuracy Amount 0.7048\n",
            "Epoch 2 Batch 1550 Training Loss Date 0.5521 Training Loss Amount 0.7927 Accuracy Date 0.8322 Accuracy Amount 0.7058\n",
            "Epoch 2 Batch 1600 Training Loss Date 0.5501 Training Loss Amount 0.7888 Accuracy Date 0.8327 Accuracy Amount 0.7065\n",
            "Epoch 2 Batch 1650 Training Loss Date 0.5484 Training Loss Amount 0.7864 Accuracy Date 0.8332 Accuracy Amount 0.7072\n",
            "Epoch 2 Batch 1700 Training Loss Date 0.5466 Training Loss Amount 0.7834 Accuracy Date 0.8338 Accuracy Amount 0.7078\n",
            "Epoch 2 Batch 1750 Training Loss Date 0.5448 Training Loss Amount 0.7813 Accuracy Date 0.8344 Accuracy Amount 0.7088\n",
            "Epoch 2 Batch 1800 Training Loss Date 0.5430 Training Loss Amount 0.7781 Accuracy Date 0.8350 Accuracy Amount 0.7089\n",
            "Epoch 2 Batch 1850 Training Loss Date 0.5411 Training Loss Amount 0.7756 Accuracy Date 0.8356 Accuracy Amount 0.7101\n",
            "Epoch 2 Batch 1900 Training Loss Date 0.5393 Training Loss Amount 0.7734 Accuracy Date 0.8361 Accuracy Amount 0.7105\n",
            "Epoch 2 Batch 1950 Training Loss Date 0.5376 Training Loss Amount 0.7710 Accuracy Date 0.8366 Accuracy Amount 0.7111\n",
            "Epoch 2 Batch 2000 Training Loss Date 0.5358 Training Loss Amount 0.7693 Accuracy Date 0.8372 Accuracy Amount 0.7121\n",
            "Epoch 2 Batch 2050 Training Loss Date 0.5340 Training Loss Amount 0.7666 Accuracy Date 0.8378 Accuracy Amount 0.7125\n",
            "Epoch 2 Batch 2100 Training Loss Date 0.5324 Training Loss Amount 0.7644 Accuracy Date 0.8383 Accuracy Amount 0.7130\n",
            "Epoch 2 Batch 2150 Training Loss Date 0.5305 Training Loss Amount 0.7618 Accuracy Date 0.8389 Accuracy Amount 0.7139\n",
            "Epoch 2 Batch 2200 Training Loss Date 0.5290 Training Loss Amount 0.7599 Accuracy Date 0.8394 Accuracy Amount 0.7149\n",
            "Epoch 2 Batch 2250 Training Loss Date 0.5274 Training Loss Amount 0.7578 Accuracy Date 0.8399 Accuracy Amount 0.7155\n",
            "Epoch 2 Batch 2300 Training Loss Date 0.5256 Training Loss Amount 0.7553 Accuracy Date 0.8405 Accuracy Amount 0.7158\n",
            "Epoch 2 Batch 2350 Training Loss Date 0.5240 Training Loss Amount 0.7532 Accuracy Date 0.8410 Accuracy Amount 0.7160\n",
            "Epoch 2 Batch 2400 Training Loss Date 0.5222 Training Loss Amount 0.7511 Accuracy Date 0.8415 Accuracy Amount 0.7165\n",
            "Epoch 2 Batch 2450 Training Loss Date 0.5206 Training Loss Amount 0.7489 Accuracy Date 0.8420 Accuracy Amount 0.7172\n",
            "Epoch 2 Batch 2500 Training Loss Date 0.5191 Training Loss Amount 0.7465 Accuracy Date 0.8425 Accuracy Amount 0.7178\n",
            "Epoch 2 Batch 2550 Training Loss Date 0.5178 Training Loss Amount 0.7444 Accuracy Date 0.8429 Accuracy Amount 0.7184\n",
            "Epoch 2 Batch 2600 Training Loss Date 0.5166 Training Loss Amount 0.7425 Accuracy Date 0.8433 Accuracy Amount 0.7191\n",
            "Epoch 2 Batch 2650 Training Loss Date 0.5148 Training Loss Amount 0.7403 Accuracy Date 0.8439 Accuracy Amount 0.7197\n",
            "Epoch 2 Batch 2700 Training Loss Date 0.5131 Training Loss Amount 0.7384 Accuracy Date 0.8445 Accuracy Amount 0.7200\n",
            "Epoch 2 Batch 2750 Training Loss Date 0.5116 Training Loss Amount 0.7363 Accuracy Date 0.8449 Accuracy Amount 0.7204\n",
            "Epoch 2 Batch 2800 Training Loss Date 0.5101 Training Loss Amount 0.7345 Accuracy Date 0.8454 Accuracy Amount 0.7207\n",
            "Epoch 2 Batch 2850 Training Loss Date 0.5087 Training Loss Amount 0.7322 Accuracy Date 0.8459 Accuracy Amount 0.7208\n",
            "Epoch 2 Batch 2900 Training Loss Date 0.5074 Training Loss Amount 0.7300 Accuracy Date 0.8463 Accuracy Amount 0.7211\n",
            "Epoch 2 Batch 2950 Training Loss Date 0.5063 Training Loss Amount 0.7282 Accuracy Date 0.8466 Accuracy Amount 0.7213\n",
            "Epoch 2 Batch 3000 Training Loss Date 0.5051 Training Loss Amount 0.7265 Accuracy Date 0.8471 Accuracy Amount 0.7219\n",
            "Epoch 2 Batch 3050 Training Loss Date 0.5037 Training Loss Amount 0.7251 Accuracy Date 0.8475 Accuracy Amount 0.7226\n",
            "Epoch 2 Training Loss Date 0.5029 Training Loss Amount 0.7239 Accuracy Date 0.8478 Accuracy Amount 0.7228\n",
            "Epoch 2 Validation Loss Date 0.3839 Validation Loss Amount 0.5738 Accuracy Date 0.8874 Accuracy Amount 0.7478\n",
            "Time taken for 1 epoch: 566.54 secs\n",
            "\n",
            "Epoch 3 Batch 0 Training Loss Date 0.4468 Training Loss Amount 0.6217 Accuracy Date 0.8750 Accuracy Amount 0.6594\n",
            "Epoch 3 Batch 50 Training Loss Date 0.4350 Training Loss Amount 0.6193 Accuracy Date 0.8687 Accuracy Amount 0.7213\n",
            "Epoch 3 Batch 100 Training Loss Date 0.4288 Training Loss Amount 0.6133 Accuracy Date 0.8715 Accuracy Amount 0.7366\n",
            "Epoch 3 Batch 150 Training Loss Date 0.4252 Training Loss Amount 0.6112 Accuracy Date 0.8725 Accuracy Amount 0.7413\n",
            "Epoch 3 Batch 200 Training Loss Date 0.4247 Training Loss Amount 0.6159 Accuracy Date 0.8729 Accuracy Amount 0.7447\n",
            "Epoch 3 Batch 250 Training Loss Date 0.4249 Training Loss Amount 0.6116 Accuracy Date 0.8731 Accuracy Amount 0.7471\n",
            "Epoch 3 Batch 300 Training Loss Date 0.4252 Training Loss Amount 0.6116 Accuracy Date 0.8732 Accuracy Amount 0.7487\n",
            "Epoch 3 Batch 350 Training Loss Date 0.4238 Training Loss Amount 0.6102 Accuracy Date 0.8735 Accuracy Amount 0.7508\n",
            "Epoch 3 Batch 400 Training Loss Date 0.4237 Training Loss Amount 0.6114 Accuracy Date 0.8736 Accuracy Amount 0.7508\n",
            "Epoch 3 Batch 450 Training Loss Date 0.4238 Training Loss Amount 0.6123 Accuracy Date 0.8734 Accuracy Amount 0.7499\n",
            "Epoch 3 Batch 500 Training Loss Date 0.4233 Training Loss Amount 0.6131 Accuracy Date 0.8735 Accuracy Amount 0.7497\n",
            "Epoch 3 Batch 550 Training Loss Date 0.4222 Training Loss Amount 0.6131 Accuracy Date 0.8739 Accuracy Amount 0.7498\n",
            "Epoch 3 Batch 600 Training Loss Date 0.4216 Training Loss Amount 0.6115 Accuracy Date 0.8740 Accuracy Amount 0.7496\n",
            "Epoch 3 Batch 650 Training Loss Date 0.4217 Training Loss Amount 0.6109 Accuracy Date 0.8739 Accuracy Amount 0.7474\n",
            "Epoch 3 Batch 700 Training Loss Date 0.4212 Training Loss Amount 0.6095 Accuracy Date 0.8740 Accuracy Amount 0.7487\n",
            "Epoch 3 Batch 750 Training Loss Date 0.4206 Training Loss Amount 0.6087 Accuracy Date 0.8744 Accuracy Amount 0.7476\n",
            "Epoch 3 Batch 800 Training Loss Date 0.4206 Training Loss Amount 0.6074 Accuracy Date 0.8744 Accuracy Amount 0.7464\n",
            "Epoch 3 Batch 850 Training Loss Date 0.4195 Training Loss Amount 0.6080 Accuracy Date 0.8748 Accuracy Amount 0.7472\n",
            "Epoch 3 Batch 900 Training Loss Date 0.4198 Training Loss Amount 0.6062 Accuracy Date 0.8747 Accuracy Amount 0.7471\n",
            "Epoch 3 Batch 950 Training Loss Date 0.4193 Training Loss Amount 0.6054 Accuracy Date 0.8749 Accuracy Amount 0.7469\n",
            "Epoch 3 Batch 1000 Training Loss Date 0.4183 Training Loss Amount 0.6040 Accuracy Date 0.8753 Accuracy Amount 0.7471\n",
            "Epoch 3 Batch 1050 Training Loss Date 0.4172 Training Loss Amount 0.6031 Accuracy Date 0.8755 Accuracy Amount 0.7471\n",
            "Epoch 3 Batch 1100 Training Loss Date 0.4163 Training Loss Amount 0.6016 Accuracy Date 0.8758 Accuracy Amount 0.7476\n",
            "Epoch 3 Batch 1150 Training Loss Date 0.4168 Training Loss Amount 0.5996 Accuracy Date 0.8756 Accuracy Amount 0.7476\n",
            "Epoch 3 Batch 1200 Training Loss Date 0.4163 Training Loss Amount 0.5987 Accuracy Date 0.8757 Accuracy Amount 0.7476\n",
            "Epoch 3 Batch 1250 Training Loss Date 0.4153 Training Loss Amount 0.5982 Accuracy Date 0.8761 Accuracy Amount 0.7475\n",
            "Epoch 3 Batch 1300 Training Loss Date 0.4152 Training Loss Amount 0.5979 Accuracy Date 0.8760 Accuracy Amount 0.7475\n",
            "Epoch 3 Batch 1350 Training Loss Date 0.4143 Training Loss Amount 0.5971 Accuracy Date 0.8763 Accuracy Amount 0.7486\n",
            "Epoch 3 Batch 1400 Training Loss Date 0.4138 Training Loss Amount 0.5959 Accuracy Date 0.8765 Accuracy Amount 0.7486\n",
            "Epoch 3 Batch 1450 Training Loss Date 0.4133 Training Loss Amount 0.5946 Accuracy Date 0.8766 Accuracy Amount 0.7484\n",
            "Epoch 3 Batch 1500 Training Loss Date 0.4131 Training Loss Amount 0.5931 Accuracy Date 0.8768 Accuracy Amount 0.7480\n",
            "Epoch 3 Batch 1550 Training Loss Date 0.4127 Training Loss Amount 0.5924 Accuracy Date 0.8769 Accuracy Amount 0.7483\n",
            "Epoch 3 Batch 1600 Training Loss Date 0.4119 Training Loss Amount 0.5918 Accuracy Date 0.8773 Accuracy Amount 0.7486\n",
            "Epoch 3 Batch 1650 Training Loss Date 0.4115 Training Loss Amount 0.5908 Accuracy Date 0.8774 Accuracy Amount 0.7489\n",
            "Epoch 3 Batch 1700 Training Loss Date 0.4110 Training Loss Amount 0.5906 Accuracy Date 0.8776 Accuracy Amount 0.7486\n",
            "Epoch 3 Batch 1750 Training Loss Date 0.4105 Training Loss Amount 0.5905 Accuracy Date 0.8778 Accuracy Amount 0.7492\n",
            "Epoch 3 Batch 1800 Training Loss Date 0.4098 Training Loss Amount 0.5892 Accuracy Date 0.8779 Accuracy Amount 0.7502\n",
            "Epoch 3 Batch 1850 Training Loss Date 0.4099 Training Loss Amount 0.5898 Accuracy Date 0.8779 Accuracy Amount 0.7501\n",
            "Epoch 3 Batch 1900 Training Loss Date 0.4096 Training Loss Amount 0.5894 Accuracy Date 0.8779 Accuracy Amount 0.7505\n",
            "Epoch 3 Batch 1950 Training Loss Date 0.4093 Training Loss Amount 0.5889 Accuracy Date 0.8780 Accuracy Amount 0.7510\n",
            "Epoch 3 Batch 2000 Training Loss Date 0.4089 Training Loss Amount 0.5885 Accuracy Date 0.8781 Accuracy Amount 0.7513\n",
            "Epoch 3 Batch 2050 Training Loss Date 0.4084 Training Loss Amount 0.5881 Accuracy Date 0.8783 Accuracy Amount 0.7515\n",
            "Epoch 3 Batch 2100 Training Loss Date 0.4080 Training Loss Amount 0.5878 Accuracy Date 0.8784 Accuracy Amount 0.7515\n",
            "Epoch 3 Batch 2150 Training Loss Date 0.4073 Training Loss Amount 0.5873 Accuracy Date 0.8786 Accuracy Amount 0.7516\n",
            "Epoch 3 Batch 2200 Training Loss Date 0.4070 Training Loss Amount 0.5869 Accuracy Date 0.8787 Accuracy Amount 0.7520\n",
            "Epoch 3 Batch 2250 Training Loss Date 0.4064 Training Loss Amount 0.5861 Accuracy Date 0.8789 Accuracy Amount 0.7519\n",
            "Epoch 3 Batch 2300 Training Loss Date 0.4059 Training Loss Amount 0.5851 Accuracy Date 0.8790 Accuracy Amount 0.7519\n",
            "Epoch 3 Batch 2350 Training Loss Date 0.4052 Training Loss Amount 0.5848 Accuracy Date 0.8791 Accuracy Amount 0.7519\n",
            "Epoch 3 Batch 2400 Training Loss Date 0.4048 Training Loss Amount 0.5837 Accuracy Date 0.8793 Accuracy Amount 0.7522\n",
            "Epoch 3 Batch 2450 Training Loss Date 0.4045 Training Loss Amount 0.5826 Accuracy Date 0.8794 Accuracy Amount 0.7518\n",
            "Epoch 3 Batch 2500 Training Loss Date 0.4039 Training Loss Amount 0.5819 Accuracy Date 0.8795 Accuracy Amount 0.7518\n",
            "Epoch 3 Batch 2550 Training Loss Date 0.4038 Training Loss Amount 0.5810 Accuracy Date 0.8796 Accuracy Amount 0.7518\n",
            "Epoch 3 Batch 2600 Training Loss Date 0.4033 Training Loss Amount 0.5806 Accuracy Date 0.8798 Accuracy Amount 0.7519\n",
            "Epoch 3 Batch 2650 Training Loss Date 0.4032 Training Loss Amount 0.5801 Accuracy Date 0.8799 Accuracy Amount 0.7519\n",
            "Epoch 3 Batch 2700 Training Loss Date 0.4028 Training Loss Amount 0.5793 Accuracy Date 0.8800 Accuracy Amount 0.7519\n",
            "Epoch 3 Batch 2750 Training Loss Date 0.4023 Training Loss Amount 0.5787 Accuracy Date 0.8801 Accuracy Amount 0.7519\n",
            "Epoch 3 Batch 2800 Training Loss Date 0.4018 Training Loss Amount 0.5778 Accuracy Date 0.8803 Accuracy Amount 0.7520\n",
            "Epoch 3 Batch 2850 Training Loss Date 0.4014 Training Loss Amount 0.5773 Accuracy Date 0.8804 Accuracy Amount 0.7520\n",
            "Epoch 3 Batch 2900 Training Loss Date 0.4008 Training Loss Amount 0.5768 Accuracy Date 0.8806 Accuracy Amount 0.7520\n",
            "Epoch 3 Batch 2950 Training Loss Date 0.4006 Training Loss Amount 0.5766 Accuracy Date 0.8807 Accuracy Amount 0.7523\n",
            "Epoch 3 Batch 3000 Training Loss Date 0.4006 Training Loss Amount 0.5764 Accuracy Date 0.8807 Accuracy Amount 0.7525\n",
            "Epoch 3 Batch 3050 Training Loss Date 0.4002 Training Loss Amount 0.5757 Accuracy Date 0.8808 Accuracy Amount 0.7526\n",
            "Epoch 3 Training Loss Date 0.3999 Training Loss Amount 0.5755 Accuracy Date 0.8809 Accuracy Amount 0.7528\n",
            "Epoch 3 Validation Loss Date 0.3481 Validation Loss Amount 0.5111 Accuracy Date 0.8980 Accuracy Amount 0.7600\n",
            "Time taken for 1 epoch: 562.89 secs\n",
            "\n",
            "Epoch 4 Batch 0 Training Loss Date 0.4399 Training Loss Amount 0.5116 Accuracy Date 0.8620 Accuracy Amount 0.8789\n",
            "Epoch 4 Batch 50 Training Loss Date 0.3812 Training Loss Amount 0.5639 Accuracy Date 0.8879 Accuracy Amount 0.7639\n",
            "Epoch 4 Batch 100 Training Loss Date 0.3766 Training Loss Amount 0.5421 Accuracy Date 0.8893 Accuracy Amount 0.7577\n",
            "Epoch 4 Batch 150 Training Loss Date 0.3837 Training Loss Amount 0.5480 Accuracy Date 0.8862 Accuracy Amount 0.7613\n",
            "Epoch 4 Batch 200 Training Loss Date 0.3849 Training Loss Amount 0.5433 Accuracy Date 0.8859 Accuracy Amount 0.7620\n",
            "Epoch 4 Batch 250 Training Loss Date 0.3815 Training Loss Amount 0.5398 Accuracy Date 0.8868 Accuracy Amount 0.7649\n",
            "Epoch 4 Batch 300 Training Loss Date 0.3793 Training Loss Amount 0.5407 Accuracy Date 0.8875 Accuracy Amount 0.7661\n",
            "Epoch 4 Batch 350 Training Loss Date 0.3762 Training Loss Amount 0.5397 Accuracy Date 0.8883 Accuracy Amount 0.7659\n",
            "Epoch 4 Batch 400 Training Loss Date 0.3790 Training Loss Amount 0.5410 Accuracy Date 0.8876 Accuracy Amount 0.7645\n",
            "Epoch 4 Batch 450 Training Loss Date 0.3811 Training Loss Amount 0.5431 Accuracy Date 0.8868 Accuracy Amount 0.7637\n",
            "Epoch 4 Batch 500 Training Loss Date 0.3815 Training Loss Amount 0.5425 Accuracy Date 0.8865 Accuracy Amount 0.7600\n",
            "Epoch 4 Batch 550 Training Loss Date 0.3808 Training Loss Amount 0.5432 Accuracy Date 0.8868 Accuracy Amount 0.7618\n",
            "Epoch 4 Batch 600 Training Loss Date 0.3802 Training Loss Amount 0.5443 Accuracy Date 0.8869 Accuracy Amount 0.7617\n",
            "Epoch 4 Batch 650 Training Loss Date 0.3802 Training Loss Amount 0.5446 Accuracy Date 0.8869 Accuracy Amount 0.7620\n",
            "Epoch 4 Batch 700 Training Loss Date 0.3792 Training Loss Amount 0.5429 Accuracy Date 0.8873 Accuracy Amount 0.7613\n",
            "Epoch 4 Batch 750 Training Loss Date 0.3786 Training Loss Amount 0.5426 Accuracy Date 0.8875 Accuracy Amount 0.7607\n",
            "Epoch 4 Batch 800 Training Loss Date 0.3781 Training Loss Amount 0.5423 Accuracy Date 0.8876 Accuracy Amount 0.7607\n",
            "Epoch 4 Batch 850 Training Loss Date 0.3775 Training Loss Amount 0.5415 Accuracy Date 0.8879 Accuracy Amount 0.7600\n",
            "Epoch 4 Batch 900 Training Loss Date 0.3778 Training Loss Amount 0.5405 Accuracy Date 0.8878 Accuracy Amount 0.7602\n",
            "Epoch 4 Batch 950 Training Loss Date 0.3782 Training Loss Amount 0.5403 Accuracy Date 0.8877 Accuracy Amount 0.7599\n",
            "Epoch 4 Batch 1000 Training Loss Date 0.3781 Training Loss Amount 0.5396 Accuracy Date 0.8877 Accuracy Amount 0.7597\n",
            "Epoch 4 Batch 1050 Training Loss Date 0.3773 Training Loss Amount 0.5392 Accuracy Date 0.8879 Accuracy Amount 0.7592\n",
            "Epoch 4 Batch 1100 Training Loss Date 0.3761 Training Loss Amount 0.5380 Accuracy Date 0.8882 Accuracy Amount 0.7588\n",
            "Epoch 4 Batch 1150 Training Loss Date 0.3763 Training Loss Amount 0.5376 Accuracy Date 0.8881 Accuracy Amount 0.7589\n",
            "Epoch 4 Batch 1200 Training Loss Date 0.3758 Training Loss Amount 0.5373 Accuracy Date 0.8882 Accuracy Amount 0.7603\n",
            "Epoch 4 Batch 1250 Training Loss Date 0.3754 Training Loss Amount 0.5365 Accuracy Date 0.8883 Accuracy Amount 0.7604\n",
            "Epoch 4 Batch 1300 Training Loss Date 0.3756 Training Loss Amount 0.5366 Accuracy Date 0.8882 Accuracy Amount 0.7599\n",
            "Epoch 4 Batch 1350 Training Loss Date 0.3755 Training Loss Amount 0.5359 Accuracy Date 0.8883 Accuracy Amount 0.7607\n",
            "Epoch 4 Batch 1400 Training Loss Date 0.3753 Training Loss Amount 0.5357 Accuracy Date 0.8884 Accuracy Amount 0.7603\n",
            "Epoch 4 Batch 1450 Training Loss Date 0.3752 Training Loss Amount 0.5359 Accuracy Date 0.8884 Accuracy Amount 0.7602\n",
            "Epoch 4 Batch 1500 Training Loss Date 0.3748 Training Loss Amount 0.5359 Accuracy Date 0.8884 Accuracy Amount 0.7607\n",
            "Epoch 4 Batch 1550 Training Loss Date 0.3746 Training Loss Amount 0.5351 Accuracy Date 0.8885 Accuracy Amount 0.7609\n",
            "Epoch 4 Batch 1600 Training Loss Date 0.3741 Training Loss Amount 0.5345 Accuracy Date 0.8887 Accuracy Amount 0.7609\n",
            "Epoch 4 Batch 1650 Training Loss Date 0.3740 Training Loss Amount 0.5342 Accuracy Date 0.8887 Accuracy Amount 0.7604\n",
            "Epoch 4 Batch 1700 Training Loss Date 0.3741 Training Loss Amount 0.5339 Accuracy Date 0.8887 Accuracy Amount 0.7606\n",
            "Epoch 4 Batch 1750 Training Loss Date 0.3740 Training Loss Amount 0.5339 Accuracy Date 0.8886 Accuracy Amount 0.7601\n",
            "Epoch 4 Batch 1800 Training Loss Date 0.3739 Training Loss Amount 0.5337 Accuracy Date 0.8887 Accuracy Amount 0.7601\n",
            "Epoch 4 Batch 1850 Training Loss Date 0.3734 Training Loss Amount 0.5330 Accuracy Date 0.8888 Accuracy Amount 0.7604\n",
            "Epoch 4 Batch 1900 Training Loss Date 0.3732 Training Loss Amount 0.5327 Accuracy Date 0.8889 Accuracy Amount 0.7601\n",
            "Epoch 4 Batch 1950 Training Loss Date 0.3728 Training Loss Amount 0.5322 Accuracy Date 0.8890 Accuracy Amount 0.7606\n",
            "Epoch 4 Batch 2000 Training Loss Date 0.3724 Training Loss Amount 0.5316 Accuracy Date 0.8892 Accuracy Amount 0.7605\n",
            "Epoch 4 Batch 2050 Training Loss Date 0.3722 Training Loss Amount 0.5313 Accuracy Date 0.8892 Accuracy Amount 0.7610\n",
            "Epoch 4 Batch 2100 Training Loss Date 0.3721 Training Loss Amount 0.5310 Accuracy Date 0.8892 Accuracy Amount 0.7615\n",
            "Epoch 4 Batch 2150 Training Loss Date 0.3717 Training Loss Amount 0.5308 Accuracy Date 0.8893 Accuracy Amount 0.7617\n",
            "Epoch 4 Batch 2200 Training Loss Date 0.3715 Training Loss Amount 0.5305 Accuracy Date 0.8893 Accuracy Amount 0.7615\n",
            "Epoch 4 Batch 2250 Training Loss Date 0.3712 Training Loss Amount 0.5306 Accuracy Date 0.8895 Accuracy Amount 0.7616\n",
            "Epoch 4 Batch 2300 Training Loss Date 0.3713 Training Loss Amount 0.5305 Accuracy Date 0.8895 Accuracy Amount 0.7618\n",
            "Epoch 4 Batch 2350 Training Loss Date 0.3707 Training Loss Amount 0.5299 Accuracy Date 0.8897 Accuracy Amount 0.7620\n",
            "Epoch 4 Batch 2400 Training Loss Date 0.3708 Training Loss Amount 0.5302 Accuracy Date 0.8896 Accuracy Amount 0.7623\n",
            "Epoch 4 Batch 2450 Training Loss Date 0.3710 Training Loss Amount 0.5299 Accuracy Date 0.8896 Accuracy Amount 0.7622\n",
            "Epoch 4 Batch 2500 Training Loss Date 0.3709 Training Loss Amount 0.5295 Accuracy Date 0.8897 Accuracy Amount 0.7624\n",
            "Epoch 4 Batch 2550 Training Loss Date 0.3705 Training Loss Amount 0.5292 Accuracy Date 0.8898 Accuracy Amount 0.7627\n",
            "Epoch 4 Batch 2600 Training Loss Date 0.3701 Training Loss Amount 0.5285 Accuracy Date 0.8899 Accuracy Amount 0.7628\n",
            "Epoch 4 Batch 2650 Training Loss Date 0.3697 Training Loss Amount 0.5283 Accuracy Date 0.8900 Accuracy Amount 0.7630\n",
            "Epoch 4 Batch 2700 Training Loss Date 0.3694 Training Loss Amount 0.5277 Accuracy Date 0.8901 Accuracy Amount 0.7631\n",
            "Epoch 4 Batch 2750 Training Loss Date 0.3694 Training Loss Amount 0.5276 Accuracy Date 0.8901 Accuracy Amount 0.7631\n",
            "Epoch 4 Batch 2800 Training Loss Date 0.3691 Training Loss Amount 0.5274 Accuracy Date 0.8901 Accuracy Amount 0.7632\n",
            "Epoch 4 Batch 2850 Training Loss Date 0.3691 Training Loss Amount 0.5270 Accuracy Date 0.8901 Accuracy Amount 0.7631\n",
            "Epoch 4 Batch 2900 Training Loss Date 0.3689 Training Loss Amount 0.5267 Accuracy Date 0.8902 Accuracy Amount 0.7632\n",
            "Epoch 4 Batch 2950 Training Loss Date 0.3687 Training Loss Amount 0.5260 Accuracy Date 0.8903 Accuracy Amount 0.7634\n",
            "Epoch 4 Batch 3000 Training Loss Date 0.3682 Training Loss Amount 0.5256 Accuracy Date 0.8904 Accuracy Amount 0.7631\n",
            "Epoch 4 Batch 3050 Training Loss Date 0.3680 Training Loss Amount 0.5253 Accuracy Date 0.8905 Accuracy Amount 0.7630\n",
            "Epoch 4 Training Loss Date 0.3678 Training Loss Amount 0.5252 Accuracy Date 0.8906 Accuracy Amount 0.7629\n",
            "Epoch 4 Validation Loss Date 0.3386 Validation Loss Amount 0.4930 Accuracy Date 0.9023 Accuracy Amount 0.7651\n",
            "Time taken for 1 epoch: 560.53 secs\n",
            "\n",
            "Epoch 5 Batch 0 Training Loss Date 0.3841 Training Loss Amount 0.3729 Accuracy Date 0.8750 Accuracy Amount 0.7344\n",
            "Epoch 5 Batch 50 Training Loss Date 0.3655 Training Loss Amount 0.5166 Accuracy Date 0.8927 Accuracy Amount 0.7488\n",
            "Epoch 5 Batch 100 Training Loss Date 0.3609 Training Loss Amount 0.5060 Accuracy Date 0.8922 Accuracy Amount 0.7540\n",
            "Epoch 5 Batch 150 Training Loss Date 0.3541 Training Loss Amount 0.5091 Accuracy Date 0.8947 Accuracy Amount 0.7574\n",
            "Epoch 5 Batch 200 Training Loss Date 0.3550 Training Loss Amount 0.5082 Accuracy Date 0.8939 Accuracy Amount 0.7553\n",
            "Epoch 5 Batch 250 Training Loss Date 0.3572 Training Loss Amount 0.5097 Accuracy Date 0.8932 Accuracy Amount 0.7581\n",
            "Epoch 5 Batch 300 Training Loss Date 0.3572 Training Loss Amount 0.5091 Accuracy Date 0.8934 Accuracy Amount 0.7612\n",
            "Epoch 5 Batch 350 Training Loss Date 0.3577 Training Loss Amount 0.5089 Accuracy Date 0.8933 Accuracy Amount 0.7644\n",
            "Epoch 5 Batch 400 Training Loss Date 0.3574 Training Loss Amount 0.5101 Accuracy Date 0.8933 Accuracy Amount 0.7660\n",
            "Epoch 5 Batch 450 Training Loss Date 0.3577 Training Loss Amount 0.5083 Accuracy Date 0.8932 Accuracy Amount 0.7665\n",
            "Epoch 5 Batch 500 Training Loss Date 0.3588 Training Loss Amount 0.5108 Accuracy Date 0.8928 Accuracy Amount 0.7679\n",
            "Epoch 5 Batch 550 Training Loss Date 0.3588 Training Loss Amount 0.5099 Accuracy Date 0.8928 Accuracy Amount 0.7675\n",
            "Epoch 5 Batch 600 Training Loss Date 0.3584 Training Loss Amount 0.5109 Accuracy Date 0.8929 Accuracy Amount 0.7678\n",
            "Epoch 5 Batch 650 Training Loss Date 0.3572 Training Loss Amount 0.5092 Accuracy Date 0.8934 Accuracy Amount 0.7678\n",
            "Epoch 5 Batch 700 Training Loss Date 0.3578 Training Loss Amount 0.5083 Accuracy Date 0.8932 Accuracy Amount 0.7677\n",
            "Epoch 5 Batch 750 Training Loss Date 0.3579 Training Loss Amount 0.5085 Accuracy Date 0.8932 Accuracy Amount 0.7678\n",
            "Epoch 5 Batch 800 Training Loss Date 0.3581 Training Loss Amount 0.5103 Accuracy Date 0.8931 Accuracy Amount 0.7687\n",
            "Epoch 5 Batch 850 Training Loss Date 0.3577 Training Loss Amount 0.5109 Accuracy Date 0.8933 Accuracy Amount 0.7682\n",
            "Epoch 5 Batch 900 Training Loss Date 0.3567 Training Loss Amount 0.5105 Accuracy Date 0.8936 Accuracy Amount 0.7682\n",
            "Epoch 5 Batch 950 Training Loss Date 0.3562 Training Loss Amount 0.5099 Accuracy Date 0.8936 Accuracy Amount 0.7679\n",
            "Epoch 5 Batch 1000 Training Loss Date 0.3561 Training Loss Amount 0.5099 Accuracy Date 0.8937 Accuracy Amount 0.7675\n",
            "Epoch 5 Batch 1050 Training Loss Date 0.3560 Training Loss Amount 0.5090 Accuracy Date 0.8938 Accuracy Amount 0.7671\n",
            "Epoch 5 Batch 1100 Training Loss Date 0.3559 Training Loss Amount 0.5091 Accuracy Date 0.8938 Accuracy Amount 0.7668\n",
            "Epoch 5 Batch 1150 Training Loss Date 0.3557 Training Loss Amount 0.5094 Accuracy Date 0.8939 Accuracy Amount 0.7665\n",
            "Epoch 5 Batch 1200 Training Loss Date 0.3562 Training Loss Amount 0.5092 Accuracy Date 0.8937 Accuracy Amount 0.7653\n",
            "Epoch 5 Batch 1250 Training Loss Date 0.3559 Training Loss Amount 0.5088 Accuracy Date 0.8939 Accuracy Amount 0.7660\n",
            "Epoch 5 Batch 1300 Training Loss Date 0.3557 Training Loss Amount 0.5078 Accuracy Date 0.8940 Accuracy Amount 0.7662\n",
            "Epoch 5 Batch 1350 Training Loss Date 0.3558 Training Loss Amount 0.5076 Accuracy Date 0.8939 Accuracy Amount 0.7662\n",
            "Epoch 5 Batch 1400 Training Loss Date 0.3565 Training Loss Amount 0.5083 Accuracy Date 0.8937 Accuracy Amount 0.7671\n",
            "Epoch 5 Batch 1450 Training Loss Date 0.3564 Training Loss Amount 0.5081 Accuracy Date 0.8938 Accuracy Amount 0.7672\n",
            "Epoch 5 Batch 1500 Training Loss Date 0.3562 Training Loss Amount 0.5078 Accuracy Date 0.8939 Accuracy Amount 0.7672\n",
            "Epoch 5 Batch 1550 Training Loss Date 0.3563 Training Loss Amount 0.5078 Accuracy Date 0.8939 Accuracy Amount 0.7668\n",
            "Epoch 5 Batch 1600 Training Loss Date 0.3559 Training Loss Amount 0.5073 Accuracy Date 0.8939 Accuracy Amount 0.7661\n",
            "Epoch 5 Batch 1650 Training Loss Date 0.3556 Training Loss Amount 0.5071 Accuracy Date 0.8940 Accuracy Amount 0.7659\n",
            "Epoch 5 Batch 1700 Training Loss Date 0.3554 Training Loss Amount 0.5068 Accuracy Date 0.8940 Accuracy Amount 0.7660\n",
            "Epoch 5 Batch 1750 Training Loss Date 0.3553 Training Loss Amount 0.5064 Accuracy Date 0.8941 Accuracy Amount 0.7661\n",
            "Epoch 5 Batch 1800 Training Loss Date 0.3551 Training Loss Amount 0.5063 Accuracy Date 0.8941 Accuracy Amount 0.7664\n",
            "Epoch 5 Batch 1850 Training Loss Date 0.3546 Training Loss Amount 0.5051 Accuracy Date 0.8942 Accuracy Amount 0.7665\n",
            "Epoch 5 Batch 1900 Training Loss Date 0.3544 Training Loss Amount 0.5054 Accuracy Date 0.8943 Accuracy Amount 0.7667\n",
            "Epoch 5 Batch 1950 Training Loss Date 0.3550 Training Loss Amount 0.5052 Accuracy Date 0.8941 Accuracy Amount 0.7670\n",
            "Epoch 5 Batch 2000 Training Loss Date 0.3551 Training Loss Amount 0.5055 Accuracy Date 0.8940 Accuracy Amount 0.7671\n",
            "Epoch 5 Batch 2050 Training Loss Date 0.3547 Training Loss Amount 0.5049 Accuracy Date 0.8942 Accuracy Amount 0.7675\n",
            "Epoch 5 Batch 2100 Training Loss Date 0.3549 Training Loss Amount 0.5046 Accuracy Date 0.8941 Accuracy Amount 0.7671\n",
            "Epoch 5 Batch 2150 Training Loss Date 0.3547 Training Loss Amount 0.5050 Accuracy Date 0.8941 Accuracy Amount 0.7675\n",
            "Epoch 5 Batch 2200 Training Loss Date 0.3545 Training Loss Amount 0.5046 Accuracy Date 0.8942 Accuracy Amount 0.7672\n",
            "Epoch 5 Batch 2250 Training Loss Date 0.3542 Training Loss Amount 0.5046 Accuracy Date 0.8943 Accuracy Amount 0.7670\n",
            "Epoch 5 Batch 2300 Training Loss Date 0.3540 Training Loss Amount 0.5045 Accuracy Date 0.8943 Accuracy Amount 0.7674\n",
            "Epoch 5 Batch 2350 Training Loss Date 0.3535 Training Loss Amount 0.5040 Accuracy Date 0.8945 Accuracy Amount 0.7676\n",
            "Epoch 5 Batch 2400 Training Loss Date 0.3531 Training Loss Amount 0.5034 Accuracy Date 0.8946 Accuracy Amount 0.7678\n",
            "Epoch 5 Batch 2450 Training Loss Date 0.3529 Training Loss Amount 0.5026 Accuracy Date 0.8947 Accuracy Amount 0.7675\n",
            "Epoch 5 Batch 2500 Training Loss Date 0.3527 Training Loss Amount 0.5022 Accuracy Date 0.8948 Accuracy Amount 0.7676\n",
            "Epoch 5 Batch 2550 Training Loss Date 0.3523 Training Loss Amount 0.5022 Accuracy Date 0.8949 Accuracy Amount 0.7678\n",
            "Epoch 5 Batch 2600 Training Loss Date 0.3524 Training Loss Amount 0.5022 Accuracy Date 0.8949 Accuracy Amount 0.7680\n",
            "Epoch 5 Batch 2650 Training Loss Date 0.3520 Training Loss Amount 0.5021 Accuracy Date 0.8950 Accuracy Amount 0.7678\n",
            "Epoch 5 Batch 2700 Training Loss Date 0.3520 Training Loss Amount 0.5019 Accuracy Date 0.8949 Accuracy Amount 0.7679\n",
            "Epoch 5 Batch 2750 Training Loss Date 0.3521 Training Loss Amount 0.5014 Accuracy Date 0.8949 Accuracy Amount 0.7678\n",
            "Epoch 5 Batch 2800 Training Loss Date 0.3520 Training Loss Amount 0.5011 Accuracy Date 0.8950 Accuracy Amount 0.7683\n",
            "Epoch 5 Batch 2850 Training Loss Date 0.3520 Training Loss Amount 0.5008 Accuracy Date 0.8950 Accuracy Amount 0.7682\n",
            "Epoch 5 Batch 2900 Training Loss Date 0.3519 Training Loss Amount 0.5008 Accuracy Date 0.8950 Accuracy Amount 0.7677\n",
            "Epoch 5 Batch 2950 Training Loss Date 0.3522 Training Loss Amount 0.5006 Accuracy Date 0.8949 Accuracy Amount 0.7676\n",
            "Epoch 5 Batch 3000 Training Loss Date 0.3520 Training Loss Amount 0.5008 Accuracy Date 0.8950 Accuracy Amount 0.7675\n",
            "Epoch 5 Batch 3050 Training Loss Date 0.3519 Training Loss Amount 0.5003 Accuracy Date 0.8950 Accuracy Amount 0.7676\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Training Loss Date 0.3516 Training Loss Amount 0.5002 Accuracy Date 0.8951 Accuracy Amount 0.7677\n",
            "Epoch 5 Validation Loss Date 0.3221 Validation Loss Amount 0.4851 Accuracy Date 0.9044 Accuracy Amount 0.7692\n",
            "Time taken for 1 epoch: 560.67 secs\n",
            "\n",
            "Epoch 6 Batch 0 Training Loss Date 0.3901 Training Loss Amount 0.5159 Accuracy Date 0.8802 Accuracy Amount 0.8906\n",
            "Epoch 6 Batch 50 Training Loss Date 0.3485 Training Loss Amount 0.4918 Accuracy Date 0.8948 Accuracy Amount 0.7627\n",
            "Epoch 6 Batch 100 Training Loss Date 0.3476 Training Loss Amount 0.4921 Accuracy Date 0.8949 Accuracy Amount 0.7776\n",
            "Epoch 6 Batch 150 Training Loss Date 0.3495 Training Loss Amount 0.4865 Accuracy Date 0.8945 Accuracy Amount 0.7686\n",
            "Epoch 6 Batch 200 Training Loss Date 0.3478 Training Loss Amount 0.4872 Accuracy Date 0.8952 Accuracy Amount 0.7799\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-d019644633c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# inp -> ocr, tar -> amount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_amt\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtar_dt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_dt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtar_amt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QfcsSWswSdGV"
      },
      "source": [
        "## Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "y6APsFrgImLW"
      },
      "source": [
        "The following steps are used for evaluation:\n",
        "\n",
        "* Encode the input sentence using the ocr tokenizer (`tokenizer_ocr`). Moreover, add the start and end token so the input is equivalent to what the model is trained with. This is the encoder input.\n",
        "* The decoder input is the `start token == tokenizer_en.vocab_size`.\n",
        "* Calculate the padding masks and the look ahead masks.\n",
        "* The `decoder` then outputs the predictions by looking at the `encoder output` and its own output (self-attention).\n",
        "* Select the last word and calculate the argmax of that.\n",
        "* Concatentate the predicted word to the decoder input as pass it to the decoder.\n",
        "* In this approach, the decoder predicts the next word based on the previous words it predicted.\n",
        "\n",
        "Note: The model used here has less capacity to keep the example relatively faster so the predictions maybe less right. To reproduce the results in the paper, use the entire dataset and base transformer model or transformer XL, by changing the hyperparameters above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.586Z"
        },
        "colab_type": "code",
        "id": "5buvMlnvyrFm",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "  start_token = [tokenizer_ocr.vocab_size]\n",
        "  end_token = [tokenizer_ocr.vocab_size + 1]\n",
        "  \n",
        "  # inp sentence is portuguese, hence adding the start and end token\n",
        "  inp_sentence = start_token + tokenizer_ocr.encode(inp_sentence) + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "  decoder_input_dt = [tokenizer_date.vocab_size]\n",
        "  output_dt = tf.expand_dims(decoder_input_dt, 0)\n",
        "  decoder_input_amt = [tokenizer_amount.vocab_size]\n",
        "  output_amt = tf.expand_dims(decoder_input_amt, 0)\n",
        "    \n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask_dt, dec_padding_mask, combined_mask_amt = create_masks(\n",
        "        encoder_input, output_dt, output_amt)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions_dt, attention_weights_dt, predictions_amt, attention_weights_amt = transformer(encoder_input, \n",
        "                                                 output_dt,\n",
        "                                                 output_amt,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask_dt,\n",
        "                                                 combined_mask_amt,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions_dt = predictions_dt[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "    predictions_amt = predictions_amt[: ,-1:, :]\n",
        "\n",
        "    predicted_id_dt = tf.cast(tf.argmax(predictions_dt, axis=-1), tf.int32)\n",
        "    predicted_id_amt = tf.cast(tf.argmax(predictions_amt, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id_dt == tokenizer_date.vocab_size+1:\n",
        "      return tf.squeeze(output_dt, axis=0), attention_weights_dt\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output_dt = tf.concat([output_dt, predicted_id_dt], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output_dt, axis=0), attention_weights_dt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.589Z"
        },
        "colab_type": "code",
        "id": "CN-BV43FMBej",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def plot_attention_weights(attention, sentence, result, layer):\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "  \n",
        "  sentence = tokenizer_ocr.encode(sentence)\n",
        "  \n",
        "  attention = tf.squeeze(attention[layer], axis=0)\n",
        "  \n",
        "  for head in range(attention.shape[0]):\n",
        "    ax = fig.add_subplot(2, 4, head+1)\n",
        "    \n",
        "    # plot the attention weights\n",
        "    ax.matshow(attention[head][:-1, :], cmap='viridis')\n",
        "\n",
        "    fontdict = {'fontsize': 10}\n",
        "    \n",
        "    ax.set_xticks(range(len(sentence)+2))\n",
        "    ax.set_yticks(range(len(result)))\n",
        "    \n",
        "    ax.set_ylim(len(result)-1.5, -0.5)\n",
        "        \n",
        "    ax.set_xticklabels(\n",
        "        ['<start>']+[tokenizer_ocr.decode([i]) for i in sentence]+['<end>'], \n",
        "        fontdict=fontdict, rotation=90)\n",
        "    \n",
        "    ax.set_yticklabels([tokenizer_date.decode([i]) for i in result \n",
        "                        if i < tokenizer_date.vocab_size], \n",
        "                       fontdict=fontdict)\n",
        "    \n",
        "    ax.set_xlabel('Head {}'.format(head+1))\n",
        "  \n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.591Z"
        },
        "colab_type": "code",
        "id": "lU2_yG_vBGza",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "def translate(ocr, plot='', log=True):\n",
        "  enc_result, attention_weights = evaluate(ocr)\n",
        "  \n",
        "  result = tokenizer_date.decode([i for i in enc_result \n",
        "                                            if i < tokenizer_date.vocab_size])  \n",
        "  \n",
        "  if plot:\n",
        "    plot_attention_weights(attention_weights, ocr, enc_result, plot)\n",
        "\n",
        "  if log:\n",
        "    print('Input: {}'.format(ocr))\n",
        "    print('Predicted date: {}'.format(result))\n",
        "  else:\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.593Z"
        },
        "colab_type": "code",
        "id": "YsxrAlvFG8SZ",
        "pycharm": {
          "is_executing": false
        },
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "translate(\"* 0KQ8 OK DETALJHANDEL AB ORG NR: 556029-4588 37548 Servogatan 12 SE-19560 TEL NR: 08-59113055  Arlandastad  Belopp 18,00 19,00 37.00 kr 37,00 kr BRUTTO 37,00  Antal  Beskrivning #w#m #W# ■** Aria Kvarg Mild 0,2% 250g Varm Dryck Li ten  1  1  EuroCard:  NETTO MOMS 33,04 3,96  MOMS %  12%  20:30 SEK 37,00  2017-12-18  KOP  EuroCard Butiksnr: 134890 Cal 7 000 SWE 746784 REF.NR :  Termid: 37548122  Personlig kod 375481223988 A0000000041010 0000001000  AID  TVR  6800  TSI  KASSÖRENS NAMN: F, Erik DATUM: 2017-12-18 TRAN: 245569  110:20:30:36 KASSA NR:  2  TACK FOR BESOKET VAEKOMMEN ATER\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_1MxkSZvz0jX"
      },
      "source": [
        "You can pass different layers and attention blocks of the decoder to the `plot` parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubyeeQ-sOMXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "translate(\"EURMWST 19,00% A9,04 EUR 56,60 EUR# 29/02/2021\", plot='decoder_layer2_block2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2019-11-10T06:32:19.596Z"
        },
        "colab_type": "code",
        "id": "t-kFyiOLH0xg",
        "pycharm": {
          "is_executing": false
        },
        "colab": {}
      },
      "source": [
        "#!tar -czvf checkpoints.tar.gz checkpoints"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsKGQ8277SXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "validation_set_num = 1500\n",
        "date_pred = []\n",
        "date_gt = []\n",
        "ocr_text = []\n",
        "pbar = tqdm(total=validation_set_num)\n",
        "\n",
        "for index, row in testing_df[:validation_set_num].iterrows():\n",
        "  try:\n",
        "    date_pred.append(translate(row[\"ocr_text\"], log=False))\n",
        "    date_gt.append(row[\"date\"])\n",
        "    ocr_text.append(row[\"ocr_text\"])\n",
        "  except Exception:\n",
        "    continue\n",
        "\n",
        "  pbar.update()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fyni3wxy7xqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_result = pd.DataFrame()\n",
        "df_result[\"dt_pred\"] = date_pred\n",
        "df_result[\"dt_gt\"] = date_gt\n",
        "df_result[\"ocr_text\"] = ocr_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PiFAM1Q8Unj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_result[\"dt_correct\"] = df_result[\"dt_pred\"] == df_result[\"dt_gt\"]\n",
        "df_result[\"dt_correct_mnt_day\"] = df_result.apply(lambda x: \"-\".join(x[\"dt_pred\"].split(\"-\")[1:3]) == \"-\".join(x[\"dt_gt\"].split(\"-\")[1:3]), axis=1)\n",
        "df_result[\"dt_correct_day\"] = df_result.apply(lambda x: \"-\".join(x[\"dt_pred\"].split(\"-\")[2:3]) == \"-\".join(x[\"dt_gt\"].split(\"-\")[2:3]), axis=1)\n",
        "df_result[\"dt_correct_month\"] = df_result.apply(lambda x: \"-\".join(x[\"dt_pred\"].split(\"-\")[1:2]) == \"-\".join(x[\"dt_gt\"].split(\"-\")[1:2]), axis=1)\n",
        "df_result[\"dt_correct_year\"] = df_result.apply(lambda x: \"-\".join(x[\"dt_pred\"].split(\"-\")[0:1]) == \"-\".join(x[\"dt_gt\"].split(\"-\")[0:1]), axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoh2fNKS8XKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Date Accuracy  {df_result['dt_correct'].mean()}\")\n",
        "print(f\"Month-Day Accuracy {df_result['dt_correct_mnt_day'].mean()}\")\n",
        "print(f\"Day Accuracy {df_result['dt_correct_day'].mean()}\")\n",
        "print(f\"Month Accuracy {df_result['dt_correct_month'].mean()}\")\n",
        "print(f\"Year Accuracy {df_result['dt_correct_year'].mean()}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCnJ1Cqi8ZZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pd.set_option('display.max_colwidth', -1)\n",
        "\n",
        "df_result[df_result[\"dt_correct\"] == False].tail(20)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}